{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Computer Vision Algorithm for Tracking Planarian Motion\n",
    "developed by Hokin Deng xueqiandeng@yahoo.com"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Import Dependencies and Data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Import packages and make sure of the python technicality"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python Version: 3.11.4 (main, Jul  5 2023, 08:41:25) [Clang 14.0.6 ]\n",
      "Python Executable: /Users/billdeng/anaconda3/envs/unlearning_Version1/bin/python\n",
      "Python Path: ['/Users/billdeng/PycharmProjects/unicellular', '/Users/billdeng/PycharmProjects/unicellular', '/Users/billdeng/anaconda3/envs/unlearning_Version1/lib/python311.zip', '/Users/billdeng/anaconda3/envs/unlearning_Version1/lib/python3.11', '/Users/billdeng/anaconda3/envs/unlearning_Version1/lib/python3.11/lib-dynload', '', '/Users/billdeng/anaconda3/envs/unlearning_Version1/lib/python3.11/site-packages']\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "print(\"Python Version:\", sys.version)\n",
    "print(\"Python Executable:\", sys.executable)\n",
    "print(\"Python Path:\", sys.path)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "import cv2\n",
    "import math\n",
    "import copy\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import numpy as np\n",
    "import argparse\n",
    "import imutils\n",
    "import matplotlib.pyplot as plt\n",
    "np.set_printoptions(threshold=sys.maxsize)\n",
    "np.set_printoptions(threshold=50)\n",
    "import queue\n",
    "import math"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The video data should be named as \"sample.avi\" and put in the folder as the notebook."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Open a video file (replace 'sample.avi' with the path)\n",
    "cap = cv2.VideoCapture('sample.avi')\n",
    "\n",
    "# Check if the video file was opened successfully\n",
    "if not cap.isOpened():\n",
    "    print(\"Error: Could not open video file.\")\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Display the video if want to have a look at it"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "#cv2.startWindowThread()\n",
    "# Loop to read and display frames\n",
    "#while True:\n",
    "    # Read a frame from the video\n",
    "#    ret, frame = cap.read()\n",
    "    # If the video has ended, break out of the loop\n",
    "#    if not ret:\n",
    "#        break\n",
    "    # Display the frame in a window\n",
    "#    cv2.imshow('Video', frame)\n",
    "    # Exit the loop if the 'q' key is pressed\n",
    "#    if cv2.waitKey(25) & 0xFF == ord('q'):\n",
    "#        break\n",
    "\n",
    "# Release the video capture object and close the window"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "First, get basic properties about our video"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frame Count: 1202\n",
      "Frame Width: 2160\n",
      "Frame Height: 2160\n",
      "Frame Rate: 10.0 frames per second\n",
      "Video Duration: 120.20 seconds\n"
     ]
    }
   ],
   "source": [
    "# Get basic video properties\n",
    "frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "frame_rate = cap.get(cv2.CAP_PROP_FPS)\n",
    "video_duration_sec = frame_count / frame_rate if frame_rate > 0 else 0\n",
    "\n",
    "# Print the video properties\n",
    "print(f\"Frame Count: {frame_count}\")\n",
    "print(f\"Frame Width: {frame_width}\")\n",
    "print(f\"Frame Height: {frame_height}\")\n",
    "print(f\"Frame Rate: {frame_rate} frames per second\")\n",
    "print(f\"Video Duration: {video_duration_sec:.2f} seconds\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ret <class 'bool'>\n",
      "frame <class 'numpy.ndarray'>\n",
      "ret True\n",
      "frame [[[255 255 255]\n",
      "  [255 255 255]\n",
      "  [255 255 255]\n",
      "  ...\n",
      "  [255 255 255]\n",
      "  [255 255 255]\n",
      "  [255 255 255]]\n",
      "\n",
      " [[255 255 255]\n",
      "  [255 255 255]\n",
      "  [255 255 255]\n",
      "  ...\n",
      "  [255 255 255]\n",
      "  [255 255 255]\n",
      "  [255 255 255]]\n",
      "\n",
      " [[255 255 255]\n",
      "  [255 255 255]\n",
      "  [255 255 255]\n",
      "  ...\n",
      "  [255 255 255]\n",
      "  [255 255 255]\n",
      "  [255 255 255]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[255 255 255]\n",
      "  [255 255 255]\n",
      "  [255 255 255]\n",
      "  ...\n",
      "  [255 255 255]\n",
      "  [255 255 255]\n",
      "  [255 255 255]]\n",
      "\n",
      " [[255 255 255]\n",
      "  [255 255 255]\n",
      "  [255 255 255]\n",
      "  ...\n",
      "  [255 255 255]\n",
      "  [255 255 255]\n",
      "  [255 255 255]]\n",
      "\n",
      " [[255 255 255]\n",
      "  [255 255 255]\n",
      "  [255 255 255]\n",
      "  ...\n",
      "  [255 255 255]\n",
      "  [255 255 255]\n",
      "  [255 255 255]]]\n"
     ]
    }
   ],
   "source": [
    "example_ret, example_frame = cap.read()\n",
    "print( 'ret', type(example_ret))\n",
    "print( 'frame', type(example_frame))\n",
    "print('ret', example_ret)\n",
    "print('frame', example_frame)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "# frame_number = 100  # The frame you want to access\n",
    "# Set the video position to the desired frame\n",
    "# cap.set(cv2.CAP_PROP_POS_FRAMES, frame_number - 1)\n",
    "# Read the frame\n",
    "# ret, frame = cap.read()\n",
    "# if ret:\n",
    "    # Process the frame\n",
    "#     cv2.imshow('Frame 100', frame)\n",
    "#    cv2.waitKey(0)  # Wait for a key press to close the image window\n",
    "# else:\n",
    "#    print(\"Error: Unable to read the frame\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Inspect function for a frame"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "def inspect_frame_from_video(cap_for_here, frame_number):\n",
    "    \"\"\"\n",
    "    Inspects a specific frame in a video.\n",
    "\n",
    "    :param cap_for_here: video reference\n",
    "    :param frame_number: The frame number to inspect (1-based index).\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    # Open the video file\n",
    "    if not cap_for_here.isOpened():\n",
    "        print(\"Error: Unable to open video file\")\n",
    "        return\n",
    "    # Check if the frame number is valid\n",
    "    total_frames = cap_for_here.get(cv2.CAP_PROP_FRAME_COUNT)\n",
    "    if frame_number < 1 or frame_number > total_frames:\n",
    "        print(f\"Frame number should be between 1 and {int(total_frames)}\")\n",
    "        return\n",
    "    # Set the video position to the desired frame (0-based index for frame_number)\n",
    "    cap_for_here.set(cv2.CAP_PROP_POS_FRAMES, frame_number - 1)\n",
    "    # Read the frame\n",
    "    ret_temp, frame_temp = cap_for_here.read()\n",
    "    if ret_temp:\n",
    "        # Display the frame\n",
    "        # cv2.imshow(f'Frame {frame_number}', frame)\n",
    "        # cv2.waitKey(0)  # Wait for a key press to close the image window\n",
    "        # cv2.destroyAllWindows()\n",
    "        frame_h, frame_w = frame_temp.shape[:2]\n",
    "        print(f\"Frame Width: {frame_h}\")\n",
    "        print(f\"Frame Height: {frame_w}\")\n",
    "        # Color Channels\n",
    "        channels = frame_temp.shape[2] if len(frame_temp.shape) == 3 else 1\n",
    "        print(f\"Color Channels: {channels}\")\n",
    "        # Data Type\n",
    "        data_type = frame_temp.dtype\n",
    "        print(f\"Data Type: {data_type}\")\n",
    "        # Aspect Ratio\n",
    "        aspect_ratio = frame_w / frame_h\n",
    "        print(f\"Aspect Ratio: {aspect_ratio}\")\n",
    "        # Resolution (assuming a standard display resolution of 96 PPI)\n",
    "        # Color Space (assuming default BGR)\n",
    "        color_space = \"BGR\" if channels == 3 else \"Grayscale\"\n",
    "        print(f\"Color Space: {color_space}\")\n",
    "        # Histogram for each channel\n",
    "        if channels > 1:\n",
    "            for i, col in enumerate(['Blue', 'Green', 'Red']):\n",
    "                hist = cv2.calcHist([frame_temp], [i], None, [256], [0, 256])\n",
    "                print(f\"Histogram for {col} channel: {np.array(hist).flatten()}\")\n",
    "        else:\n",
    "            hist = cv2.calcHist([frame_temp], [0], None, [256], [0, 256])\n",
    "            print(f\"Histogram for Grayscale: {np.array(hist).flatten()}\")\n",
    "    else:\n",
    "        print(\"Error: Unable to read the frame\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "def inspect_given_a_frame(this_frame):\n",
    "    frame_h, frame_w = this_frame.shape[:2]\n",
    "    print(f\"Frame Width: {frame_h}\")\n",
    "    print(f\"Frame Height: {frame_w}\")\n",
    "    channels = this_frame.shape[2] if len(this_frame.shape) == 3 else 1\n",
    "    print(f\"Color Channels: {channels}\")\n",
    "    data_type = this_frame.dtype\n",
    "    print(f\"Data Type: {data_type}\")\n",
    "    aspect_ratio = frame_w / frame_h\n",
    "    print(f\"Aspect Ratio: {aspect_ratio}\")\n",
    "    color_space = \"BGR\" if channels == 3 else \"Grayscale\"\n",
    "    print(f\"Color Space: {color_space}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frame Width: 2160\n",
      "Frame Height: 2160\n",
      "Color Channels: 3\n",
      "Data Type: uint8\n",
      "Aspect Ratio: 1.0\n",
      "Color Space: BGR\n",
      "Histogram for Blue channel: [      0.       0.       0. ...   31073.   57326. 3775011.]\n",
      "Histogram for Green channel: [      0.       0.       0. ...   31073.   57326. 3775011.]\n",
      "Histogram for Red channel: [      0.       0.       0. ...   31073.   57326. 3775011.]\n"
     ]
    }
   ],
   "source": [
    "# Usage example\n",
    "video_file = 'sample.avi'\n",
    "frame_to_inspect = 100  # Adjust the frame number as needed\n",
    "inspect_frame_from_video(cap, frame_to_inspect)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frame Width: 2160\n",
      "Frame Height: 2160\n",
      "Color Channels: 3\n",
      "Data Type: uint8\n",
      "Aspect Ratio: 1.0\n",
      "Color Space: BGR\n"
     ]
    }
   ],
   "source": [
    "ret, frame_for_use = cap.read()\n",
    "inspect_given_a_frame(frame_for_use)\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Pre-Processing\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Background subtraction"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "i = 0\n",
    "# Initialize video capture\n",
    "cap = cv2.VideoCapture('sample.avi')\n",
    "# Get video properties\n",
    "frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "frame_rate = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "# Define codec and create VideoWriter object\n",
    "fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "out = cv2.VideoWriter('background_remove.avi', fourcc, frame_rate, (frame_width, frame_height), False)\n",
    "backSub = cv2.createBackgroundSubtractorMOG2()\n",
    "while True:\n",
    "    i += 1\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    fgMask = backSub.apply(frame)\n",
    "    out.write(fgMask)\n",
    "    print(f'\\rProgress: {i}', end='')\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "cap.release()\n",
    "out.release()\n",
    "cv2.destroyAllWindows()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Spatial Smoothing"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "i = 0\n",
    "cap = cv2.VideoCapture('background_remove.avi')\n",
    "frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "frame_rate = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "out = cv2.VideoWriter('background_remove_spatial_smoothed.avi', fourcc, frame_rate, (frame_width, frame_height), False)\n",
    "while True:\n",
    "    i += 1\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    blurred_frame = cv2.GaussianBlur(frame, (5, 5), 0)\n",
    "    out.write(blurred_frame)\n",
    "    print(f'\\rProgress: {i}', end='')\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "cap.release()\n",
    "out.release()\n",
    "cv2.destroyAllWindows()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Temporal Smoothing"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture('background_remove_spatial_smooth.avi')\n",
    "fourcc = cv2.VideoWriter_fourcc(*'XVID') # You can also use 'XVID'\n",
    "out = cv2.VideoWriter('b_r_s_s_t_s.avi', fourcc, frame_rate, (frame_width, frame_height), False)\n",
    "buffer_size = 5\n",
    "frame_buffer = []\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    frame_buffer.append(frame)\n",
    "    if len(frame_buffer) > buffer_size:\n",
    "        frame_buffer.pop(0)\n",
    "    # Temporal smoothing (average of frames in buffer)\n",
    "    temp_smoothed = np.mean(frame_buffer, axis=0).astype(np.uint8)\n",
    "    # Write frame to video\n",
    "    out.write(temp_smoothed)\n",
    "    # Break the loop\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "cap.release()\n",
    "out.release()\n",
    "cv2.destroyAllWindows()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "do everything all at once"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Initialize video capture\n",
    "cap = cv2.VideoCapture('sample.avi')\n",
    "# Get video properties\n",
    "frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "frame_rate = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "# Define codec and create VideoWriter object\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v') # You can also use 'XVID'\n",
    "out = cv2.VideoWriter('pre_dense.mp4', fourcc, frame_rate, (frame_width, frame_height), False)\n",
    "# Background subtractor\n",
    "backSub = cv2.createBackgroundSubtractorMOG2()\n",
    "# Buffer for temporal smoothing\n",
    "buffer_size = 5\n",
    "frame_buffer = []\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    # Background subtraction\n",
    "    fgMask = backSub.apply(frame)\n",
    "    # Spatial smoothing (Gaussian blur)\n",
    "    blurred = cv2.GaussianBlur(fgMask, (5, 5), 0)\n",
    "    # Add frame to buffer for temporal smoothing\n",
    "    frame_buffer.append(blurred)\n",
    "    if len(frame_buffer) > buffer_size:\n",
    "        frame_buffer.pop(0)\n",
    "    # Temporal smoothing (average of frames in buffer)\n",
    "    temp_smoothed = np.mean(frame_buffer, axis=0).astype(np.uint8)\n",
    "    # Write frame to video\n",
    "    out.write(temp_smoothed)\n",
    "    # Display result\n",
    "    # cv2.imshow('Frame', frame)\n",
    "    # cv2.imshow('FG Mask', fgMask)\n",
    "    # cv2.imshow('Blurred', blurred)\n",
    "    # cv2.imshow('Temporally Smoothed', temp_smoothed)\n",
    "    # Break the loop\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "# Release everything\n",
    "cap.release()\n",
    "out.release()\n",
    "cv2.destroyAllWindows()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "dense opti flow\n",
    "this takes a very very long time"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(\"pre_dense.mp4\")\n",
    "if not cap.isOpened():\n",
    "    print(\"Error opening video file\")\n",
    "    exit()\n",
    "frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "frame_rate = cap.get(cv2.CAP_PROP_FPS)\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')  # Try 'XVID' if 'mp4v' does not work\n",
    "out = cv2.VideoWriter('dense_opti_flow_v2.mp4', fourcc, frame_rate, (frame_width, frame_height), True)\n",
    "ret, first_frame = cap.read()\n",
    "if not ret:\n",
    "    print(\"Error reading first frame\")\n",
    "    cap.release()\n",
    "    exit()\n",
    "prev_gray = cv2.cvtColor(first_frame, cv2.COLOR_BGR2GRAY)\n",
    "mask = np.zeros_like(first_frame)\n",
    "mask[..., 1] = 255\n",
    "i = 0\n",
    "while True:\n",
    "    i += 1\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    flow = cv2.calcOpticalFlowFarneback(prev_gray, gray, None, 0.5, 3, 15, 3, 5, 1.2, 0)\n",
    "    magnitude, angle = cv2.cartToPolar(flow[..., 0], flow[..., 1])\n",
    "    mask[..., 0] = angle * 180 / np.pi / 2\n",
    "    mask[..., 2] = cv2.normalize(magnitude, None, 0, 255, cv2.NORM_MINMAX)\n",
    "    rgb = cv2.cvtColor(mask, cv2.COLOR_HSV2BGR)\n",
    "    out.write(rgb)\n",
    "    print(f'\\rProgress: {i}', end='')\n",
    "    prev_gray = gray\n",
    "cap.release()\n",
    "out.release()\n",
    "cv2.destroyAllWindows()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "segment each worm in dense optical flow video"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture('dense_opti_flow_v2.mp4')\n",
    "frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "frame_rate = cap.get(cv2.CAP_PROP_FPS)\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')  # Try 'XVID' if 'mp4v' does not work\n",
    "out = cv2.VideoWriter('dense_opt_segmented.mp4', fourcc, frame_rate, (frame_width, frame_height), True)\n",
    "# Read the first frame\n",
    "ret, frame1 = cap.read()\n",
    "if not ret:\n",
    "    print(\"Failed to read video\")\n",
    "    cap.release()\n",
    "prvs = cv2.cvtColor(frame1, cv2.COLOR_BGR2GRAY)\n",
    "j = 0\n",
    "while True:\n",
    "    j += 1\n",
    "    ret, frame2 = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    next = cv2.cvtColor(frame2, cv2.COLOR_BGR2GRAY)\n",
    "    # Calculate Optical Flow\n",
    "    flow = cv2.calcOpticalFlowFarneback(prvs, next, None, 0.5, 3, 15, 3, 5, 1.2, 0)\n",
    "    # Compute magnitude and angle of the flow\n",
    "    mag, ang = cv2.cartToPolar(flow[..., 0], flow[..., 1])\n",
    "    # Create motion mask\n",
    "    thresh = 3  # Set threshold for motion detection\n",
    "    motion_mask = cv2.threshold(mag, thresh, 255, cv2.THRESH_BINARY)[1]\n",
    "    # Segment mask based on connected components\n",
    "    num_labels, labels, stats, centroids = cv2.connectedComponentsWithStats(np.uint8(motion_mask), connectivity=8)\n",
    "    min_area = 50  # Minimum area for connected components\n",
    "    # Draw bounding boxes around components\n",
    "    for i in range(1, num_labels):\n",
    "        x, y, w, h, area = stats[i]\n",
    "        if area > min_area:\n",
    "            cv2.rectangle(frame2, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "    # Display the result\n",
    "    out.write(frame2)\n",
    "    # cv2.imshow('Segmented Frame', frame2)\n",
    "    # Update previous frame\n",
    "    print(f'\\rProgress: {j}', end='')\n",
    "    prvs = next\n",
    "    # Break the loop if 'q' is pressed\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "cap.release()\n",
    "out.release()\n",
    "cv2.destroyAllWindows()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "segment each worm without optical flow processing"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture('pre_dense.mp4')\n",
    "frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "frame_rate = cap.get(cv2.CAP_PROP_FPS)\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')  # Try 'XVID' if 'mp4v' does not work\n",
    "out = cv2.VideoWriter('predense_degmented.mp4', fourcc, frame_rate, (frame_width, frame_height), True)\n",
    "# Read the first frame\n",
    "ret, frame1 = cap.read()\n",
    "if not ret:\n",
    "    print(\"Failed to read video\")\n",
    "    cap.release()\n",
    "prvs = cv2.cvtColor(frame1, cv2.COLOR_BGR2GRAY)\n",
    "j = 0\n",
    "while True:\n",
    "    j += 1\n",
    "    ret, frame2 = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    next = cv2.cvtColor(frame2, cv2.COLOR_BGR2GRAY)\n",
    "    # Calculate Optical Flow\n",
    "    flow = cv2.calcOpticalFlowFarneback(prvs, next, None, 0.5, 3, 15, 3, 5, 1.2, 0)\n",
    "    # Compute magnitude and angle of the flow\n",
    "    mag, ang = cv2.cartToPolar(flow[..., 0], flow[..., 1])\n",
    "    # Create motion mask\n",
    "    thresh = 3  # Set threshold for motion detection\n",
    "    motion_mask = cv2.threshold(mag, thresh, 255, cv2.THRESH_BINARY)[1]\n",
    "    # Segment mask based on connected components\n",
    "    num_labels, labels, stats, centroids = cv2.connectedComponentsWithStats(np.uint8(motion_mask), connectivity=8)\n",
    "    min_area = 50  # Minimum area for connected components\n",
    "    # Draw bounding boxes around components\n",
    "    for i in range(1, num_labels):\n",
    "        x, y, w, h, area = stats[i]\n",
    "        if area > min_area:\n",
    "            cv2.rectangle(frame2, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "    # Display the result\n",
    "    out.write(frame2)\n",
    "    # cv2.imshow('Segmented Frame', frame2)\n",
    "    # Update previous frame\n",
    "    print(f'\\rProgress: {j}', end='')\n",
    "    prvs = next\n",
    "    # Break the loop if 'q' is pressed\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "cap.release()\n",
    "out.release()\n",
    "cv2.destroyAllWindows()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Define a function for video to image and use it"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Define a function to extract and save frames from a video file\n",
    "current_location = os.getcwd();\n",
    "output_folder = current_location + '/raw_images'\n",
    "video_path = current_location + '/sample.avi'\n",
    "\n",
    "def video2image(video_path, output_folder):\n",
    "    # Create the output folder if it doesn't exist\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "\n",
    "    # Load the video\n",
    "    video = cv2.VideoCapture(video_path)\n",
    "\n",
    "    # Initialize frame count\n",
    "    count = 0\n",
    "\n",
    "    # Iterate through video frames\n",
    "    while True:\n",
    "        # Read a frame\n",
    "        success, frame = video.read()\n",
    "        # Break if no frame is read (end of video)\n",
    "        if not success:\n",
    "            break\n",
    "        # Save the frame as an image\n",
    "        cv2.imwrite(os.path.join(output_folder, f\"frame_{count}.jpg\"), frame)\n",
    "        # Increment frame count\n",
    "        count += 1\n",
    "\n",
    "    # Release the video object\n",
    "    video.release()\n",
    "\n",
    "    return count"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Save the video into images"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "frame_count = video2image(video_path, output_folder)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Design a crop function that remove the dish"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def crop_circle(image_path):\n",
    "    # Read the image\n",
    "    image = cv2.imread(image_path)\n",
    "    # Create a mask with the same dimensions as the image\n",
    "    mask = np.zeros_like(image)\n",
    "    rows, cols, _ = mask.shape\n",
    "    # Compute the center and radius of the circle\n",
    "    center = (cols // 2, rows // 2)\n",
    "    radius = min(center[0], center[1], rows - center[1], cols - center[0])\n",
    "    # Draw the circular mask\n",
    "    cv2.circle(mask, center, radius, (255, 255, 255), -1)\n",
    "    # Apply the mask\n",
    "    circular_image = cv2.bitwise_and(image, mask)\n",
    "    # Optionally, you can remove the black background\n",
    "    masked_data = cv2.cvtColor(circular_image, cv2.COLOR_BGR2BGRA)\n",
    "    masked_data[mask == 0] = [0, 0, 0, 0]\n",
    "    # Save or display the result\n",
    "    cv2.imwrite('circular_image.png', masked_data)\n",
    "    # cv2.imshow('Circular Image', masked_data)\n",
    "    # cv2.waitKey(0)\n",
    "    # cv2.destroyAllWindows()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "threshold the original video and set the threshold"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "thresh = 127"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "apply to original video"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Load the video\n",
    "cap = cv2.VideoCapture('sample.avi')\n",
    "frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "frame_rate = cap.get(cv2.CAP_PROP_FPS)\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')  # Try 'XVID' if 'mp4v' does not work\n",
    "out = cv2.VideoWriter('threshold_original.mp4', fourcc, frame_rate, (frame_width, frame_height), True)\n",
    "# Check if video opened successfully\n",
    "if not cap.isOpened():\n",
    "    print(\"Error opening video file\")\n",
    "# Read until video is completed\n",
    "j = 0\n",
    "while cap.isOpened():\n",
    "    j += 1\n",
    "    # Capture frame-by-frame\n",
    "    ret, frame = cap.read()\n",
    "    if ret:\n",
    "        # Convert to grayscale\n",
    "        # Apply threshold for each channel\n",
    "        _, red_channel = cv2.threshold(frame[:,:,0], thresh, 255, cv2.THRESH_BINARY)\n",
    "        _, green_channel = cv2.threshold(frame[:,:,1], thresh, 255, cv2.THRESH_BINARY)\n",
    "        _, blue_channel = cv2.threshold(frame[:,:,2], thresh, 255, cv2.THRESH_BINARY)\n",
    "    # Combine the channels back\n",
    "        thresh_frame = cv2.merge([red_channel, green_channel, blue_channel])\n",
    "        # Display the resulting frame\n",
    "        out.write(thresh_frame)\n",
    "        # Press Q on keyboard to exit\n",
    "        if cv2.waitKey(25) & 0xFF == ord('q'):\n",
    "            break\n",
    "        print(f'\\rProgress: {j}', end='')\n",
    "    else:\n",
    "        break\n",
    "# When everything done, release the video capture object\n",
    "cap.release()\n",
    "out.release()\n",
    "cv2.destroyAllWindows()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "apply erode and dilate"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Load the video\n",
    "cap = cv2.VideoCapture('pre_dense.mp4')\n",
    "kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (7, 7))\n",
    "frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "frame_rate = cap.get(cv2.CAP_PROP_FPS)\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')  # Try 'XVID' if 'mp4v' does not work\n",
    "out = cv2.VideoWriter('pre_dense_erode_dilate.mp4', fourcc, frame_rate, (frame_width, frame_height), True)\n",
    "j = 0\n",
    "while True:\n",
    "    j += 1\n",
    "    # Read each frame\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    # Apply erosion and then dilation\n",
    "    eroded_frame = cv2.erode(frame, kernel, iterations=1)\n",
    "    dilated_frame = cv2.dilate(eroded_frame, kernel, iterations=1)\n",
    "    # Display the processed frame\n",
    "    out.write(dilated_frame)\n",
    "    print(f'\\rProgress: {j}', end='')\n",
    "    # Break the loop with a key press\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release resources\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "out.release()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "motion segmentation using connectivity after erode and dilate"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture('pre_dense_erode_dilate.mp4')\n",
    "frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "frame_rate = cap.get(cv2.CAP_PROP_FPS)\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "out = cv2.VideoWriter('pre_dense_erode_dilate_segmented.mp4', fourcc, frame_rate, (frame_width, frame_height), True)\n",
    "# Read the first frame\n",
    "ret, frame1 = cap.read()\n",
    "if not ret:\n",
    "    print(\"Failed to read video\")\n",
    "    cap.release()\n",
    "prvs = cv2.cvtColor(frame1, cv2.COLOR_BGR2GRAY)\n",
    "j = 0\n",
    "while True:\n",
    "    j += 1\n",
    "    ret, frame2 = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    next = cv2.cvtColor(frame2, cv2.COLOR_BGR2GRAY)\n",
    "    # Calculate Optical Flow\n",
    "    flow = cv2.calcOpticalFlowFarneback(prvs, next, None, 0.5, 3, 15, 3, 5, 1.2, 0)\n",
    "    # Compute magnitude and angle of the flow\n",
    "    mag, ang = cv2.cartToPolar(flow[..., 0], flow[..., 1])\n",
    "    # Create motion mask\n",
    "    thresh = 1  # Set threshold for motion detection\n",
    "    motion_mask = cv2.threshold(mag, thresh, 255, cv2.THRESH_BINARY)[1]\n",
    "    cv2.imshow('Segmented Frame', motion_mask)\n",
    "    # Segment mask based on connected components\n",
    "    num_labels, labels, stats, centroids = cv2.connectedComponentsWithStats(np.uint8(motion_mask), connectivity=8)\n",
    "    min_area = 300  # Minimum area for connected components\n",
    "    # Draw bounding boxes around components\n",
    "    for i in range(1, num_labels):\n",
    "        x, y, w, h, area = stats[i]\n",
    "        if area > min_area:\n",
    "            cv2.rectangle(frame2, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "    # cv2.imshow('Segmented Frame', frame2)\n",
    "    # Display the result\n",
    "    out.write(frame2)\n",
    "    # Update previous frame\n",
    "    print(f'\\rProgress: {j}', end='')\n",
    "    prvs = next\n",
    "    # Break the loop if 'q' is pressed\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "cap.release()\n",
    "out.release()\n",
    "cv2.destroyAllWindows()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "erode and dilate also motion mask, save motion mask actually"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture('pre_dense_erode_dilate.mp4')\n",
    "frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "frame_rate = cap.get(cv2.CAP_PROP_FPS)\n",
    "erode_kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (2, 2))\n",
    "dilate_kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (15, 15))\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "out = cv2.VideoWriter('motion_mask_segmented.mp4', fourcc, frame_rate, (frame_width, frame_height), True)\n",
    "# Read the first frame\n",
    "ret, frame1 = cap.read()\n",
    "if not ret:\n",
    "    print(\"Failed to read video\")\n",
    "    cap.release()\n",
    "prvs = cv2.cvtColor(frame1, cv2.COLOR_BGR2GRAY)\n",
    "j = 0\n",
    "while True:\n",
    "    j += 1\n",
    "    ret, frame2 = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    next = cv2.cvtColor(frame2, cv2.COLOR_BGR2GRAY)\n",
    "    # Calculate Optical Flow\n",
    "    flow = cv2.calcOpticalFlowFarneback(prvs, next, None, 0.5, 3, 15, 3, 5, 1.2, 0)\n",
    "    # Compute magnitude and angle of the flow\n",
    "    mag, ang = cv2.cartToPolar(flow[..., 0], flow[..., 1])\n",
    "    # Create motion mask\n",
    "    thresh = 1  # Set threshold for motion detection\n",
    "    motion_mask = cv2.threshold(mag, thresh, 255, cv2.THRESH_BINARY)[1]\n",
    "    eroded_frame = cv2.erode(motion_mask, erode_kernel, iterations=1)\n",
    "    dilated_frame = cv2.dilate(eroded_frame, dilate_kernel, iterations=1)\n",
    "    # cv2.imshow('Segmented Frame', dilated_frame)\n",
    "    # Segment mask based on connected components\n",
    "    num_labels, labels, stats, centroids = cv2.connectedComponentsWithStats(np.uint8(dilated_frame), connectivity=8)\n",
    "    min_area = 300  # Minimum area for connected components\n",
    "    # Draw bounding boxes around components\n",
    "    for i in range(1, num_labels):\n",
    "        x, y, w, h, area = stats[i]\n",
    "        if area > min_area:\n",
    "            cv2.rectangle(frame2, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "    for i in range(1, num_labels):\n",
    "        x, y, w, h, area = stats[i]\n",
    "        if area > min_area:\n",
    "            cv2.rectangle(dilated_frame, (x, y), (x + w, y + h), (255, 255, 255), 2)\n",
    "    save_frame = cv2.normalize(dilated_frame, None, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX, dtype=cv2.CV_8U)\n",
    "    # cv2.imshow('Segmented Frame', dilated_frame)\n",
    "    # cv2.imshow('Segmented Frame', frame2)\n",
    "    # Display the result\n",
    "    rgb_frame = cv2.cvtColor(save_frame, cv2.COLOR_GRAY2RGB)\n",
    "    # inspect_given_a_frame(frame2)\n",
    "    # inspect_given_a_frame(rgb_frame)\n",
    "    out.write(save_frame)\n",
    "    # Update previous frame\n",
    "    print(f'\\rProgress: {j}', end='')\n",
    "    prvs = next\n",
    "    # Break the loop if 'q' is pressed\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "cap.release()\n",
    "out.release()\n",
    "cv2.destroyAllWindows()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture('pre_dense_erode_dilate.mp4')\n",
    "frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "frame_rate = cap.get(cv2.CAP_PROP_FPS)\n",
    "erode_kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (2, 2))\n",
    "dilate_kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (15, 15))\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "out = cv2.VideoWriter('motion_mask_segmented_v2.mp4', fourcc, frame_rate, (frame_width, frame_height), True)\n",
    "# Read the first frame\n",
    "ret, frame1 = cap.read()\n",
    "if not ret:\n",
    "    print(\"Failed to read video\")\n",
    "    cap.release()\n",
    "prvs = cv2.cvtColor(frame1, cv2.COLOR_BGR2GRAY)\n",
    "j = 0\n",
    "while True:\n",
    "    j += 1\n",
    "    ret, frame2 = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    next = cv2.cvtColor(frame2, cv2.COLOR_BGR2GRAY)\n",
    "    # Calculate Optical Flow\n",
    "    flow = cv2.calcOpticalFlowFarneback(prvs, next, None, 0.5, 3, 15, 3, 5, 1.2, 0)\n",
    "    # Compute magnitude and angle of the flow\n",
    "    mag, ang = cv2.cartToPolar(flow[..., 0], flow[..., 1])\n",
    "    # Create motion mask\n",
    "    thresh = 1  # Set threshold for motion detection\n",
    "    motion_mask = cv2.threshold(mag, thresh, 255, cv2.THRESH_BINARY)[1]\n",
    "    eroded_frame = cv2.erode(motion_mask, erode_kernel, iterations=1)\n",
    "    dilated_frame = cv2.dilate(eroded_frame, dilate_kernel, iterations=1)\n",
    "    # cv2.imshow('Segmented Frame', dilated_frame)\n",
    "    # Segment mask based on connected components\n",
    "    num_labels, labels, stats, centroids = cv2.connectedComponentsWithStats(np.uint8(dilated_frame), connectivity=8)\n",
    "    min_area = 300  # Minimum area for connected components\n",
    "    # Draw bounding boxes around components\n",
    "    for i in range(1, num_labels):\n",
    "        x, y, w, h, area = stats[i]\n",
    "        if area > min_area:\n",
    "            cv2.rectangle(frame2, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "    for i in range(1, num_labels):\n",
    "        x, y, w, h, area = stats[i]\n",
    "        if area > min_area:\n",
    "            cv2.rectangle(dilated_frame, (x, y), (x + w, y + h), (255, 255, 255), 2)\n",
    "    t_frame = cv2.normalize(dilated_frame, None, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX, dtype=cv2.CV_8U)\n",
    "    # cv2.imshow('Segmented Frame', dilated_frame)\n",
    "    # cv2.imshow('Segmented Frame', frame2)\n",
    "    # Display the result\n",
    "    rgb_frame = cv2.cvtColor(t_frame, cv2.COLOR_GRAY2RGB)\n",
    "    # inspect_given_a_frame(frame2)\n",
    "    # inspect_given_a_frame(rgb_frame)\n",
    "    # inspect_given_a_frame(rgb_frame)\n",
    "    out.write(rgb_frame)\n",
    "    # Update previous frame\n",
    "    print(f'\\rProgress: {j}', end='')\n",
    "    prvs = next\n",
    "    # Break the loop if 'q' is pressed\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "cap.release()\n",
    "out.release()\n",
    "cv2.destroyAllWindows()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "dilate even more for segmentation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Overlay"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "original_cap = cv2.VideoCapture('sample.avi')\n",
    "cap = cv2.VideoCapture('pre_dense_erode_dilate.mp4')\n",
    "frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "frame_rate = cap.get(cv2.CAP_PROP_FPS)\n",
    "erode_kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (2, 2))\n",
    "dilate_kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (45, 45))\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "out = cv2.VideoWriter('overlay.mp4', fourcc, frame_rate, (frame_width, frame_height), True)\n",
    "# Read the first frame\n",
    "ret, frame1 = cap.read()\n",
    "not_useful, ori_frame = original_cap.read()\n",
    "if not ret:\n",
    "    print(\"Failed to read video\")\n",
    "    cap.release()\n",
    "prvs = cv2.cvtColor(frame1, cv2.COLOR_BGR2GRAY)\n",
    "j = 0\n",
    "while True:\n",
    "    j += 1\n",
    "    ret, frame2 = cap.read()\n",
    "    not_useful, ori_frame = original_cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    next = cv2.cvtColor(frame2, cv2.COLOR_BGR2GRAY)\n",
    "    # Calculate Optical Flow\n",
    "    flow = cv2.calcOpticalFlowFarneback(prvs, next, None, 0.5, 3, 15, 3, 5, 1.2, 0)\n",
    "    # Compute magnitude and angle of the flow\n",
    "    mag, ang = cv2.cartToPolar(flow[..., 0], flow[..., 1])\n",
    "    # Create motion mask\n",
    "    thresh = 1  # Set threshold for motion detection\n",
    "    motion_mask = cv2.threshold(mag, thresh, 255, cv2.THRESH_BINARY)[1]\n",
    "    eroded_frame = cv2.erode(motion_mask, erode_kernel, iterations=1)\n",
    "    dilated_frame = cv2.dilate(eroded_frame, dilate_kernel, iterations=1)\n",
    "    num_labels, labels, stats, centroids = cv2.connectedComponentsWithStats(np.uint8(dilated_frame), connectivity=8)\n",
    "    min_area = 300\n",
    "    for i in range(1, num_labels):\n",
    "        x, y, w, h, area = stats[i]\n",
    "        if area > min_area:\n",
    "            cv2.rectangle(frame2, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "    for i in range(1, num_labels):\n",
    "        x, y, w, h, area = stats[i]\n",
    "        if area > min_area:\n",
    "            cv2.rectangle(dilated_frame, (x, y), (x + w, y + h), (255, 255, 255), 2)\n",
    "    for i in range(1, num_labels):\n",
    "        x, y, w, h, area = stats[i]\n",
    "        if area > min_area:\n",
    "            cv2.rectangle(ori_frame, (x, y), (x + w, y + h), (250,128,114), 2)\n",
    "    temp_frame = cv2.normalize(dilated_frame, None, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX, dtype=cv2.CV_8U)\n",
    "    rgb_frame = cv2.cvtColor(temp_frame, cv2.COLOR_GRAY2RGB)\n",
    "    out.write(ori_frame)\n",
    "    print(f'\\rProgress: {j}', end='')\n",
    "    prvs = next\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "cap.release()\n",
    "out.release()\n",
    "cv2.destroyAllWindows()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Segmentation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Initialize Paths"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "current_location = os.getcwd();\n",
    "output_folder = current_location + '/worm_segmentation'\n",
    "video_path = current_location + '/sample.avi'"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Initialize input videos and output videos"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "motion_mask_cap = cv2.VideoCapture('pre_dense_erode_dilate.mp4')\n",
    "original_video_cap = cv2.VideoCapture('sample.avi')\n",
    "frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "frame_rate = cap.get(cv2.CAP_PROP_FPS)\n",
    "erode_kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (2, 2))\n",
    "dilate_kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (45, 45))\n",
    "f_worm_1 = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "f_worm_2 = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "f_worm_3 = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "f_worm_4 = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "f_worm_5 = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "f_worm_6 = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "f_worm_7 = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "f_worm_8 = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "f_worm_9 = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "f_worm_10 = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "worm_1_out = cv2.VideoWriter(output_folder + '/worm_1_out.mp4', f_worm_1, frame_rate, (frame_width, frame_height), True)\n",
    "worm_2_out = cv2.VideoWriter(output_folder + '/worm_2_out.mp4', f_worm_2, frame_rate, (frame_width, frame_height), True)\n",
    "worm_3_out = cv2.VideoWriter(output_folder + '/worm_3_out.mp4', f_worm_3, frame_rate, (frame_width, frame_height), True)\n",
    "worm_4_out = cv2.VideoWriter(output_folder + '/worm_4_out.mp4', f_worm_4, frame_rate, (frame_width, frame_height), True)\n",
    "worm_5_out = cv2.VideoWriter(output_folder + '/worm_5_out.mp4', f_worm_5, frame_rate, (frame_width, frame_height), True)\n",
    "worm_6_out = cv2.VideoWriter(output_folder + '/worm_6_out.mp4', f_worm_6, frame_rate, (frame_width, frame_height), True)\n",
    "worm_7_out = cv2.VideoWriter(output_folder + '/worm_7_out.mp4', f_worm_7, frame_rate, (frame_width, frame_height), True)\n",
    "worm_8_out = cv2.VideoWriter(output_folder + '/worm_8_out.mp4', f_worm_8, frame_rate, (frame_width, frame_height), True)\n",
    "worm_9_out = cv2.VideoWriter(output_folder + '/worm_9_out.mp4', f_worm_9, frame_rate, (frame_width, frame_height), True)\n",
    "worm_10_out = cv2.VideoWriter(output_folder + '/worm_10_out.mp4', f_worm_10, frame_rate, (frame_width, frame_height), True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "clean up make sure all number is good"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "total_it_number = 0\n",
    "\n",
    "if not motion_mask_cap.isOpened():\n",
    "    print(\"Error: Could not open motion mask.\")\n",
    "else:\n",
    "    # Get the total number of frames in the video\n",
    "    total_it_number = int(motion_mask_cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    print(f\"Total number of frames in the motion mask: {total_it_number}\")\n",
    "\n",
    "if not original_video_cap.isOpened():\n",
    "    print(\"Error: Could not open motion mask.\")\n",
    "else:\n",
    "    # Get the total number of frames in the video\n",
    "    total_it_number = int(original_video_cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    print(f\"Total number of frames in the original video: {total_it_number}\")\n",
    "\n",
    "print(f\"to process the video need to iterate over: {total_it_number}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Read the first motion\n",
    "ret, motion_frame_init = motion_mask_cap.read()\n",
    "if not ret:\n",
    "    print(\"Failed to read the motion ask\")\n",
    "    cap.release()\n",
    "else:\n",
    "    print(\"motion frame read\")\n",
    "print(\"Shape of Frame\", motion_frame_init.shape)\n",
    "print(\"First Row\", motion_frame_init[:,1])\n",
    "previous_motion_frame = cv2.cvtColor(motion_frame_init, cv2.COLOR_BGR2GRAY)\n",
    "inspect_given_a_frame(previous_motion_frame)\n",
    "print(\"Shape of Frame\", previous_motion_frame.shape)\n",
    "print(\"First Row\", previous_motion_frame[:,1].shape)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "pilot the code a bit"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for j in range(total_it_number):\n",
    "    ret, temporal_motion_frame = motion_mask_cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    current_motion_frame = cv2.cvtColor(temporal_motion_frame, cv2.COLOR_BGR2GRAY)\n",
    "    # Calculate Optical Flow\n",
    "    flow = cv2.calcOpticalFlowFarneback(previous_motion_frame, current_motion_frame, None, 0.5, 3, 15, 3, 5, 1.2, 0)\n",
    "    # Compute magnitude and angle of the flow\n",
    "    mag, ang = cv2.cartToPolar(flow[..., 0], flow[..., 1])\n",
    "    # Create motion mask\n",
    "    thresh = 1  # Set threshold for motion detection\n",
    "    motion_mask = cv2.threshold(mag, thresh, 255, cv2.THRESH_BINARY)[1]\n",
    "    eroded_frame = cv2.erode(motion_mask, erode_kernel, iterations=1)\n",
    "    dilated_frame = cv2.dilate(eroded_frame, dilate_kernel, iterations=1)\n",
    "    num_labels, labels, stats, centroids = cv2.connectedComponentsWithStats(np.uint8(dilated_frame), connectivity=8)\n",
    "    print('Number of labels', num_labels, '\\n')\n",
    "    print('labels', labels, '\\n')\n",
    "    print('stats', stats, '\\n')\n",
    "    print('centroids', centroids, '\\n')\n",
    "    min_area = 5000\n",
    "    for i in range(1, num_labels):\n",
    "        x, y, w, h, area = stats[i]\n",
    "        if area > min_area:\n",
    "            cv2.rectangle(frame2, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "    for i in range(1, num_labels):\n",
    "        x, y, w, h, area = stats[i]\n",
    "        if area > min_area:\n",
    "            cv2.rectangle(dilated_frame, (x, y), (x + w, y + h), (255, 255, 255), 2)\n",
    "    temp_frame = cv2.normalize(dilated_frame, None, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX, dtype=cv2.CV_8U)\n",
    "    rgb_frame = cv2.cvtColor(temp_frame, cv2.COLOR_GRAY2RGB)\n",
    "    cv2.imshow('Segmented Frame', rgb_frame)\n",
    "    out.write(rgb_frame)\n",
    "    print(f'\\rProgress: {j}', end='')\n",
    "    prvs = next\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tye = num_labels, labels, stats, centroids"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "num_la, la, st, cent = tye"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(type(num_la))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(centroids[2][0])\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(st)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(st[1][0])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(centroids.shape)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "stats_2 = stats\n",
    "print(stats_2)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "hum = stats"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "hum = np.append(hum, stats_2)\n",
    "print(hum)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "make an object array and work with that"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "hum = np.array(None,dtype=object)\n",
    "print(hum)\n",
    "hum = np.append(hum, stats_2)\n",
    "print(hum)\n",
    "hum = np.append(hum, stats_2)\n",
    "print(hum)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(tye[0])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(tye[1])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(tye[2])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(tye[3])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Construction of a Worm Object\n",
    "## the idea is that, to make a worm object, this object should be able to take each frame as input,\n",
    "## find the motion mask to correspond to the worm, and append to the worm object"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class Worm:\n",
    "    def __init__(self, worm_index, original_cap, label_history, stats_history, centroid_history, another_variable_in_case=None):\n",
    "        self.worm_index = worm_index\n",
    "        self.original_cap = original_cap\n",
    "        self.label_history = label_history\n",
    "        self.stats_history = stats_history\n",
    "        self.centroid_history = centroid_history\n",
    "        self.another_variable_in_case = another_variable_in_case\n",
    "\n",
    "    def display_info(self):\n",
    "        info = f\"worn index: {self.worm_index}, video_cap: {self.original_cap}, label_history: {self.label_history}, stats_history: {self.stats_history}, centroid_history: {self.centroid_history}\"\n",
    "        print(info)\n",
    "\n",
    "    def update_another_variable(self, another_variable_in_case):\n",
    "        \"\"\"Update the car's mileage.\"\"\"\n",
    "        if another_variable_in_case >= self.another_variable_in_case:\n",
    "            self.another_variable_in_case = another_variable_in_case\n",
    "        else:\n",
    "            print(\"Error: another_variable_in_case cannot be reduced.\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Pilot with Single Worm first before doing 10"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Basic set up"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "current_location = os.getcwd();\n",
    "output_folder = current_location + '/worm_segmentation'\n",
    "video_path = current_location + '/sample.avi'\n",
    "if not os.path.exists(output_folder):\n",
    "    os.makedirs(output_folder)\n",
    "motion_mask_cap = cv2.VideoCapture('pre_dense_erode_dilate.mp4')\n",
    "original_video_cap = cv2.VideoCapture('sample.avi')\n",
    "frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "frame_rate = cap.get(cv2.CAP_PROP_FPS)\n",
    "erode_kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (2, 2))\n",
    "dilate_kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (45, 45))\n",
    "f_worm_1 = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "worm_1_out = cv2.VideoWriter(output_folder + '/worm_1_out.mp4', f_worm_1, frame_rate, (frame_width, frame_height), True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "loop through the first 1/10 of the video"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "total_it_number = 0\n",
    "\n",
    "if not motion_mask_cap.isOpened():\n",
    "    print(\"Error: Could not open motion mask.\")\n",
    "else:\n",
    "    # Get the total number of frames in the video\n",
    "    total_it_number = int(motion_mask_cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    print(f\"Total number of frames in the motion mask: {total_it_number}\")\n",
    "\n",
    "if not original_video_cap.isOpened():\n",
    "    print(\"Error: Could not open motion mask.\")\n",
    "else:\n",
    "    # Get the total number of frames in the video\n",
    "    total_it_number = int(original_video_cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    print(f\"Total number of frames in the original video: {total_it_number}\")\n",
    "\n",
    "print(f\"to process the video need to iterate over: {total_it_number}\")\n",
    "\n",
    "total_it_number = int(total_it_number / 10)\n",
    "\n",
    "print(f\"to process the video need to iterate over: {total_it_number}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "actually, construct the worm object before looping, that means, I need to make the first frame"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Read the first motion\n",
    "ret, motion_frame_init = motion_mask_cap.read()\n",
    "if not ret:\n",
    "    print(\"Failed to read the motion ask\")\n",
    "    cap.release()\n",
    "else:\n",
    "    print(\"motion frame read\")\n",
    "print(\"Shape of Frame\", motion_frame_init.shape)\n",
    "print(\"First Row\", motion_frame_init[:,1])\n",
    "previous_motion_frame = cv2.cvtColor(motion_frame_init, cv2.COLOR_BGR2GRAY)\n",
    "inspect_given_a_frame(previous_motion_frame)\n",
    "print(\"Shape of Frame\", previous_motion_frame.shape)\n",
    "print(\"First Row\", previous_motion_frame[:,1].shape)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "for worm initialization, let us analyze frame by frame"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "ret, current_motion_frame = motion_mask_cap.read()\n",
    "current_motion_frame = cv2.cvtColor(current_motion_frame, cv2.COLOR_BGR2GRAY)\n",
    "# Calculate Optical Flow\n",
    "flow = cv2.calcOpticalFlowFarneback(previous_motion_frame, current_motion_frame, None, 0.5, 3, 15, 3, 5, 1.2, 0)\n",
    "# Compute magnitude and angle of the flow\n",
    "mag, ang = cv2.cartToPolar(flow[..., 0], flow[..., 1])\n",
    "# Create motion mask\n",
    "thresh = 1  # Set threshold for motion detection\n",
    "motion_mask = cv2.threshold(mag, thresh, 255, cv2.THRESH_BINARY)[1]\n",
    "eroded_frame = cv2.erode(motion_mask, erode_kernel, iterations=1)\n",
    "dilated_frame = cv2.dilate(eroded_frame, dilate_kernel, iterations=1)\n",
    "num_labels, labels, stats, centroids = cv2.connectedComponentsWithStats(np.uint8(dilated_frame), connectivity=8)\n",
    "print('Number of labels', num_labels, '\\n')\n",
    "print('labels', labels, '\\n')\n",
    "print('stats', stats, '\\n')\n",
    "print('centroids', centroids, '\\n')\n",
    "min_area = 5000\n",
    "for i in range(1, num_labels):\n",
    "    x, y, w, h, area = stats[i]\n",
    "    if area > min_area:\n",
    "        cv2.rectangle(frame2, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "for i in range(1, num_labels):\n",
    "    x, y, w, h, area = stats[i]\n",
    "    if area > min_area:\n",
    "        cv2.rectangle(dilated_frame, (x, y), (x + w, y + h), (255, 255, 255), 2)\n",
    "previous_motion_frame = current_motion_frame"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(sum(sum(labels)))\n",
    "print('Number of labels', num_labels)\n",
    "\n",
    "# Create the heatmap\n",
    "plt.imshow(labels, cmap='hot', interpolation='nearest')\n",
    "\n",
    "# Adding a colorbar\n",
    "plt.colorbar()\n",
    "\n",
    "# Adding titles and labels (optional)\n",
    "plt.title('Heatmap of a 2D NumPy Array')\n",
    "plt.xlabel('X-axis Label')\n",
    "plt.ylabel('Y-axis Label')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "let us look at next frame"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "ret, current_motion_frame = motion_mask_cap.read()\n",
    "current_motion_frame = cv2.cvtColor(current_motion_frame, cv2.COLOR_BGR2GRAY)\n",
    "# Calculate Optical Flow\n",
    "flow = cv2.calcOpticalFlowFarneback(previous_motion_frame, current_motion_frame, None, 0.5, 3, 15, 3, 5, 1.2, 0)\n",
    "# Compute magnitude and angle of the flow\n",
    "mag, ang = cv2.cartToPolar(flow[..., 0], flow[..., 1])\n",
    "# Create motion mask\n",
    "thresh = 1  # Set threshold for motion detection\n",
    "motion_mask = cv2.threshold(mag, thresh, 255, cv2.THRESH_BINARY)[1]\n",
    "eroded_frame = cv2.erode(motion_mask, erode_kernel, iterations=1)\n",
    "dilated_frame = cv2.dilate(eroded_frame, dilate_kernel, iterations=1)\n",
    "num_labels, labels, stats, centroids = cv2.connectedComponentsWithStats(np.uint8(dilated_frame), connectivity=8)\n",
    "print('Number of labels', num_labels, '\\n')\n",
    "print('labels', labels, '\\n')\n",
    "print('stats', stats, '\\n')\n",
    "print('centroids', centroids, '\\n')\n",
    "min_area = 5000\n",
    "for i in range(1, num_labels):\n",
    "    x, y, w, h, area = stats[i]\n",
    "    if area > min_area:\n",
    "        cv2.rectangle(frame2, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "for i in range(1, num_labels):\n",
    "    x, y, w, h, area = stats[i]\n",
    "    if area > min_area:\n",
    "        cv2.rectangle(dilated_frame, (x, y), (x + w, y + h), (255, 255, 255), 2)\n",
    "previous_motion_frame = current_motion_frame"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(sum(sum(labels)))\n",
    "print('Number of labels', num_labels)\n",
    "\n",
    "# Create the heatmap\n",
    "plt.imshow(labels, cmap='hot', interpolation='nearest')\n",
    "\n",
    "# Adding a colorbar\n",
    "plt.colorbar()\n",
    "\n",
    "# Adding titles and labels (optional)\n",
    "plt.title('Heatmap of a 2D NumPy Array')\n",
    "plt.xlabel('X-axis Label')\n",
    "plt.ylabel('Y-axis Label')\n",
    "\n",
    "# Show\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "standard initialization"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# reset cap\n",
    "cap.set(cv2.CAP_PROP_POS_FRAMES, 0)\n",
    "# Read the first motion\n",
    "ret, motion_frame_init = motion_mask_cap.read()\n",
    "if not ret:\n",
    "    print(\"Failed to read the motion ask\")\n",
    "    cap.release()\n",
    "else:\n",
    "    print(\"motion frame read\")\n",
    "print(\"Shape of Frame\", motion_frame_init.shape)\n",
    "print(\"First Row\", motion_frame_init[:,1])\n",
    "previous_motion_frame = cv2.cvtColor(motion_frame_init, cv2.COLOR_BGR2GRAY)\n",
    "inspect_given_a_frame(previous_motion_frame)\n",
    "print(\"Shape of Frame\", previous_motion_frame.shape)\n",
    "print(\"First Row\", previous_motion_frame[:,1].shape)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "loop through the video"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for j in range(total_it_number):\n",
    "    ret, temporal_motion_frame = motion_mask_cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    current_motion_frame = cv2.cvtColor(temporal_motion_frame, cv2.COLOR_BGR2GRAY)\n",
    "    # Calculate Optical Flow\n",
    "    flow = cv2.calcOpticalFlowFarneback(previous_motion_frame, current_motion_frame, None, 0.5, 3, 15, 3, 5, 1.2, 0)\n",
    "    # Compute magnitude and angle of the flow\n",
    "    mag, ang = cv2.cartToPolar(flow[..., 0], flow[..., 1])\n",
    "    # Create motion mask\n",
    "    thresh = 1  # Set threshold for motion detection\n",
    "    motion_mask = cv2.threshold(mag, thresh, 255, cv2.THRESH_BINARY)[1]\n",
    "    eroded_frame = cv2.erode(motion_mask, erode_kernel, iterations=1)\n",
    "    dilated_frame = cv2.dilate(eroded_frame, dilate_kernel, iterations=1)\n",
    "    num_labels, labels, stats, centroids = cv2.connectedComponentsWithStats(np.uint8(dilated_frame), connectivity=8)\n",
    "    print('Number of labels', num_labels, '\\n')\n",
    "    print('labels', labels, '\\n')\n",
    "    print('stats', stats, '\\n')\n",
    "    print('centroids', centroids, '\\n')\n",
    "    min_area = 5000\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    for i in range(1, num_labels):\n",
    "        x, y, w, h, area = stats[i]\n",
    "        if area > min_area:\n",
    "            cv2.rectangle(frame2, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "    for i in range(1, num_labels):\n",
    "        x, y, w, h, area = stats[i]\n",
    "        if area > min_area:\n",
    "            cv2.rectangle(dilated_frame, (x, y), (x + w, y + h), (255, 255, 255), 2)\n",
    "    temp_frame = cv2.normalize(dilated_frame, None, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX, dtype=cv2.CV_8U)\n",
    "    rgb_frame = cv2.cvtColor(temp_frame, cv2.COLOR_GRAY2RGB)\n",
    "    cv2.imshow('Segmented Frame', rgb_frame)\n",
    "    out.write(rgb_frame)\n",
    "    print(f'\\rProgress: {j}', end='')\n",
    "    prvs = next\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Cut to 1/4 of the original for easy processing"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Initialize video capture\n",
    "cap = cv2.VideoCapture('sample.avi')\n",
    "# Get video properties\n",
    "frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "frame_rate = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "# Define codec and create VideoWriter object\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "out = cv2.VideoWriter('shorter.mp4', fourcc, frame_rate, (frame_width, frame_height), True)\n",
    "for i in range(int(frame_count/4)):\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    out.write(frame)\n",
    "    print(f'\\rProgress: {i}', end='')\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "cap.release()\n",
    "out.release()\n",
    "cv2.destroyAllWindows()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Background Subtraction, Spatial Smoothing, Temporal Smoothing"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Initialize video capture\n",
    "cap = cv2.VideoCapture('shorter.mp4')\n",
    "# Get video properties\n",
    "frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "frame_rate = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "# Define codec and create VideoWriter object\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v') # You can also use 'XVID'\n",
    "out = cv2.VideoWriter('processed_v1.mp4', fourcc, frame_rate, (frame_width, frame_height), False)\n",
    "# Background subtractor\n",
    "backSub = cv2.createBackgroundSubtractorMOG2()\n",
    "# Buffer for temporal smoothing\n",
    "buffer_size = 5\n",
    "frame_buffer = []\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    # Background subtraction\n",
    "    fgMask = backSub.apply(frame)\n",
    "    # Spatial smoothing (Gaussian blur)\n",
    "    blurred = cv2.GaussianBlur(fgMask, (5, 5), 0)\n",
    "    # Add frame to buffer for temporal smoothing\n",
    "    frame_buffer.append(blurred)\n",
    "    if len(frame_buffer) > buffer_size:\n",
    "        frame_buffer.pop(0)\n",
    "    # Temporal smoothing (average of frames in buffer)\n",
    "    temp_smoothed = np.mean(frame_buffer, axis=0).astype(np.uint8)\n",
    "    # Write frame to video\n",
    "    out.write(temp_smoothed)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "# Release everything\n",
    "cap.release()\n",
    "out.release()\n",
    "cv2.destroyAllWindows()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Erode and Dilate"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Load the video\n",
    "cap = cv2.VideoCapture('processed_v1.mp4')\n",
    "kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (7, 7))\n",
    "frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "frame_rate = cap.get(cv2.CAP_PROP_FPS)\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "out = cv2.VideoWriter('processed_v2.mp4', fourcc, frame_rate, (frame_width, frame_height), True)\n",
    "j = 0\n",
    "while True:\n",
    "    j += 1\n",
    "    # Read each frame\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    # Apply erosion and then dilation\n",
    "    eroded_frame = cv2.erode(frame, kernel, iterations=1)\n",
    "    dilated_frame = cv2.dilate(eroded_frame, kernel, iterations=1)\n",
    "    # Display the processed frame\n",
    "    out.write(dilated_frame)\n",
    "    print(f'\\rProgress: {j}', end='')\n",
    "    # Break the loop with a key press\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release resources\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "out.release()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Overlay"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "original_cap = cv2.VideoCapture('shorter.mp4')\n",
    "cap = cv2.VideoCapture('processed_v2.mp4')\n",
    "frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "frame_rate = cap.get(cv2.CAP_PROP_FPS)\n",
    "erode_kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (2, 2))\n",
    "dilate_kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (75, 75))\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "out = cv2.VideoWriter('processed_v3.mp4', fourcc, frame_rate, (frame_width, frame_height), True)\n",
    "# Read the first frame\n",
    "ret, frame1 = cap.read()\n",
    "not_useful, ori_frame = original_cap.read()\n",
    "if not ret:\n",
    "    print(\"Failed to read video\")\n",
    "    cap.release()\n",
    "prvs = cv2.cvtColor(frame1, cv2.COLOR_BGR2GRAY)\n",
    "j = 0\n",
    "while True:\n",
    "    j += 1\n",
    "    ret, frame2 = cap.read()\n",
    "    not_useful, ori_frame = original_cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    next = cv2.cvtColor(frame2, cv2.COLOR_BGR2GRAY)\n",
    "    # Calculate Optical Flow\n",
    "    flow = cv2.calcOpticalFlowFarneback(prvs, next, None, 0.5, 3, 15, 3, 5, 1.2, 0)\n",
    "    # Compute magnitude and angle of the flow\n",
    "    mag, ang = cv2.cartToPolar(flow[..., 0], flow[..., 1])\n",
    "    # Create motion mask\n",
    "    thresh = 1  # Set threshold for motion detection\n",
    "    motion_mask = cv2.threshold(mag, thresh, 255, cv2.THRESH_BINARY)[1]\n",
    "    eroded_frame = cv2.erode(motion_mask, erode_kernel, iterations=1)\n",
    "    dilated_frame = cv2.dilate(eroded_frame, dilate_kernel, iterations=1)\n",
    "    num_labels, labels, stats, centroids = cv2.connectedComponentsWithStats(np.uint8(dilated_frame), connectivity=8)\n",
    "    min_area = 300\n",
    "    for i in range(1, num_labels):\n",
    "        x, y, w, h, area = stats[i]\n",
    "        if area > min_area:\n",
    "            cv2.rectangle(frame2, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "    for i in range(1, num_labels):\n",
    "        x, y, w, h, area = stats[i]\n",
    "        if area > min_area:\n",
    "            cv2.rectangle(dilated_frame, (x, y), (x + w, y + h), (255, 255, 255), 2)\n",
    "    for i in range(1, num_labels):\n",
    "        x, y, w, h, area = stats[i]\n",
    "        if area > min_area:\n",
    "            cv2.rectangle(ori_frame, (x, y), (x + w, y + h), (250,128,114), 2)\n",
    "    temp_frame = cv2.normalize(dilated_frame, None, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX, dtype=cv2.CV_8U)\n",
    "    rgb_frame = cv2.cvtColor(temp_frame, cv2.COLOR_GRAY2RGB)\n",
    "    out.write(ori_frame)\n",
    "    print(f'\\rProgress: {j}', end='')\n",
    "    prvs = next\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "cap.release()\n",
    "out.release()\n",
    "cv2.destroyAllWindows()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Worm Object"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Make Minimum Area a Global Parameter"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "min_area = 2000"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Helper Function"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "def create_list_of_n_objects(n):\n",
    "    \"\"\"\n",
    "    Create a list of n objects, each object is its index in the list.\n",
    "    \"\"\"\n",
    "    return [i for i in range(n)]\n",
    "\n",
    "# Example usage\n",
    "n = 10\n",
    "list_of_n_objects = create_list_of_n_objects(n)\n",
    "\n",
    "def iterate_and_print_list(input_list):\n",
    "    \"\"\"\n",
    "    Iteratively list all objects in the provided list.\n",
    "    \"\"\"\n",
    "    for item in input_list:\n",
    "        print(item)\n",
    "\n",
    "# Example usage with a list of 10 items\n",
    "iterate_and_print_list(list_of_n_objects)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Do the squares overlap? True\n"
     ]
    }
   ],
   "source": [
    "def check_overlap(x1, y1, width1, height1, x2, y2, width2, height2):\n",
    "    # Check if one square is to the left of the other\n",
    "    if x1 + width1 <= x2 or x2 + width2 <= x1:\n",
    "        return False\n",
    "    # Check if one square is above the other\n",
    "    if y1 + height1 <= y2 or y2 + height2 <= y1:\n",
    "        return False\n",
    "    # If neither condition is true, squares must be overlapping\n",
    "    return True\n",
    "\n",
    "# Example usage\n",
    "x1, y1, width1, height1 = 10, 10, 30, 30  # Square 1\n",
    "x2, y2, width2, height2 = 20, 20, 30, 30  # Square 2\n",
    "\n",
    "overlap = check_overlap(x1, y1, width1, height1, x2, y2, width2, height2)\n",
    "print(\"Do the squares overlap?\", overlap)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 2nd item in the queue is: item2\n"
     ]
    }
   ],
   "source": [
    "def get_nth_item(q, n):\n",
    "    if n >= q.qsize():\n",
    "        return None  # or raise an exception if you prefer\n",
    "\n",
    "    for _ in range(n - 1):\n",
    "        q.get()\n",
    "\n",
    "    nth_item = q.get()\n",
    "    return nth_item\n",
    "\n",
    "# Example\n",
    "q = queue.Queue()\n",
    "q.put('item1')\n",
    "q.put('item2')\n",
    "q.put('item3')\n",
    "\n",
    "nth_item = get_nth_item(q, 2)  # Get the 2nd item\n",
    "print(\"The 2nd item in the queue is:\", nth_item)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The distance between the points is: 5.0\n"
     ]
    }
   ],
   "source": [
    "def distance_between_points(x1, y1, x2, y2):\n",
    "    return math.sqrt((x2 - x1) ** 2 + (y2 - y1) ** 2)\n",
    "\n",
    "# Example usage\n",
    "x1, y1 = 1, 2\n",
    "x2, y2 = 4, 6\n",
    "distance = distance_between_points(x1, y1, x2, y2)\n",
    "print(\"The distance between the points is:\", distance)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "def white_outside_rectangle(fra, x, y, width, height):\n",
    "    \"\"\"\n",
    "    Makes everything outside a specified rectangle in the frame purely white.\n",
    "    :param fra: The original image/frame.\n",
    "    :param x: The x-coordinate of the top-left corner of the rectangle.\n",
    "    :param y: The y-coordinate of the top-left corner of the rectangle.\n",
    "    :param width: The width of the rectangle.\n",
    "    :param height: The height of the rectangle.\n",
    "    :return: Modified frame with white outside the specified rectangle.\n",
    "    \"\"\"\n",
    "    # Make a copy of the frame\n",
    "    modified_frame = np.copy(fra)\n",
    "    # Draw white rectangles over the areas outside the specified rectangle\n",
    "    # Top\n",
    "    modified_frame[0:y, :] = 255\n",
    "    # Bottom\n",
    "    modified_frame[y+height:fra.shape[0], :] = 255\n",
    "    # Left\n",
    "    modified_frame[:, 0:x] = 255\n",
    "    # Right\n",
    "    modified_frame[:, x+width:fra.shape[1]] = 255\n",
    "    return modified_frame\n",
    "# Example usage\n",
    "# frame = cv2.imread('path_to_your_image.jpg')  # Load your frame or image\n",
    "# modified_frame = white_outside_rectangle(frame, 50, 50, 100, 100)\n",
    "# cv2.imshow('Modified Frame', modified_frame)\n",
    "# cv2.waitKey(0)\n",
    "# cv2.destroyAllWindows()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Main Code"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "current_location = os.getcwd();\n",
    "output_folder = current_location + '/pilot_images'\n",
    "monitor_folder = current_location + '/monitor_images'\n",
    "video_path = current_location + '/sample.avi'\n",
    "if not os.path.exists(output_folder):\n",
    "    os.makedirs(output_folder)\n",
    "if not os.path.exists(monitor_folder):\n",
    "    os.makedirs(monitor_folder)\n",
    "\n",
    "class Worm:\n",
    "    def __init__(self, worm_index, original_Cap, init_label, init_stats,init_centroid,another_variable_in_case=None):\n",
    "        self.worm_index = worm_index\n",
    "        self.original_cap = original_Cap\n",
    "        self.current_label = init_label\n",
    "        self.current_stats = init_stats\n",
    "        self.current_centroid = init_centroid\n",
    "        self.label_history = queue.Queue()\n",
    "        self.stats_history = queue.Queue()\n",
    "        self.centroid_history = queue.Queue()\n",
    "        self.another_variable_in_case = another_variable_in_case\n",
    "        # also establish the output flow\n",
    "        self.frame_width = int(original_Cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "        self.frame_height = int(original_Cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "        self.frame_rate = original_Cap.get(cv2.CAP_PROP_FPS)\n",
    "        cur_location = os.getcwd()\n",
    "        text = '/worm_' + str(worm_index)\n",
    "        self.output_folder = cur_location + text\n",
    "        if not os.path.exists(self.output_folder):\n",
    "            os.makedirs(self.output_folder)\n",
    "\n",
    "    def display_info(self):\n",
    "        info = f\"worn index: {self.worm_index}, video_cap: {self.original_cap}, label_history: {self.label_history}, stats_history: {self.stats_history}, centroid_history: {self.centroid_history}, \\n\"\n",
    "        print(info)\n",
    "        info = f\"current label: {self.current_label}, current_stats: {self.current_stats}, current_centroid: {self.current_centroid}, stats_history: {self.stats_history}, centroid_history: {self.centroid_history}, \\n\"\n",
    "        print(info)\n",
    "        print('Current Stats \\n')\n",
    "        print(self.current_stats)\n",
    "        print('\\n')\n",
    "        print('\\n')\n",
    "        print('Current Label \\n')\n",
    "        print(self.current_label)\n",
    "        print('\\n')\n",
    "        print('\\n')\n",
    "        print('Current Centroid\\n')\n",
    "        print(self.current_centroid)\n",
    "        print('\\n')\n",
    "        print('\\n')\n",
    "\n",
    "    def update_another_variable(self, another_variable_in_case):\n",
    "        \"\"\"Update the car's mileage.\"\"\"\n",
    "        if another_variable_in_case >= self.another_variable_in_case:\n",
    "            self.another_variable_in_case = another_variable_in_case\n",
    "        else:\n",
    "            print(\"Error: another_variable_in_case cannot be reduced.\")\n",
    "\n",
    "    def take_the_bundle_in(self, bundle):\n",
    "        num_labels, labels, stats, centroids = bundle\n",
    "        # save the previous iteration\n",
    "        labels_modified = np.where(labels == self.current_label, self.current_label, 0)\n",
    "        self.label_history.put(labels_modified)\n",
    "        self.stats_history.put(self.current_stats)\n",
    "        self.centroid_history.put(self.current_centroid)\n",
    "        overlap_labels = queue.Queue()\n",
    "        # determine which one is the next label\n",
    "        for i in range(1, num_labels):\n",
    "            x, y, w, h, area = stats[i]\n",
    "            if area > min_area:\n",
    "                overlap_yes = check_overlap(x, y, w, h, self.current_stats[0], self.current_stats[1], self.current_stats[2], self.current_stats[3])\n",
    "                if overlap_yes:\n",
    "                    overlap_labels.put(i)\n",
    "        # find the closest centroid\n",
    "        centroid_distance = 10000000000 # first assume that the two centroids are very far away\n",
    "        best_label = -1 # let us not know which one is the best label at the beginning\n",
    "        for j in range(overlap_labels.qsize()):\n",
    "            this_label_selected = get_nth_item(overlap_labels, j)\n",
    "            this_centroid = centroids[this_label_selected]\n",
    "            this_distance = distance_between_points(this_centroid[0], this_centroid[1], self.current_centroid[0], self.current_centroid[1])\n",
    "            if this_distance < centroid_distance:\n",
    "                centroid_distance = this_distance\n",
    "                best_label = this_label_selected\n",
    "        # update the current frame\n",
    "        if best_label == -1: # make sure some labels are selected, if  no overlapping frames exist, jump to the nea\n",
    "            print('No Criterion satisfied, jump to the nearest frame')\n",
    "            for l in range(1, num_labels):\n",
    "                this_centroid = centroids[l]\n",
    "                this_distance = distance_between_points(this_centroid[0], this_centroid[1], self.current_centroid[0], self.current_centroid[1])\n",
    "                if this_distance < centroid_distance:\n",
    "                    centroid_distance = this_distance\n",
    "                    best_label = l\n",
    "        self.current_label = best_label\n",
    "        self.current_stats = stats[best_label]\n",
    "        self.current_centroid = centroids[best_label]\n",
    "        self.display_info()\n",
    "\n",
    "    def write_frame(self, n_th_frame):\n",
    "        self.original_cap.set(cv2.CAP_PROP_POS_FRAMES, n_th_frame)\n",
    "        get, this_frame = original_cap.read()\n",
    "        if not get:\n",
    "            print('something wrong happened')\n",
    "        x, y, w, h, area = self.current_stats\n",
    "        cv2.rectangle(this_frame, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "        this_frame = white_outside_rectangle(this_frame, x, y, w, h)\n",
    "        cv2.imwrite(os.path.join(self.output_folder, f\"frame_{n_th_frame}.jpg\"), this_frame)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Frame-by-frame processing"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Process the First Frame"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original_cap = cv2.VideoCapture('shorter.mp4')\n",
    "\n",
    "cap = cv2.VideoCapture('processed_v2.mp4')\n",
    "\n",
    "frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "frame_rate = cap.get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "erode_kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (2, 2))\n",
    "dilate_kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (75, 75))\n",
    "\n",
    "# Read the first frame\n",
    "ret1, frame1 = cap.read()\n",
    "prvs = cv2.cvtColor(frame1, cv2.COLOR_BGR2GRAY)\n",
    "# Read the second frame\n",
    "ret2, frame2 = cap.read()\n",
    "next = cv2.cvtColor(frame2, cv2.COLOR_BGR2GRAY)\n",
    "# Read the original frame\n",
    "ret3, ori_frame = original_cap.read()\n",
    "\n",
    "# Calculate Optical Flow\n",
    "flow = cv2.calcOpticalFlowFarneback(prvs, next, None, 0.5, 3, 15, 3, 5, 1.2, 0)\n",
    "# Compute magnitude and angle of the flow\n",
    "mag, ang = cv2.cartToPolar(flow[..., 0], flow[..., 1])\n",
    "# Create motion mask\n",
    "thresh = 1  # Set threshold for motion detection\n",
    "motion_mask = cv2.threshold(mag, thresh, 255, cv2.THRESH_BINARY)[1]\n",
    "eroded_frame = cv2.erode(motion_mask, erode_kernel, iterations=1)\n",
    "dilated_frame = cv2.dilate(eroded_frame, dilate_kernel, iterations=1)\n",
    "num_labels, labels, stats, centroids = cv2.connectedComponentsWithStats(np.uint8(dilated_frame), connectivity=8)\n",
    "for i in range(1, num_labels):\n",
    "    x, y, w, h, area = stats[i]\n",
    "    if area > min_area:\n",
    "        cv2.rectangle(dilated_frame, (x, y), (x + w, y + h), (255, 255, 255), 2)\n",
    "for i in range(1, num_labels):\n",
    "    x, y, w, h, area = stats[i]\n",
    "    if area > min_area:\n",
    "        cv2.rectangle(ori_frame, (x, y), (x + w, y + h), (250,128,114), 2)\n",
    "temp_frame = cv2.normalize(dilated_frame, None, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX, dtype=cv2.CV_8U)\n",
    "rgb_frame = cv2.cvtColor(temp_frame, cv2.COLOR_GRAY2RGB)\n",
    "prvs = next\n",
    "cv2.imwrite(os.path.join(monitor_folder, f\"frame_{1}.jpg\"), rgb_frame)\n",
    "cv2.imwrite(os.path.join(output_folder, f\"frame_{1}.jpg\"), ori_frame)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Process the Second Frame"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read the second frame\n",
    "ret2, frame2 = cap.read()\n",
    "next = cv2.cvtColor(frame2, cv2.COLOR_BGR2GRAY)\n",
    "# Read the original frame\n",
    "ret3, ori_frame = original_cap.read()\n",
    "\n",
    "# Calculate Optical Flow\n",
    "flow = cv2.calcOpticalFlowFarneback(prvs, next, None, 0.5, 3, 15, 3, 5, 1.2, 0)\n",
    "# Compute magnitude and angle of the flow\n",
    "mag, ang = cv2.cartToPolar(flow[..., 0], flow[..., 1])\n",
    "# Create motion mask\n",
    "thresh = 1  # Set threshold for motion detection\n",
    "motion_mask = cv2.threshold(mag, thresh, 255, cv2.THRESH_BINARY)[1]\n",
    "eroded_frame = cv2.erode(motion_mask, erode_kernel, iterations=1)\n",
    "dilated_frame = cv2.dilate(eroded_frame, dilate_kernel, iterations=1)\n",
    "num_labels, labels, stats, centroids = cv2.connectedComponentsWithStats(np.uint8(dilated_frame), connectivity=8)\n",
    "for i in range(1, num_labels):\n",
    "    x, y, w, h, area = stats[i]\n",
    "    if area > min_area:\n",
    "        cv2.rectangle(dilated_frame, (x, y), (x + w, y + h), (255, 255, 255), 2)\n",
    "for i in range(1, num_labels):\n",
    "    x, y, w, h, area = stats[i]\n",
    "    if area > min_area:\n",
    "        cv2.rectangle(ori_frame, (x, y), (x + w, y + h), (250,128,114), 2)\n",
    "temp_frame = cv2.normalize(dilated_frame, None, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX, dtype=cv2.CV_8U)\n",
    "rgb_frame = cv2.cvtColor(temp_frame, cv2.COLOR_GRAY2RGB)\n",
    "prvs = next\n",
    "cv2.imwrite(os.path.join(monitor_folder, f\"frame_{2}.jpg\"), rgb_frame)\n",
    "cv2.imwrite(os.path.join(output_folder, f\"frame_{2}.jpg\"), ori_frame)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Process the Third Frame"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read the second frame\n",
    "ret2, frame2 = cap.read()\n",
    "next = cv2.cvtColor(frame2, cv2.COLOR_BGR2GRAY)\n",
    "# Read the original frame\n",
    "ret3, ori_frame = original_cap.read()\n",
    "\n",
    "# Calculate Optical Flow\n",
    "flow = cv2.calcOpticalFlowFarneback(prvs, next, None, 0.5, 3, 15, 3, 5, 1.2, 0)\n",
    "# Compute magnitude and angle of the flow\n",
    "mag, ang = cv2.cartToPolar(flow[..., 0], flow[..., 1])\n",
    "# Create motion mask\n",
    "thresh = 1  # Set threshold for motion detection\n",
    "motion_mask = cv2.threshold(mag, thresh, 255, cv2.THRESH_BINARY)[1]\n",
    "eroded_frame = cv2.erode(motion_mask, erode_kernel, iterations=1)\n",
    "dilated_frame = cv2.dilate(eroded_frame, dilate_kernel, iterations=1)\n",
    "num_labels, labels, stats, centroids = cv2.connectedComponentsWithStats(np.uint8(dilated_frame), connectivity=8)\n",
    "for i in range(1, num_labels):\n",
    "    x, y, w, h, area = stats[i]\n",
    "    if area > min_area:\n",
    "        cv2.rectangle(dilated_frame, (x, y), (x + w, y + h), (255, 255, 255), 2)\n",
    "for i in range(1, num_labels):\n",
    "    x, y, w, h, area = stats[i]\n",
    "    if area > min_area:\n",
    "        cv2.rectangle(ori_frame, (x, y), (x + w, y + h), (250,128,114), 2)\n",
    "temp_frame = cv2.normalize(dilated_frame, None, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX, dtype=cv2.CV_8U)\n",
    "rgb_frame = cv2.cvtColor(temp_frame, cv2.COLOR_GRAY2RGB)\n",
    "prvs = next\n",
    "cv2.imwrite(os.path.join(monitor_folder, f\"frame_{3}.jpg\"), rgb_frame)\n",
    "cv2.imwrite(os.path.join(output_folder, f\"frame_{3}.jpg\"), ori_frame)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Set Up the First Worm Object: Worm_1"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "worn index: 4, video_cap: < cv2.VideoCapture 0x11ee848b0>, label_history: <queue.Queue object at 0x11eeef0d0>, stats_history: <queue.Queue object at 0x11eeef610>, centroid_history: <queue.Queue object at 0x11eeefbd0>, \n",
      "\n",
      "current label: 4, current_stats: [1010  120   86   87 7454], current_centroid: [1052.49946338  163.04292997], stats_history: <queue.Queue object at 0x11eeef610>, centroid_history: <queue.Queue object at 0x11eeefbd0>, \n",
      "\n",
      "Current Stats \n",
      "\n",
      "[1010  120   86   87 7454]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Current Label \n",
      "\n",
      "4\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Current Centroid\n",
      "\n",
      "[1052.49946338  163.04292997]\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "worm_1 = Worm(4, original_cap, 4, stats[4], centroids[4])\n",
    "worm_1.display_info()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Set up the Worm List"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "# set up the number that I want to track for worms\n",
    "track_number = 10\n",
    "\n",
    "# create the list of n tracking number\n",
    "w_list = create_list_of_n_objects(track_number)\n",
    "\n",
    "for i in range(1, len(w_list)):\n",
    "    temp_Worm = Worm(i, original_cap, i, stats[i], centroids[i])\n",
    "    w_list[i] = temp_Worm"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Check everything in the list"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "worn index: 1, video_cap: < cv2.VideoCapture 0x11ee848b0>, label_history: <queue.Queue object at 0x11ef029d0>, stats_history: <queue.Queue object at 0x11ef027d0>, centroid_history: <queue.Queue object at 0x11ef01a90>, \n",
      "\n",
      "current label: 1, current_stats: [ 935    0   78   77 6003], current_centroid: [973.48075962  37.98150925], stats_history: <queue.Queue object at 0x11ef027d0>, centroid_history: <queue.Queue object at 0x11ef01a90>, \n",
      "\n",
      "Current Stats \n",
      "\n",
      "[ 935    0   78   77 6003]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Current Label \n",
      "\n",
      "1\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Current Centroid\n",
      "\n",
      "[973.48075962  37.98150925]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "worn index: 2, video_cap: < cv2.VideoCapture 0x11ee848b0>, label_history: <queue.Queue object at 0x11eed8050>, stats_history: <queue.Queue object at 0x11eeed510>, centroid_history: <queue.Queue object at 0x11eeef810>, \n",
      "\n",
      "current label: 2, current_stats: [1053   12   79   79 6233], current_centroid: [1092.04925397   50.97561367], stats_history: <queue.Queue object at 0x11eeed510>, centroid_history: <queue.Queue object at 0x11eeef810>, \n",
      "\n",
      "Current Stats \n",
      "\n",
      "[1053   12   79   79 6233]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Current Label \n",
      "\n",
      "2\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Current Centroid\n",
      "\n",
      "[1092.04925397   50.97561367]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "worn index: 3, video_cap: < cv2.VideoCapture 0x11ee848b0>, label_history: <queue.Queue object at 0x11eee2690>, stats_history: <queue.Queue object at 0x11eee1610>, centroid_history: <queue.Queue object at 0x11eedb7d0>, \n",
      "\n",
      "current label: 3, current_stats: [1122  102   82  104 8225], current_centroid: [1162.04863222  153.05471125], stats_history: <queue.Queue object at 0x11eee1610>, centroid_history: <queue.Queue object at 0x11eedb7d0>, \n",
      "\n",
      "Current Stats \n",
      "\n",
      "[1122  102   82  104 8225]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Current Label \n",
      "\n",
      "3\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Current Centroid\n",
      "\n",
      "[1162.04863222  153.05471125]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "worn index: 4, video_cap: < cv2.VideoCapture 0x11ee848b0>, label_history: <queue.Queue object at 0x11eee2790>, stats_history: <queue.Queue object at 0x11eee15d0>, centroid_history: <queue.Queue object at 0x11eee0b50>, \n",
      "\n",
      "current label: 4, current_stats: [1010  120   86   87 7454], current_centroid: [1052.49946338  163.04292997], stats_history: <queue.Queue object at 0x11eee15d0>, centroid_history: <queue.Queue object at 0x11eee0b50>, \n",
      "\n",
      "Current Stats \n",
      "\n",
      "[1010  120   86   87 7454]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Current Label \n",
      "\n",
      "4\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Current Centroid\n",
      "\n",
      "[1052.49946338  163.04292997]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "worn index: 5, video_cap: < cv2.VideoCapture 0x11ee848b0>, label_history: <queue.Queue object at 0x11eee3790>, stats_history: <queue.Queue object at 0x11eee3c10>, centroid_history: <queue.Queue object at 0x11eee1210>, \n",
      "\n",
      "current label: 5, current_stats: [  359   124   443   377 67112], current_centroid: [574.51002801 280.78412206], stats_history: <queue.Queue object at 0x11eee3c10>, centroid_history: <queue.Queue object at 0x11eee1210>, \n",
      "\n",
      "Current Stats \n",
      "\n",
      "[  359   124   443   377 67112]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Current Label \n",
      "\n",
      "5\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Current Centroid\n",
      "\n",
      "[574.51002801 280.78412206]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "worn index: 6, video_cap: < cv2.VideoCapture 0x11ee848b0>, label_history: <queue.Queue object at 0x106e1ee50>, stats_history: <queue.Queue object at 0x11eeebf90>, centroid_history: <queue.Queue object at 0x11eeead10>, \n",
      "\n",
      "current label: 6, current_stats: [1277  158   76   76 5776], current_centroid: [1314.5  195.5], stats_history: <queue.Queue object at 0x11eeebf90>, centroid_history: <queue.Queue object at 0x11eeead10>, \n",
      "\n",
      "Current Stats \n",
      "\n",
      "[1277  158   76   76 5776]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Current Label \n",
      "\n",
      "6\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Current Centroid\n",
      "\n",
      "[1314.5  195.5]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "worn index: 7, video_cap: < cv2.VideoCapture 0x11ee848b0>, label_history: <queue.Queue object at 0x11eee8f50>, stats_history: <queue.Queue object at 0x11eeeb850>, centroid_history: <queue.Queue object at 0x11eee94d0>, \n",
      "\n",
      "current label: 7, current_stats: [ 156  463   78   76 5925], current_centroid: [194.49367089 500.51898734], stats_history: <queue.Queue object at 0x11eeeb850>, centroid_history: <queue.Queue object at 0x11eee94d0>, \n",
      "\n",
      "Current Stats \n",
      "\n",
      "[ 156  463   78   76 5925]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Current Label \n",
      "\n",
      "7\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Current Centroid\n",
      "\n",
      "[194.49367089 500.51898734]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "worn index: 8, video_cap: < cv2.VideoCapture 0x11ee848b0>, label_history: <queue.Queue object at 0x11d29a210>, stats_history: <queue.Queue object at 0x11eece490>, centroid_history: <queue.Queue object at 0x11eecc250>, \n",
      "\n",
      "current label: 8, current_stats: [  172   564   122   141 16269], current_centroid: [231.24426824 633.56358719], stats_history: <queue.Queue object at 0x11eece490>, centroid_history: <queue.Queue object at 0x11eecc250>, \n",
      "\n",
      "Current Stats \n",
      "\n",
      "[  172   564   122   141 16269]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Current Label \n",
      "\n",
      "8\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Current Centroid\n",
      "\n",
      "[231.24426824 633.56358719]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "worn index: 9, video_cap: < cv2.VideoCapture 0x11ee848b0>, label_history: <queue.Queue object at 0x11ef02850>, stats_history: <queue.Queue object at 0x11ef01d10>, centroid_history: <queue.Queue object at 0x11ef02190>, \n",
      "\n",
      "current label: 9, current_stats: [ 1887   653   156   195 25521], current_centroid: [1964.71074801  741.59680263], stats_history: <queue.Queue object at 0x11ef01d10>, centroid_history: <queue.Queue object at 0x11ef02190>, \n",
      "\n",
      "Current Stats \n",
      "\n",
      "[ 1887   653   156   195 25521]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Current Label \n",
      "\n",
      "9\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Current Centroid\n",
      "\n",
      "[1964.71074801  741.59680263]\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, len(w_list)):\n",
    "    temp_Worm = Worm(i, original_cap, i, stats[i], centroids[i])\n",
    "    temp_Worm.display_info()\n",
    "    w_list[i] = temp_Worm"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Process the Fourth Frame and take the bundle in by the Worm"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "worn index: 4, video_cap: < cv2.VideoCapture 0x11ee848b0>, label_history: <queue.Queue object at 0x11eeef0d0>, stats_history: <queue.Queue object at 0x11eeef610>, centroid_history: <queue.Queue object at 0x11eeefbd0>, \n",
      "\n",
      "current label: 1, current_stats: [ 1026   117   210    97 18763], current_centroid: [1129.28305708  165.21739594], stats_history: <queue.Queue object at 0x11eeef610>, centroid_history: <queue.Queue object at 0x11eeefbd0>, \n",
      "\n",
      "Current Stats \n",
      "\n",
      "[ 1026   117   210    97 18763]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Current Label \n",
      "\n",
      "1\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Current Centroid\n",
      "\n",
      "[1129.28305708  165.21739594]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "worn index: 1, video_cap: < cv2.VideoCapture 0x11ee848b0>, label_history: <queue.Queue object at 0x11ee821d0>, stats_history: <queue.Queue object at 0x11ef0ccd0>, centroid_history: <queue.Queue object at 0x11ef0cdd0>, \n",
      "\n",
      "current label: 1, current_stats: [ 1026   117   210    97 18763], current_centroid: [1129.28305708  165.21739594], stats_history: <queue.Queue object at 0x11ef0ccd0>, centroid_history: <queue.Queue object at 0x11ef0cdd0>, \n",
      "\n",
      "Current Stats \n",
      "\n",
      "[ 1026   117   210    97 18763]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Current Label \n",
      "\n",
      "1\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Current Centroid\n",
      "\n",
      "[1129.28305708  165.21739594]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "worn index: 2, video_cap: < cv2.VideoCapture 0x11ee848b0>, label_history: <queue.Queue object at 0x11ef01a90>, stats_history: <queue.Queue object at 0x11ef027d0>, centroid_history: <queue.Queue object at 0x11ef029d0>, \n",
      "\n",
      "current label: 2, current_stats: [  435   124   386   256 51413], current_centroid: [620.73300527 248.3179546 ], stats_history: <queue.Queue object at 0x11ef027d0>, centroid_history: <queue.Queue object at 0x11ef029d0>, \n",
      "\n",
      "Current Stats \n",
      "\n",
      "[  435   124   386   256 51413]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Current Label \n",
      "\n",
      "2\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Current Centroid\n",
      "\n",
      "[620.73300527 248.3179546 ]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "worn index: 3, video_cap: < cv2.VideoCapture 0x11ee848b0>, label_history: <queue.Queue object at 0x11eeefd50>, stats_history: <queue.Queue object at 0x11eeee710>, centroid_history: <queue.Queue object at 0x11eeeed50>, \n",
      "\n",
      "current label: 3, current_stats: [ 341  285   76   76 5775], current_centroid: [378.49350649 322.49350649], stats_history: <queue.Queue object at 0x11eeee710>, centroid_history: <queue.Queue object at 0x11eeeed50>, \n",
      "\n",
      "Current Stats \n",
      "\n",
      "[ 341  285   76   76 5775]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Current Label \n",
      "\n",
      "3\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Current Centroid\n",
      "\n",
      "[378.49350649 322.49350649]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "worn index: 4, video_cap: < cv2.VideoCapture 0x11ee848b0>, label_history: <queue.Queue object at 0x11eedb7d0>, stats_history: <queue.Queue object at 0x11eeda850>, centroid_history: <queue.Queue object at 0x11eee2690>, \n",
      "\n",
      "current label: 4, current_stats: [ 363  407   90   89 7934], current_centroid: [407.39727754 451.01222586], stats_history: <queue.Queue object at 0x11eeda850>, centroid_history: <queue.Queue object at 0x11eee2690>, \n",
      "\n",
      "Current Stats \n",
      "\n",
      "[ 363  407   90   89 7934]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Current Label \n",
      "\n",
      "4\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Current Centroid\n",
      "\n",
      "[407.39727754 451.01222586]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "worn index: 5, video_cap: < cv2.VideoCapture 0x11ee848b0>, label_history: <queue.Queue object at 0x11eee0b50>, stats_history: <queue.Queue object at 0x11eee15d0>, centroid_history: <queue.Queue object at 0x11eee2790>, \n",
      "\n",
      "current label: 5, current_stats: [  171   557   127   142 16565], current_centroid: [234.27232116 627.71403562], stats_history: <queue.Queue object at 0x11eee15d0>, centroid_history: <queue.Queue object at 0x11eee2790>, \n",
      "\n",
      "Current Stats \n",
      "\n",
      "[  171   557   127   142 16565]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Current Label \n",
      "\n",
      "5\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Current Centroid\n",
      "\n",
      "[234.27232116 627.71403562]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "worn index: 6, video_cap: < cv2.VideoCapture 0x11ee848b0>, label_history: <queue.Queue object at 0x11eee1210>, stats_history: <queue.Queue object at 0x11eee3c10>, centroid_history: <queue.Queue object at 0x11eee3790>, \n",
      "\n",
      "current label: 6, current_stats: [ 1887   657   160   196 26049], current_centroid: [1966.45045875  744.73146762], stats_history: <queue.Queue object at 0x11eee3c10>, centroid_history: <queue.Queue object at 0x11eee3790>, \n",
      "\n",
      "Current Stats \n",
      "\n",
      "[ 1887   657   160   196 26049]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Current Label \n",
      "\n",
      "6\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Current Centroid\n",
      "\n",
      "[1966.45045875  744.73146762]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "worn index: 7, video_cap: < cv2.VideoCapture 0x11ee848b0>, label_history: <queue.Queue object at 0x106e1ee50>, stats_history: <queue.Queue object at 0x11eeeb410>, centroid_history: <queue.Queue object at 0x11eee9cd0>, \n",
      "\n",
      "current label: 7, current_stats: [  676   714   229   312 44939], current_centroid: [785.38512205 871.66690403], stats_history: <queue.Queue object at 0x11eeeb410>, centroid_history: <queue.Queue object at 0x11eee9cd0>, \n",
      "\n",
      "Current Stats \n",
      "\n",
      "[  676   714   229   312 44939]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Current Label \n",
      "\n",
      "7\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Current Centroid\n",
      "\n",
      "[785.38512205 871.66690403]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "worn index: 8, video_cap: < cv2.VideoCapture 0x11ee848b0>, label_history: <queue.Queue object at 0x11eee94d0>, stats_history: <queue.Queue object at 0x11eeeb850>, centroid_history: <queue.Queue object at 0x11eee8f50>, \n",
      "\n",
      "current label: 8, current_stats: [ 103  755   78   76 5926], current_centroid: [141.5 792.5], stats_history: <queue.Queue object at 0x11eeeb850>, centroid_history: <queue.Queue object at 0x11eee8f50>, \n",
      "\n",
      "Current Stats \n",
      "\n",
      "[ 103  755   78   76 5926]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Current Label \n",
      "\n",
      "8\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Current Centroid\n",
      "\n",
      "[141.5 792.5]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "worn index: 9, video_cap: < cv2.VideoCapture 0x11ee848b0>, label_history: <queue.Queue object at 0x11ef01850>, stats_history: <queue.Queue object at 0x11eece490>, centroid_history: <queue.Queue object at 0x11d29a210>, \n",
      "\n",
      "current label: 9, current_stats: [  999   809   186   192 26570], current_centroid: [1094.37245013  902.39397817], stats_history: <queue.Queue object at 0x11eece490>, centroid_history: <queue.Queue object at 0x11d29a210>, \n",
      "\n",
      "Current Stats \n",
      "\n",
      "[  999   809   186   192 26570]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Current Label \n",
      "\n",
      "9\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Current Centroid\n",
      "\n",
      "[1094.37245013  902.39397817]\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read the second frame\n",
    "ret2, frame2 = cap.read()\n",
    "next = cv2.cvtColor(frame2, cv2.COLOR_BGR2GRAY)\n",
    "# Read the original frame\n",
    "ret3, ori_frame = original_cap.read()\n",
    "\n",
    "# Calculate Optical Flow\n",
    "flow = cv2.calcOpticalFlowFarneback(prvs, next, None, 0.5, 3, 15, 3, 5, 1.2, 0)\n",
    "# Compute magnitude and angle of the flow\n",
    "mag, ang = cv2.cartToPolar(flow[..., 0], flow[..., 1])\n",
    "# Create motion mask\n",
    "thresh = 1  # Set threshold for motion detection\n",
    "motion_mask = cv2.threshold(mag, thresh, 255, cv2.THRESH_BINARY)[1]\n",
    "eroded_frame = cv2.erode(motion_mask, erode_kernel, iterations=1)\n",
    "dilated_frame = cv2.dilate(eroded_frame, dilate_kernel, iterations=1)\n",
    "bundle = cv2.connectedComponentsWithStats(np.uint8(dilated_frame), connectivity=8)\n",
    "num_labels, labels, stats, centroids = cv2.connectedComponentsWithStats(np.uint8(dilated_frame), connectivity=8)\n",
    "\n",
    "worm_1.take_the_bundle_in(bundle)\n",
    "worm_1.write_frame(4)\n",
    "\n",
    "# process the worm list\n",
    "for i in range(1, len(w_list)):\n",
    "    temp_Worm = Worm(i, original_cap, i, stats[i], centroids[i])\n",
    "    temp_Worm.take_the_bundle_in(bundle)\n",
    "    temp_Worm.write_frame(4)\n",
    "    w_list[i] = temp_Worm\n",
    "\n",
    "for i in range(1, num_labels):\n",
    "    x, y, w, h, area = stats[i]\n",
    "    if area > min_area:\n",
    "        cv2.rectangle(dilated_frame, (x, y), (x + w, y + h), (255, 255, 255), 2)\n",
    "for i in range(1, num_labels):\n",
    "    x, y, w, h, area = stats[i]\n",
    "    if area > min_area:\n",
    "        cv2.rectangle(ori_frame, (x, y), (x + w, y + h), (250,128,114), 2)\n",
    "temp_frame = cv2.normalize(dilated_frame, None, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX, dtype=cv2.CV_8U)\n",
    "rgb_frame = cv2.cvtColor(temp_frame, cv2.COLOR_GRAY2RGB)\n",
    "prvs = next\n",
    "cv2.imwrite(os.path.join(monitor_folder, f\"frame_{4}.jpg\"), rgb_frame)\n",
    "cv2.imwrite(os.path.join(output_folder, f\"frame_{4}.jpg\"), ori_frame)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "worn index: 4, video_cap: < cv2.VideoCapture 0x11ee848b0>, label_history: <queue.Queue object at 0x11eeef0d0>, stats_history: <queue.Queue object at 0x11eeef610>, centroid_history: <queue.Queue object at 0x11eeefbd0>, \n",
      "\n",
      "current label: 1, current_stats: [ 1026   117   210    97 18763], current_centroid: [1129.28305708  165.21739594], stats_history: <queue.Queue object at 0x11eeef610>, centroid_history: <queue.Queue object at 0x11eeefbd0>, \n",
      "\n",
      "Current Stats \n",
      "\n",
      "[ 1026   117   210    97 18763]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Current Label \n",
      "\n",
      "1\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Current Centroid\n",
      "\n",
      "[1129.28305708  165.21739594]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "/Users/billdeng/PycharmProjects/unicellular/worm_4\n"
     ]
    }
   ],
   "source": [
    "worm_1.display_info()\n",
    "print(worm_1.output_folder)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Proceed to the Fifth Frame"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "worn index: 4, video_cap: < cv2.VideoCapture 0x11ee848b0>, label_history: <queue.Queue object at 0x11eeef0d0>, stats_history: <queue.Queue object at 0x11eeef610>, centroid_history: <queue.Queue object at 0x11eeefbd0>, \n",
      "\n",
      "current label: 1, current_stats: [1027  119   88   86 7537], current_centroid: [1070.38675866  161.48401221], stats_history: <queue.Queue object at 0x11eeef610>, centroid_history: <queue.Queue object at 0x11eeefbd0>, \n",
      "\n",
      "Current Stats \n",
      "\n",
      "[1027  119   88   86 7537]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Current Label \n",
      "\n",
      "1\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Current Centroid\n",
      "\n",
      "[1070.38675866  161.48401221]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "worn index: 1, video_cap: < cv2.VideoCapture 0x11ee848b0>, label_history: <queue.Queue object at 0x11eecf190>, stats_history: <queue.Queue object at 0x11ef0d850>, centroid_history: <queue.Queue object at 0x11ef0d910>, \n",
      "\n",
      "current label: 1, current_stats: [1027  119   88   86 7537], current_centroid: [1070.38675866  161.48401221], stats_history: <queue.Queue object at 0x11ef0d850>, centroid_history: <queue.Queue object at 0x11ef0d910>, \n",
      "\n",
      "Current Stats \n",
      "\n",
      "[1027  119   88   86 7537]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Current Label \n",
      "\n",
      "1\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Current Centroid\n",
      "\n",
      "[1070.38675866  161.48401221]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "worn index: 2, video_cap: < cv2.VideoCapture 0x11ee848b0>, label_history: <queue.Queue object at 0x11ee821d0>, stats_history: <queue.Queue object at 0x11ef0ca50>, centroid_history: <queue.Queue object at 0x11ef0cc10>, \n",
      "\n",
      "current label: 2, current_stats: [  429   122   379   263 62111], current_centroid: [599.31598268 239.4958864 ], stats_history: <queue.Queue object at 0x11ef0ca50>, centroid_history: <queue.Queue object at 0x11ef0cc10>, \n",
      "\n",
      "Current Stats \n",
      "\n",
      "[  429   122   379   263 62111]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Current Label \n",
      "\n",
      "2\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Current Centroid\n",
      "\n",
      "[599.31598268 239.4958864 ]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "worn index: 3, video_cap: < cv2.VideoCapture 0x11ee848b0>, label_history: <queue.Queue object at 0x11ef029d0>, stats_history: <queue.Queue object at 0x11ef027d0>, centroid_history: <queue.Queue object at 0x11ef01a90>, \n",
      "\n",
      "current label: 3, current_stats: [  194   552   109   125 13189], current_centroid: [248.10144818 613.3115475 ], stats_history: <queue.Queue object at 0x11ef027d0>, centroid_history: <queue.Queue object at 0x11ef01a90>, \n",
      "\n",
      "Current Stats \n",
      "\n",
      "[  194   552   109   125 13189]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Current Label \n",
      "\n",
      "3\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Current Centroid\n",
      "\n",
      "[248.10144818 613.3115475 ]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "worn index: 4, video_cap: < cv2.VideoCapture 0x11ee848b0>, label_history: <queue.Queue object at 0x11eed8ed0>, stats_history: <queue.Queue object at 0x11eeee710>, centroid_history: <queue.Queue object at 0x11eeefd50>, \n",
      "\n",
      "current label: 4, current_stats: [ 1890   649   162   208 28281], current_centroid: [1970.73109155  743.5184753 ], stats_history: <queue.Queue object at 0x11eeee710>, centroid_history: <queue.Queue object at 0x11eeefd50>, \n",
      "\n",
      "Current Stats \n",
      "\n",
      "[ 1890   649   162   208 28281]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Current Label \n",
      "\n",
      "4\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Current Centroid\n",
      "\n",
      "[1970.73109155  743.5184753 ]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "worn index: 5, video_cap: < cv2.VideoCapture 0x11ee848b0>, label_history: <queue.Queue object at 0x11eee2690>, stats_history: <queue.Queue object at 0x11eee2250>, centroid_history: <queue.Queue object at 0x11eedb7d0>, \n",
      "\n",
      "current label: 5, current_stats: [  670   705   230   310 46125], current_centroid: [780.69910027 864.7581355 ], stats_history: <queue.Queue object at 0x11eee2250>, centroid_history: <queue.Queue object at 0x11eedb7d0>, \n",
      "\n",
      "Current Stats \n",
      "\n",
      "[  670   705   230   310 46125]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Current Label \n",
      "\n",
      "5\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Current Centroid\n",
      "\n",
      "[780.69910027 864.7581355 ]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "worn index: 6, video_cap: < cv2.VideoCapture 0x11ee848b0>, label_history: <queue.Queue object at 0x11eee2790>, stats_history: <queue.Queue object at 0x11eee15d0>, centroid_history: <queue.Queue object at 0x11eee0b50>, \n",
      "\n",
      "current label: 6, current_stats: [   69   848   118   189 20841], current_centroid: [129.23065112 944.04692673], stats_history: <queue.Queue object at 0x11eee15d0>, centroid_history: <queue.Queue object at 0x11eee0b50>, \n",
      "\n",
      "Current Stats \n",
      "\n",
      "[   69   848   118   189 20841]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Current Label \n",
      "\n",
      "6\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Current Centroid\n",
      "\n",
      "[129.23065112 944.04692673]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "worn index: 7, video_cap: < cv2.VideoCapture 0x11ee848b0>, label_history: <queue.Queue object at 0x11eee3790>, stats_history: <queue.Queue object at 0x11eee3c10>, centroid_history: <queue.Queue object at 0x11eee1210>, \n",
      "\n",
      "current label: 7, current_stats: [ 1048   860   141   144 17929], current_centroid: [1120.69418261  931.31836689], stats_history: <queue.Queue object at 0x11eee3c10>, centroid_history: <queue.Queue object at 0x11eee1210>, \n",
      "\n",
      "Current Stats \n",
      "\n",
      "[ 1048   860   141   144 17929]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Current Label \n",
      "\n",
      "7\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Current Centroid\n",
      "\n",
      "[1120.69418261  931.31836689]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "worn index: 8, video_cap: < cv2.VideoCapture 0x11ee848b0>, label_history: <queue.Queue object at 0x106e1ee50>, stats_history: <queue.Queue object at 0x11eeea290>, centroid_history: <queue.Queue object at 0x11eeeb810>, \n",
      "\n",
      "current label: 8, current_stats: [ 1883   901   233   596 79554], current_centroid: [1988.06056264 1185.98996908], stats_history: <queue.Queue object at 0x11eeea290>, centroid_history: <queue.Queue object at 0x11eeeb810>, \n",
      "\n",
      "Current Stats \n",
      "\n",
      "[ 1883   901   233   596 79554]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Current Label \n",
      "\n",
      "8\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Current Centroid\n",
      "\n",
      "[1988.06056264 1185.98996908]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "worn index: 9, video_cap: < cv2.VideoCapture 0x11ee848b0>, label_history: <queue.Queue object at 0x11eee8f50>, stats_history: <queue.Queue object at 0x11eeeb850>, centroid_history: <queue.Queue object at 0x11eee94d0>, \n",
      "\n",
      "current label: 9, current_stats: [  455   950   110    93 10011], current_centroid: [509.40485466 996.04844671], stats_history: <queue.Queue object at 0x11eeeb850>, centroid_history: <queue.Queue object at 0x11eee94d0>, \n",
      "\n",
      "Current Stats \n",
      "\n",
      "[  455   950   110    93 10011]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Current Label \n",
      "\n",
      "9\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Current Centroid\n",
      "\n",
      "[509.40485466 996.04844671]\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read the second frame\n",
    "ret2, frame2 = cap.read()\n",
    "next = cv2.cvtColor(frame2, cv2.COLOR_BGR2GRAY)\n",
    "# Read the original frame\n",
    "ret3, ori_frame = original_cap.read()\n",
    "\n",
    "# Calculate Optical Flow\n",
    "flow = cv2.calcOpticalFlowFarneback(prvs, next, None, 0.5, 3, 15, 3, 5, 1.2, 0)\n",
    "# Compute magnitude and angle of the flow\n",
    "mag, ang = cv2.cartToPolar(flow[..., 0], flow[..., 1])\n",
    "# Create motion mask\n",
    "thresh = 1  # Set threshold for motion detection\n",
    "motion_mask = cv2.threshold(mag, thresh, 255, cv2.THRESH_BINARY)[1]\n",
    "eroded_frame = cv2.erode(motion_mask, erode_kernel, iterations=1)\n",
    "dilated_frame = cv2.dilate(eroded_frame, dilate_kernel, iterations=1)\n",
    "bundle = cv2.connectedComponentsWithStats(np.uint8(dilated_frame), connectivity=8)\n",
    "num_labels, labels, stats, centroids = cv2.connectedComponentsWithStats(np.uint8(dilated_frame), connectivity=8)\n",
    "\n",
    "worm_1.take_the_bundle_in(bundle)\n",
    "worm_1.write_frame(5)\n",
    "\n",
    "# process the worm list\n",
    "for i in range(1, len(w_list)):\n",
    "    temp_Worm = Worm(i, original_cap, i, stats[i], centroids[i])\n",
    "    temp_Worm.take_the_bundle_in(bundle)\n",
    "    temp_Worm.write_frame(5)\n",
    "    w_list[i] = temp_Worm\n",
    "\n",
    "for i in range(1, num_labels):\n",
    "    x, y, w, h, area = stats[i]\n",
    "    if area > min_area:\n",
    "        cv2.rectangle(dilated_frame, (x, y), (x + w, y + h), (255, 255, 255), 2)\n",
    "for i in range(1, num_labels):\n",
    "    x, y, w, h, area = stats[i]\n",
    "    if area > min_area:\n",
    "        cv2.rectangle(ori_frame, (x, y), (x + w, y + h), (250,128,114), 2)\n",
    "temp_frame = cv2.normalize(dilated_frame, None, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX, dtype=cv2.CV_8U)\n",
    "rgb_frame = cv2.cvtColor(temp_frame, cv2.COLOR_GRAY2RGB)\n",
    "prvs = next\n",
    "cv2.imwrite(os.path.join(monitor_folder, f\"frame_{5}.jpg\"), rgb_frame)\n",
    "cv2.imwrite(os.path.join(output_folder, f\"frame_{5}.jpg\"), ori_frame)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "worn index: 4, video_cap: < cv2.VideoCapture 0x11ee848b0>, label_history: <queue.Queue object at 0x11eeef0d0>, stats_history: <queue.Queue object at 0x11eeef610>, centroid_history: <queue.Queue object at 0x11eeefbd0>, \n",
      "\n",
      "current label: 1, current_stats: [1027  119   88   86 7537], current_centroid: [1070.38675866  161.48401221], stats_history: <queue.Queue object at 0x11eeef610>, centroid_history: <queue.Queue object at 0x11eeefbd0>, \n",
      "\n",
      "Current Stats \n",
      "\n",
      "[1027  119   88   86 7537]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Current Label \n",
      "\n",
      "1\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Current Centroid\n",
      "\n",
      "[1070.38675866  161.48401221]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "/Users/billdeng/PycharmProjects/unicellular/worm_4\n"
     ]
    }
   ],
   "source": [
    "worm_1.display_info()\n",
    "print(worm_1.output_folder)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Proceed to the Sixth Frame"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Criterion satisfied, jump to the nearest frame\n",
      "worn index: 4, video_cap: < cv2.VideoCapture 0x11ee848b0>, label_history: <queue.Queue object at 0x11eeef0d0>, stats_history: <queue.Queue object at 0x11eeef610>, centroid_history: <queue.Queue object at 0x11eeefbd0>, \n",
      "\n",
      "current label: 1, current_stats: [  425   123   397   267 57098], current_centroid: [617.63856878 248.42047007], stats_history: <queue.Queue object at 0x11eeef610>, centroid_history: <queue.Queue object at 0x11eeefbd0>, \n",
      "\n",
      "Current Stats \n",
      "\n",
      "[  425   123   397   267 57098]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Current Label \n",
      "\n",
      "1\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Current Centroid\n",
      "\n",
      "[617.63856878 248.42047007]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "worn index: 1, video_cap: < cv2.VideoCapture 0x11ee848b0>, label_history: <queue.Queue object at 0x11ee83050>, stats_history: <queue.Queue object at 0x11ef0eb50>, centroid_history: <queue.Queue object at 0x11ef0ec10>, \n",
      "\n",
      "current label: 1, current_stats: [  425   123   397   267 57098], current_centroid: [617.63856878 248.42047007], stats_history: <queue.Queue object at 0x11ef0eb50>, centroid_history: <queue.Queue object at 0x11ef0ec10>, \n",
      "\n",
      "Current Stats \n",
      "\n",
      "[  425   123   397   267 57098]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Current Label \n",
      "\n",
      "1\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Current Centroid\n",
      "\n",
      "[617.63856878 248.42047007]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "worn index: 2, video_cap: < cv2.VideoCapture 0x11ee848b0>, label_history: <queue.Queue object at 0x11eecf190>, stats_history: <queue.Queue object at 0x11ef0d950>, centroid_history: <queue.Queue object at 0x11ef0d7d0>, \n",
      "\n",
      "current label: 2, current_stats: [ 327  278   85   82 6937], current_centroid: [369.00562203 318.6055932 ], stats_history: <queue.Queue object at 0x11ef0d950>, centroid_history: <queue.Queue object at 0x11ef0d7d0>, \n",
      "\n",
      "Current Stats \n",
      "\n",
      "[ 327  278   85   82 6937]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Current Label \n",
      "\n",
      "2\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Current Centroid\n",
      "\n",
      "[369.00562203 318.6055932 ]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "worn index: 3, video_cap: < cv2.VideoCapture 0x11ee848b0>, label_history: <queue.Queue object at 0x11ee821d0>, stats_history: <queue.Queue object at 0x11ef0ca90>, centroid_history: <queue.Queue object at 0x11ef0c8d0>, \n",
      "\n",
      "current label: 3, current_stats: [ 200  407   83   82 6793], current_centroid: [241.00559399 447.52907405], stats_history: <queue.Queue object at 0x11ef0ca90>, centroid_history: <queue.Queue object at 0x11ef0c8d0>, \n",
      "\n",
      "Current Stats \n",
      "\n",
      "[ 200  407   83   82 6793]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Current Label \n",
      "\n",
      "3\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Current Centroid\n",
      "\n",
      "[241.00559399 447.52907405]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "worn index: 4, video_cap: < cv2.VideoCapture 0x11ee848b0>, label_history: <queue.Queue object at 0x11ef01a90>, stats_history: <queue.Queue object at 0x11ef027d0>, centroid_history: <queue.Queue object at 0x11ef029d0>, \n",
      "\n",
      "current label: 4, current_stats: [  164   545   142   176 21809], current_centroid: [234.33637489 631.99857857], stats_history: <queue.Queue object at 0x11ef027d0>, centroid_history: <queue.Queue object at 0x11ef029d0>, \n",
      "\n",
      "Current Stats \n",
      "\n",
      "[  164   545   142   176 21809]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Current Label \n",
      "\n",
      "4\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Current Centroid\n",
      "\n",
      "[234.33637489 631.99857857]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "worn index: 5, video_cap: < cv2.VideoCapture 0x11ee848b0>, label_history: <queue.Queue object at 0x11eeeec50>, stats_history: <queue.Queue object at 0x11eeeef90>, centroid_history: <queue.Queue object at 0x11eeef810>, \n",
      "\n",
      "current label: 5, current_stats: [  1883    627    234    871 130069], current_centroid: [1988.49712845 1061.91055517], stats_history: <queue.Queue object at 0x11eeeef90>, centroid_history: <queue.Queue object at 0x11eeef810>, \n",
      "\n",
      "Current Stats \n",
      "\n",
      "[  1883    627    234    871 130069]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Current Label \n",
      "\n",
      "5\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Current Centroid\n",
      "\n",
      "[1988.49712845 1061.91055517]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "worn index: 6, video_cap: < cv2.VideoCapture 0x11ee848b0>, label_history: <queue.Queue object at 0x11eedbd50>, stats_history: <queue.Queue object at 0x11eeda850>, centroid_history: <queue.Queue object at 0x11eee0ed0>, \n",
      "\n",
      "current label: 6, current_stats: [  53  676   85   89 7507], current_centroid: [ 94.92513654 719.95883842], stats_history: <queue.Queue object at 0x11eeda850>, centroid_history: <queue.Queue object at 0x11eee0ed0>, \n",
      "\n",
      "Current Stats \n",
      "\n",
      "[  53  676   85   89 7507]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Current Label \n",
      "\n",
      "6\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Current Centroid\n",
      "\n",
      "[ 94.92513654 719.95883842]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "worn index: 7, video_cap: < cv2.VideoCapture 0x11ee848b0>, label_history: <queue.Queue object at 0x11eee0b50>, stats_history: <queue.Queue object at 0x11eee15d0>, centroid_history: <queue.Queue object at 0x11eee2790>, \n",
      "\n",
      "current label: 7, current_stats: [  668   698   243   341 53478], current_centroid: [784.86353267 870.38376529], stats_history: <queue.Queue object at 0x11eee15d0>, centroid_history: <queue.Queue object at 0x11eee2790>, \n",
      "\n",
      "Current Stats \n",
      "\n",
      "[  668   698   243   341 53478]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Current Label \n",
      "\n",
      "7\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Current Centroid\n",
      "\n",
      "[784.86353267 870.38376529]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "worn index: 8, video_cap: < cv2.VideoCapture 0x11ee848b0>, label_history: <queue.Queue object at 0x11eee1d10>, stats_history: <queue.Queue object at 0x11eee2050>, centroid_history: <queue.Queue object at 0x11eee3cd0>, \n",
      "\n",
      "current label: 8, current_stats: [  994   816   200   190 28051], current_centroid: [1096.34045132  907.20630281], stats_history: <queue.Queue object at 0x11eee2050>, centroid_history: <queue.Queue object at 0x11eee3cd0>, \n",
      "\n",
      "Current Stats \n",
      "\n",
      "[  994   816   200   190 28051]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Current Label \n",
      "\n",
      "8\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Current Centroid\n",
      "\n",
      "[1096.34045132  907.20630281]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "worn index: 9, video_cap: < cv2.VideoCapture 0x11ee848b0>, label_history: <queue.Queue object at 0x106e1ee50>, stats_history: <queue.Queue object at 0x11eeead10>, centroid_history: <queue.Queue object at 0x11eee8a50>, \n",
      "\n",
      "current label: 9, current_stats: [   70   832   122   224 24652], current_centroid: [131.14996755 945.50997891], stats_history: <queue.Queue object at 0x11eeead10>, centroid_history: <queue.Queue object at 0x11eee8a50>, \n",
      "\n",
      "Current Stats \n",
      "\n",
      "[   70   832   122   224 24652]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Current Label \n",
      "\n",
      "9\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Current Centroid\n",
      "\n",
      "[131.14996755 945.50997891]\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read the second frame\n",
    "ret2, frame2 = cap.read()\n",
    "next = cv2.cvtColor(frame2, cv2.COLOR_BGR2GRAY)\n",
    "# Read the original frame\n",
    "ret3, ori_frame = original_cap.read()\n",
    "\n",
    "# Calculate Optical Flow\n",
    "flow = cv2.calcOpticalFlowFarneback(prvs, next, None, 0.5, 3, 15, 3, 5, 1.2, 0)\n",
    "# Compute magnitude and angle of the flow\n",
    "mag, ang = cv2.cartToPolar(flow[..., 0], flow[..., 1])\n",
    "# Create motion mask\n",
    "thresh = 1  # Set threshold for motion detection\n",
    "motion_mask = cv2.threshold(mag, thresh, 255, cv2.THRESH_BINARY)[1]\n",
    "eroded_frame = cv2.erode(motion_mask, erode_kernel, iterations=1)\n",
    "dilated_frame = cv2.dilate(eroded_frame, dilate_kernel, iterations=1)\n",
    "bundle = cv2.connectedComponentsWithStats(np.uint8(dilated_frame), connectivity=8)\n",
    "num_labels, labels, stats, centroids = cv2.connectedComponentsWithStats(np.uint8(dilated_frame), connectivity=8)\n",
    "\n",
    "worm_1.take_the_bundle_in(bundle)\n",
    "worm_1.write_frame(6)\n",
    "\n",
    "# process the worm list\n",
    "for i in range(1, len(w_list)):\n",
    "    temp_Worm = Worm(i, original_cap, i, stats[i], centroids[i])\n",
    "    temp_Worm.take_the_bundle_in(bundle)\n",
    "    temp_Worm.write_frame(6)\n",
    "    w_list[i] = temp_Worm\n",
    "\n",
    "for i in range(1, num_labels):\n",
    "    x, y, w, h, area = stats[i]\n",
    "    if area > min_area:\n",
    "        cv2.rectangle(dilated_frame, (x, y), (x + w, y + h), (255, 255, 255), 2)\n",
    "for i in range(1, num_labels):\n",
    "    x, y, w, h, area = stats[i]\n",
    "    if area > min_area:\n",
    "        cv2.rectangle(ori_frame, (x, y), (x + w, y + h), (250,128,114), 2)\n",
    "temp_frame = cv2.normalize(dilated_frame, None, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX, dtype=cv2.CV_8U)\n",
    "rgb_frame = cv2.cvtColor(temp_frame, cv2.COLOR_GRAY2RGB)\n",
    "prvs = next\n",
    "cv2.imwrite(os.path.join(monitor_folder, f\"frame_{6}.jpg\"), rgb_frame)\n",
    "cv2.imwrite(os.path.join(output_folder, f\"frame_{6}.jpg\"), ori_frame)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "worn index: 4, video_cap: < cv2.VideoCapture 0x11ee848b0>, label_history: <queue.Queue object at 0x11eeef0d0>, stats_history: <queue.Queue object at 0x11eeef610>, centroid_history: <queue.Queue object at 0x11eeefbd0>, \n",
      "\n",
      "current label: 1, current_stats: [  425   123   397   267 57098], current_centroid: [617.63856878 248.42047007], stats_history: <queue.Queue object at 0x11eeef610>, centroid_history: <queue.Queue object at 0x11eeefbd0>, \n",
      "\n",
      "Current Stats \n",
      "\n",
      "[  425   123   397   267 57098]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Current Label \n",
      "\n",
      "1\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Current Centroid\n",
      "\n",
      "[617.63856878 248.42047007]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "/Users/billdeng/PycharmProjects/unicellular/worm_4\n"
     ]
    }
   ],
   "source": [
    "worm_1.display_info()\n",
    "print(worm_1.output_folder)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Reset and Process all the frames"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Start with the First Frame"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original_cap = cv2.VideoCapture('shorter.mp4')\n",
    "\n",
    "cap = cv2.VideoCapture('processed_v2.mp4')\n",
    "\n",
    "frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "frame_rate = cap.get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "erode_kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (2, 2))\n",
    "dilate_kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (75, 75))\n",
    "\n",
    "# Read the first frame\n",
    "ret1, frame1 = cap.read()\n",
    "prvs = cv2.cvtColor(frame1, cv2.COLOR_BGR2GRAY)\n",
    "# Read the second frame\n",
    "ret2, frame2 = cap.read()\n",
    "next = cv2.cvtColor(frame2, cv2.COLOR_BGR2GRAY)\n",
    "# Read the original frame\n",
    "ret3, ori_frame = original_cap.read()\n",
    "\n",
    "# Calculate Optical Flow\n",
    "flow = cv2.calcOpticalFlowFarneback(prvs, next, None, 0.5, 3, 15, 3, 5, 1.2, 0)\n",
    "# Compute magnitude and angle of the flow\n",
    "mag, ang = cv2.cartToPolar(flow[..., 0], flow[..., 1])\n",
    "# Create motion mask\n",
    "thresh = 1  # Set threshold for motion detection\n",
    "motion_mask = cv2.threshold(mag, thresh, 255, cv2.THRESH_BINARY)[1]\n",
    "eroded_frame = cv2.erode(motion_mask, erode_kernel, iterations=1)\n",
    "dilated_frame = cv2.dilate(eroded_frame, dilate_kernel, iterations=1)\n",
    "num_labels, labels, stats, centroids = cv2.connectedComponentsWithStats(np.uint8(dilated_frame), connectivity=8)\n",
    "for i in range(1, num_labels):\n",
    "    x, y, w, h, area = stats[i]\n",
    "    if area > min_area:\n",
    "        cv2.rectangle(dilated_frame, (x, y), (x + w, y + h), (255, 255, 255), 2)\n",
    "for i in range(1, num_labels):\n",
    "    x, y, w, h, area = stats[i]\n",
    "    if area > min_area:\n",
    "        cv2.rectangle(ori_frame, (x, y), (x + w, y + h), (250,128,114), 2)\n",
    "temp_frame = cv2.normalize(dilated_frame, None, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX, dtype=cv2.CV_8U)\n",
    "rgb_frame = cv2.cvtColor(temp_frame, cv2.COLOR_GRAY2RGB)\n",
    "prvs = next\n",
    "cv2.imwrite(os.path.join(monitor_folder, f\"frame_{1}.jpg\"), rgb_frame)\n",
    "cv2.imwrite(os.path.join(output_folder, f\"frame_{1}.jpg\"), ori_frame)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Process the Second Frame"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read the second frame\n",
    "ret2, frame2 = cap.read()\n",
    "next = cv2.cvtColor(frame2, cv2.COLOR_BGR2GRAY)\n",
    "# Read the original frame\n",
    "ret3, ori_frame = original_cap.read()\n",
    "\n",
    "# Calculate Optical Flow\n",
    "flow = cv2.calcOpticalFlowFarneback(prvs, next, None, 0.5, 3, 15, 3, 5, 1.2, 0)\n",
    "# Compute magnitude and angle of the flow\n",
    "mag, ang = cv2.cartToPolar(flow[..., 0], flow[..., 1])\n",
    "# Create motion mask\n",
    "thresh = 1  # Set threshold for motion detection\n",
    "motion_mask = cv2.threshold(mag, thresh, 255, cv2.THRESH_BINARY)[1]\n",
    "eroded_frame = cv2.erode(motion_mask, erode_kernel, iterations=1)\n",
    "dilated_frame = cv2.dilate(eroded_frame, dilate_kernel, iterations=1)\n",
    "num_labels, labels, stats, centroids = cv2.connectedComponentsWithStats(np.uint8(dilated_frame), connectivity=8)\n",
    "for i in range(1, num_labels):\n",
    "    x, y, w, h, area = stats[i]\n",
    "    if area > min_area:\n",
    "        cv2.rectangle(dilated_frame, (x, y), (x + w, y + h), (255, 255, 255), 2)\n",
    "for i in range(1, num_labels):\n",
    "    x, y, w, h, area = stats[i]\n",
    "    if area > min_area:\n",
    "        cv2.rectangle(ori_frame, (x, y), (x + w, y + h), (250,128,114), 2)\n",
    "temp_frame = cv2.normalize(dilated_frame, None, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX, dtype=cv2.CV_8U)\n",
    "rgb_frame = cv2.cvtColor(temp_frame, cv2.COLOR_GRAY2RGB)\n",
    "prvs = next\n",
    "cv2.imwrite(os.path.join(monitor_folder, f\"frame_{2}.jpg\"), rgb_frame)\n",
    "cv2.imwrite(os.path.join(output_folder, f\"frame_{2}.jpg\"), ori_frame)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Set up worm list"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [],
   "source": [
    "# set up the number that I want to track for worms\n",
    "track_number = 10\n",
    "\n",
    "# create the list of n tracking number\n",
    "w_list = create_list_of_n_objects(track_number)\n",
    "\n",
    "for i in range(1, len(w_list)):\n",
    "    temp_Worm = Worm(i, original_cap, i, stats[i], centroids[i])\n",
    "    w_list[i] = temp_Worm"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Make sure the list is well set"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "worn index: 1, video_cap: < cv2.VideoCapture 0x11ee854b0>, label_history: <queue.Queue object at 0x11eeeeb90>, stats_history: <queue.Queue object at 0x11eec3e90>, centroid_history: <queue.Queue object at 0x11eec0dd0>, \n",
      "\n",
      "current label: 1, current_stats: [  924     0   537   250 75010], current_centroid: [1172.90573257  136.73210239], stats_history: <queue.Queue object at 0x11eec3e90>, centroid_history: <queue.Queue object at 0x11eec0dd0>, \n",
      "\n",
      "Current Stats \n",
      "\n",
      "[  924     0   537   250 75010]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Current Label \n",
      "\n",
      "1\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Current Centroid\n",
      "\n",
      "[1172.90573257  136.73210239]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "worn index: 2, video_cap: < cv2.VideoCapture 0x11ee854b0>, label_history: <queue.Queue object at 0x11d6b3a90>, stats_history: <queue.Queue object at 0x11eeec250>, centroid_history: <queue.Queue object at 0x11eeefd50>, \n",
      "\n",
      "current label: 2, current_stats: [ 810   31  112   89 9733], current_centroid: [865.21103462  75.09339361], stats_history: <queue.Queue object at 0x11eeec250>, centroid_history: <queue.Queue object at 0x11eeefd50>, \n",
      "\n",
      "Current Stats \n",
      "\n",
      "[ 810   31  112   89 9733]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Current Label \n",
      "\n",
      "2\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Current Centroid\n",
      "\n",
      "[865.21103462  75.09339361]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "worn index: 3, video_cap: < cv2.VideoCapture 0x11ee854b0>, label_history: <queue.Queue object at 0x11edcf190>, stats_history: <queue.Queue object at 0x11eeead10>, centroid_history: <queue.Queue object at 0x11eee8a50>, \n",
      "\n",
      "current label: 3, current_stats: [  313    94   493   460 83193], current_centroid: [568.12996286 289.41301552], stats_history: <queue.Queue object at 0x11eeead10>, centroid_history: <queue.Queue object at 0x11eee8a50>, \n",
      "\n",
      "Current Stats \n",
      "\n",
      "[  313    94   493   460 83193]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Current Label \n",
      "\n",
      "3\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Current Centroid\n",
      "\n",
      "[568.12996286 289.41301552]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "worn index: 4, video_cap: < cv2.VideoCapture 0x11ee854b0>, label_history: <queue.Queue object at 0x11eee1490>, stats_history: <queue.Queue object at 0x11eee2690>, centroid_history: <queue.Queue object at 0x11eee10d0>, \n",
      "\n",
      "current label: 4, current_stats: [ 845  134  108   87 8724], current_centroid: [898.27086199 177.18924805], stats_history: <queue.Queue object at 0x11eee2690>, centroid_history: <queue.Queue object at 0x11eee10d0>, \n",
      "\n",
      "Current Stats \n",
      "\n",
      "[ 845  134  108   87 8724]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Current Label \n",
      "\n",
      "4\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Current Centroid\n",
      "\n",
      "[898.27086199 177.18924805]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "worn index: 5, video_cap: < cv2.VideoCapture 0x11ee854b0>, label_history: <queue.Queue object at 0x11eee1c10>, stats_history: <queue.Queue object at 0x11eee2b90>, centroid_history: <queue.Queue object at 0x11eee0050>, \n",
      "\n",
      "current label: 5, current_stats: [ 1483   226   219   169 23599], current_centroid: [1591.12521717  311.15767617], stats_history: <queue.Queue object at 0x11eee2b90>, centroid_history: <queue.Queue object at 0x11eee0050>, \n",
      "\n",
      "Current Stats \n",
      "\n",
      "[ 1483   226   219   169 23599]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Current Label \n",
      "\n",
      "5\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Current Centroid\n",
      "\n",
      "[1591.12521717  311.15767617]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "worn index: 6, video_cap: < cv2.VideoCapture 0x11ee854b0>, label_history: <queue.Queue object at 0x11eea6d50>, stats_history: <queue.Queue object at 0x11eea7ad0>, centroid_history: <queue.Queue object at 0x11eee2c10>, \n",
      "\n",
      "current label: 6, current_stats: [1725  400   79   77 6076], current_centroid: [1763.98156682  437.99374589], stats_history: <queue.Queue object at 0x11eea7ad0>, centroid_history: <queue.Queue object at 0x11eee2c10>, \n",
      "\n",
      "Current Stats \n",
      "\n",
      "[1725  400   79   77 6076]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Current Label \n",
      "\n",
      "6\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Current Centroid\n",
      "\n",
      "[1763.98156682  437.99374589]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "worn index: 7, video_cap: < cv2.VideoCapture 0x11ee854b0>, label_history: <queue.Queue object at 0x11eeedc50>, stats_history: <queue.Queue object at 0x11eea6790>, centroid_history: <queue.Queue object at 0x11eea7390>, \n",
      "\n",
      "current label: 7, current_stats: [  103   553   219   167 27210], current_centroid: [211.98834987 627.30812201], stats_history: <queue.Queue object at 0x11eea6790>, centroid_history: <queue.Queue object at 0x11eea7390>, \n",
      "\n",
      "Current Stats \n",
      "\n",
      "[  103   553   219   167 27210]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Current Label \n",
      "\n",
      "7\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Current Centroid\n",
      "\n",
      "[211.98834987 627.30812201]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "worn index: 8, video_cap: < cv2.VideoCapture 0x11ee854b0>, label_history: <queue.Queue object at 0x11ee2e710>, stats_history: <queue.Queue object at 0x11eea70d0>, centroid_history: <queue.Queue object at 0x11eea7b10>, \n",
      "\n",
      "current label: 8, current_stats: [ 1885   632   229   417 54610], current_centroid: [1992.21197583  839.92440945], stats_history: <queue.Queue object at 0x11eea70d0>, centroid_history: <queue.Queue object at 0x11eea7b10>, \n",
      "\n",
      "Current Stats \n",
      "\n",
      "[ 1885   632   229   417 54610]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Current Label \n",
      "\n",
      "8\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Current Centroid\n",
      "\n",
      "[1992.21197583  839.92440945]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "worn index: 9, video_cap: < cv2.VideoCapture 0x11ee854b0>, label_history: <queue.Queue object at 0x11eeaf450>, stats_history: <queue.Queue object at 0x11eeaf250>, centroid_history: <queue.Queue object at 0x11eea5b10>, \n",
      "\n",
      "current label: 9, current_stats: [    0   647   210   412 46450], current_centroid: [119.48213132 871.19386437], stats_history: <queue.Queue object at 0x11eeaf250>, centroid_history: <queue.Queue object at 0x11eea5b10>, \n",
      "\n",
      "Current Stats \n",
      "\n",
      "[    0   647   210   412 46450]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Current Label \n",
      "\n",
      "9\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Current Centroid\n",
      "\n",
      "[119.48213132 871.19386437]\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, len(w_list)):\n",
    "    temp_Worm = Worm(i, original_cap, i, stats[i], centroids[i])\n",
    "    temp_Worm.display_info()\n",
    "    w_list[i] = temp_Worm"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Run the loop to parse the video"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frame Number: 3 \n",
      "Number of Labels 23\n",
      "Labels [[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n",
      "stats [[      0       0    2160    2160 3893072]\n",
      " [    935       0      78      77    6003]\n",
      " [   1053      12      79      79    6233]\n",
      " ...\n",
      " [    236    1591     139     121   15414]\n",
      " [    376    1638     225     234   41513]\n",
      " [    819    1746     742     359  135367]]\n",
      "centroid [[1077.21786831 1037.29386664]\n",
      " [ 973.48075962   37.98150925]\n",
      " [1092.04925397   50.97561367]\n",
      " [1162.04863222  153.05471125]\n",
      " [1052.49946338  163.04292997]\n",
      " [ 574.51002801  280.78412206]\n",
      " [1314.5         195.5       ]\n",
      " [ 194.49367089  500.51898734]\n",
      " [ 231.24426824  633.56358719]\n",
      " [1964.71074801  741.59680263]\n",
      " [ 786.54372894  874.09124046]\n",
      " [1091.62124831  901.54975718]\n",
      " [2028.20329935  978.70038945]\n",
      " [ 127.90485275  979.32025754]\n",
      " [ 522.45516024 1071.90412143]\n",
      " [1551.01466915 1174.70608928]\n",
      " [1978.05242965 1268.70604034]\n",
      " [1065.22762008 1306.94187176]\n",
      " [1766.30660305 1725.09000701]\n",
      " [ 715.50804037 1808.16119301]\n",
      " [ 305.0599455  1651.35857013]\n",
      " [ 478.48649821 1741.69187965]\n",
      " [1204.8524308  1948.90595197]]\n",
      "worn index: 1, video_cap: < cv2.VideoCapture 0x11ee854b0>, label_history: <queue.Queue object at 0x11ee2e4d0>, stats_history: <queue.Queue object at 0x11eee4bd0>, centroid_history: <queue.Queue object at 0x11eee4f50>, \n",
      "\n",
      "current label: 1, current_stats: [ 935    0   78   77 6003], current_centroid: [973.48075962  37.98150925], stats_history: <queue.Queue object at 0x11eee4bd0>, centroid_history: <queue.Queue object at 0x11eee4f50>, \n",
      "\n",
      "Current Stats \n",
      "\n",
      "[ 935    0   78   77 6003]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Current Label \n",
      "\n",
      "1\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Current Centroid\n",
      "\n",
      "[973.48075962  37.98150925]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "worn index: 2, video_cap: < cv2.VideoCapture 0x11ee854b0>, label_history: <queue.Queue object at 0x11eeeeb90>, stats_history: <queue.Queue object at 0x11eec2590>, centroid_history: <queue.Queue object at 0x11eec2810>, \n",
      "\n",
      "current label: 2, current_stats: [1053   12   79   79 6233], current_centroid: [1092.04925397   50.97561367], stats_history: <queue.Queue object at 0x11eec2590>, centroid_history: <queue.Queue object at 0x11eec2810>, \n",
      "\n",
      "Current Stats \n",
      "\n",
      "[1053   12   79   79 6233]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Current Label \n",
      "\n",
      "2\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Current Centroid\n",
      "\n",
      "[1092.04925397   50.97561367]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "worn index: 3, video_cap: < cv2.VideoCapture 0x11ee854b0>, label_history: <queue.Queue object at 0x11ee5f190>, stats_history: <queue.Queue object at 0x11eeeff10>, centroid_history: <queue.Queue object at 0x11eeeef90>, \n",
      "\n",
      "current label: 3, current_stats: [1122  102   82  104 8225], current_centroid: [1162.04863222  153.05471125], stats_history: <queue.Queue object at 0x11eeeff10>, centroid_history: <queue.Queue object at 0x11eeeef90>, \n",
      "\n",
      "Current Stats \n",
      "\n",
      "[1122  102   82  104 8225]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Current Label \n",
      "\n",
      "3\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Current Centroid\n",
      "\n",
      "[1162.04863222  153.05471125]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "worn index: 4, video_cap: < cv2.VideoCapture 0x11ee854b0>, label_history: <queue.Queue object at 0x11edcf190>, stats_history: <queue.Queue object at 0x11eeeb050>, centroid_history: <queue.Queue object at 0x11eeebf90>, \n",
      "\n",
      "current label: 4, current_stats: [1010  120   86   87 7454], current_centroid: [1052.49946338  163.04292997], stats_history: <queue.Queue object at 0x11eeeb050>, centroid_history: <queue.Queue object at 0x11eeebf90>, \n",
      "\n",
      "Current Stats \n",
      "\n",
      "[1010  120   86   87 7454]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Current Label \n",
      "\n",
      "4\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Current Centroid\n",
      "\n",
      "[1052.49946338  163.04292997]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "worn index: 5, video_cap: < cv2.VideoCapture 0x11ee854b0>, label_history: <queue.Queue object at 0x11eee3110>, stats_history: <queue.Queue object at 0x11eee1610>, centroid_history: <queue.Queue object at 0x11eee3b90>, \n",
      "\n",
      "current label: 5, current_stats: [  359   124   443   377 67112], current_centroid: [574.51002801 280.78412206], stats_history: <queue.Queue object at 0x11eee1610>, centroid_history: <queue.Queue object at 0x11eee3b90>, \n",
      "\n",
      "Current Stats \n",
      "\n",
      "[  359   124   443   377 67112]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Current Label \n",
      "\n",
      "5\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Current Centroid\n",
      "\n",
      "[574.51002801 280.78412206]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "worn index: 6, video_cap: < cv2.VideoCapture 0x11ee854b0>, label_history: <queue.Queue object at 0x11eee0050>, stats_history: <queue.Queue object at 0x11eee2b90>, centroid_history: <queue.Queue object at 0x11eee1c10>, \n",
      "\n",
      "current label: 6, current_stats: [1277  158   76   76 5776], current_centroid: [1314.5  195.5], stats_history: <queue.Queue object at 0x11eee2b90>, centroid_history: <queue.Queue object at 0x11eee1c10>, \n",
      "\n",
      "Current Stats \n",
      "\n",
      "[1277  158   76   76 5776]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Current Label \n",
      "\n",
      "6\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Current Centroid\n",
      "\n",
      "[1314.5  195.5]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "worn index: 7, video_cap: < cv2.VideoCapture 0x11ee854b0>, label_history: <queue.Queue object at 0x11eee1d10>, stats_history: <queue.Queue object at 0x11eee1750>, centroid_history: <queue.Queue object at 0x11eea6d50>, \n",
      "\n",
      "current label: 7, current_stats: [ 156  463   78   76 5925], current_centroid: [194.49367089 500.51898734], stats_history: <queue.Queue object at 0x11eee1750>, centroid_history: <queue.Queue object at 0x11eea6d50>, \n",
      "\n",
      "Current Stats \n",
      "\n",
      "[ 156  463   78   76 5925]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Current Label \n",
      "\n",
      "7\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Current Centroid\n",
      "\n",
      "[194.49367089 500.51898734]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "worn index: 8, video_cap: < cv2.VideoCapture 0x11ee854b0>, label_history: <queue.Queue object at 0x11eea7390>, stats_history: <queue.Queue object at 0x11eea6790>, centroid_history: <queue.Queue object at 0x11eea6390>, \n",
      "\n",
      "current label: 8, current_stats: [  172   564   122   141 16269], current_centroid: [231.24426824 633.56358719], stats_history: <queue.Queue object at 0x11eea6790>, centroid_history: <queue.Queue object at 0x11eea6390>, \n",
      "\n",
      "Current Stats \n",
      "\n",
      "[  172   564   122   141 16269]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Current Label \n",
      "\n",
      "8\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Current Centroid\n",
      "\n",
      "[231.24426824 633.56358719]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "worn index: 9, video_cap: < cv2.VideoCapture 0x11ee854b0>, label_history: <queue.Queue object at 0x11ee2e710>, stats_history: <queue.Queue object at 0x11eea7250>, centroid_history: <queue.Queue object at 0x11eea7c10>, \n",
      "\n",
      "current label: 9, current_stats: [ 1887   653   156   195 25521], current_centroid: [1964.71074801  741.59680263], stats_history: <queue.Queue object at 0x11eea7250>, centroid_history: <queue.Queue object at 0x11eea7c10>, \n",
      "\n",
      "Current Stats \n",
      "\n",
      "[ 1887   653   156   195 25521]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Current Label \n",
      "\n",
      "9\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Current Centroid\n",
      "\n",
      "[1964.71074801  741.59680263]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Frame Number: 4 \n",
      "Number of Labels 19\n",
      "Labels [[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n",
      "stats [[      0       0    2160    2160 3920757]\n",
      " [   1026     117     210      97   18763]\n",
      " [    435     124     386     256   51413]\n",
      " ...\n",
      " [    236    1587     140     116   15035]\n",
      " [    379    1588     493     363  114735]\n",
      " [    736    1959      75      78    5850]]\n",
      "centroid [[1076.20338062 1036.87229379]\n",
      " [1129.28305708  165.21739594]\n",
      " [ 620.73300527  248.3179546 ]\n",
      " [ 378.49350649  322.49350649]\n",
      " [ 407.39727754  451.01222586]\n",
      " [ 234.27232116  627.71403562]\n",
      " [1966.45045875  744.73146762]\n",
      " [ 785.38512205  871.66690403]\n",
      " [ 141.5         792.5       ]\n",
      " [1094.37245013  902.39397817]\n",
      " [ 130.57567881  956.6339103 ]\n",
      " [1992.73415299 1193.00398268]\n",
      " [ 525.52005468 1075.33799777]\n",
      " [1553.25298932 1172.01448399]\n",
      " [1064.24870001 1308.38552617]\n",
      " [1407.79749064 1875.67034953]\n",
      " [ 305.81935484 1645.51659461]\n",
      " [ 622.79866649 1757.73856278]\n",
      " [ 773.         1997.5       ]]\n",
      "worn index: 1, video_cap: < cv2.VideoCapture 0x11ee854b0>, label_history: <queue.Queue object at 0x11da31a90>, stats_history: <queue.Queue object at 0x11eeafdd0>, centroid_history: <queue.Queue object at 0x11eeae390>, \n",
      "\n",
      "current label: 1, current_stats: [ 1026   117   210    97 18763], current_centroid: [1129.28305708  165.21739594], stats_history: <queue.Queue object at 0x11eeafdd0>, centroid_history: <queue.Queue object at 0x11eeae390>, \n",
      "\n",
      "Current Stats \n",
      "\n",
      "[ 1026   117   210    97 18763]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Current Label \n",
      "\n",
      "1\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Current Centroid\n",
      "\n",
      "[1129.28305708  165.21739594]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "worn index: 2, video_cap: < cv2.VideoCapture 0x11ee854b0>, label_history: <queue.Queue object at 0x11eeda990>, stats_history: <queue.Queue object at 0x11ef0d610>, centroid_history: <queue.Queue object at 0x11eee5210>, \n",
      "\n",
      "current label: 2, current_stats: [  435   124   386   256 51413], current_centroid: [620.73300527 248.3179546 ], stats_history: <queue.Queue object at 0x11ef0d610>, centroid_history: <queue.Queue object at 0x11eee5210>, \n",
      "\n",
      "Current Stats \n",
      "\n",
      "[  435   124   386   256 51413]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Current Label \n",
      "\n",
      "2\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Current Centroid\n",
      "\n",
      "[620.73300527 248.3179546 ]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "worn index: 3, video_cap: < cv2.VideoCapture 0x11ee854b0>, label_history: <queue.Queue object at 0x11eeeeb90>, stats_history: <queue.Queue object at 0x11eec3650>, centroid_history: <queue.Queue object at 0x11eec3c50>, \n",
      "\n",
      "current label: 3, current_stats: [ 341  285   76   76 5775], current_centroid: [378.49350649 322.49350649], stats_history: <queue.Queue object at 0x11eec3650>, centroid_history: <queue.Queue object at 0x11eec3c50>, \n",
      "\n",
      "Current Stats \n",
      "\n",
      "[ 341  285   76   76 5775]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Current Label \n",
      "\n",
      "3\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Current Centroid\n",
      "\n",
      "[378.49350649 322.49350649]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "worn index: 4, video_cap: < cv2.VideoCapture 0x11ee854b0>, label_history: <queue.Queue object at 0x11ee5f190>, stats_history: <queue.Queue object at 0x11eeeed50>, centroid_history: <queue.Queue object at 0x11eeef810>, \n",
      "\n",
      "current label: 4, current_stats: [ 363  407   90   89 7934], current_centroid: [407.39727754 451.01222586], stats_history: <queue.Queue object at 0x11eeeed50>, centroid_history: <queue.Queue object at 0x11eeef810>, \n",
      "\n",
      "Current Stats \n",
      "\n",
      "[ 363  407   90   89 7934]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Current Label \n",
      "\n",
      "4\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Current Centroid\n",
      "\n",
      "[407.39727754 451.01222586]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "worn index: 5, video_cap: < cv2.VideoCapture 0x11ee854b0>, label_history: <queue.Queue object at 0x11edcf190>, stats_history: <queue.Queue object at 0x11eeea190>, centroid_history: <queue.Queue object at 0x11eeea290>, \n",
      "\n",
      "current label: 5, current_stats: [  171   557   127   142 16565], current_centroid: [234.27232116 627.71403562], stats_history: <queue.Queue object at 0x11eeea190>, centroid_history: <queue.Queue object at 0x11eeea290>, \n",
      "\n",
      "Current Stats \n",
      "\n",
      "[  171   557   127   142 16565]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Current Label \n",
      "\n",
      "5\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Current Centroid\n",
      "\n",
      "[234.27232116 627.71403562]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "worn index: 6, video_cap: < cv2.VideoCapture 0x11ee854b0>, label_history: <queue.Queue object at 0x11eee10d0>, stats_history: <queue.Queue object at 0x11eee1610>, centroid_history: <queue.Queue object at 0x11eee3110>, \n",
      "\n",
      "current label: 6, current_stats: [ 1887   657   160   196 26049], current_centroid: [1966.45045875  744.73146762], stats_history: <queue.Queue object at 0x11eee1610>, centroid_history: <queue.Queue object at 0x11eee3110>, \n",
      "\n",
      "Current Stats \n",
      "\n",
      "[ 1887   657   160   196 26049]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Current Label \n",
      "\n",
      "6\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Current Centroid\n",
      "\n",
      "[1966.45045875  744.73146762]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "worn index: 7, video_cap: < cv2.VideoCapture 0x11ee854b0>, label_history: <queue.Queue object at 0x11eee1c10>, stats_history: <queue.Queue object at 0x11eee2b90>, centroid_history: <queue.Queue object at 0x11eee0050>, \n",
      "\n",
      "current label: 7, current_stats: [  676   714   229   312 44939], current_centroid: [785.38512205 871.66690403], stats_history: <queue.Queue object at 0x11eee2b90>, centroid_history: <queue.Queue object at 0x11eee0050>, \n",
      "\n",
      "Current Stats \n",
      "\n",
      "[  676   714   229   312 44939]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Current Label \n",
      "\n",
      "7\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Current Centroid\n",
      "\n",
      "[785.38512205 871.66690403]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "worn index: 8, video_cap: < cv2.VideoCapture 0x11ee854b0>, label_history: <queue.Queue object at 0x11eea6190>, stats_history: <queue.Queue object at 0x11eea7ad0>, centroid_history: <queue.Queue object at 0x11eee1d10>, \n",
      "\n",
      "current label: 8, current_stats: [ 103  755   78   76 5926], current_centroid: [141.5 792.5], stats_history: <queue.Queue object at 0x11eea7ad0>, centroid_history: <queue.Queue object at 0x11eee1d10>, \n",
      "\n",
      "Current Stats \n",
      "\n",
      "[ 103  755   78   76 5926]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Current Label \n",
      "\n",
      "8\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Current Centroid\n",
      "\n",
      "[141.5 792.5]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "worn index: 9, video_cap: < cv2.VideoCapture 0x11ee854b0>, label_history: <queue.Queue object at 0x11ef0f250>, stats_history: <queue.Queue object at 0x11eea6790>, centroid_history: <queue.Queue object at 0x11eea7390>, \n",
      "\n",
      "current label: 9, current_stats: [  999   809   186   192 26570], current_centroid: [1094.37245013  902.39397817], stats_history: <queue.Queue object at 0x11eea6790>, centroid_history: <queue.Queue object at 0x11eea7390>, \n",
      "\n",
      "Current Stats \n",
      "\n",
      "[  999   809   186   192 26570]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Current Label \n",
      "\n",
      "9\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Current Centroid\n",
      "\n",
      "[1094.37245013  902.39397817]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Frame Number: 5 \n",
      "Number of Labels 20\n",
      "Labels [[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n",
      "stats [[      0       0    2160    2160 3967403]\n",
      " [   1027     119      88      86    7537]\n",
      " [    429     122     379     263   62111]\n",
      " ...\n",
      " [    375    1632     230     242   43777]\n",
      " [   1085    1775     617     316  118524]\n",
      " [    904    1968     187     108   18077]]\n",
      "centroid [[1074.87468276 1037.53282916]\n",
      " [1070.38675866  161.48401221]\n",
      " [ 599.31598268  239.4958864 ]\n",
      " [ 248.10144818  613.3115475 ]\n",
      " [1970.73109155  743.5184753 ]\n",
      " [ 780.69910027  864.7581355 ]\n",
      " [ 129.23065112  944.04692673]\n",
      " [1120.69418261  931.31836689]\n",
      " [1988.06056264 1185.98996908]\n",
      " [ 509.40485466  996.04844671]\n",
      " [1555.89177252 1168.12662432]\n",
      " [ 552.15066602 1135.48719718]\n",
      " [1060.89541008 1297.82732295]\n",
      " [1907.02302871 1567.01195161]\n",
      " [1779.31849936 1700.07724509]\n",
      " [ 305.48956193 1642.00576579]\n",
      " [ 707.42576525 1771.22544593]\n",
      " [ 478.08186947 1740.01918816]\n",
      " [1348.23367419 1922.52524383]\n",
      " [ 999.12042927 2023.17436522]]\n",
      "worn index: 1, video_cap: < cv2.VideoCapture 0x11ee854b0>, label_history: <queue.Queue object at 0x11eea6e10>, stats_history: <queue.Queue object at 0x11eea6910>, centroid_history: <queue.Queue object at 0x11eea7250>, \n",
      "\n",
      "current label: 1, current_stats: [1027  119   88   86 7537], current_centroid: [1070.38675866  161.48401221], stats_history: <queue.Queue object at 0x11eea6910>, centroid_history: <queue.Queue object at 0x11eea7250>, \n",
      "\n",
      "Current Stats \n",
      "\n",
      "[1027  119   88   86 7537]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Current Label \n",
      "\n",
      "1\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Current Centroid\n",
      "\n",
      "[1070.38675866  161.48401221]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "worn index: 2, video_cap: < cv2.VideoCapture 0x11ee854b0>, label_history: <queue.Queue object at 0x11eea7d50>, stats_history: <queue.Queue object at 0x11eeafdd0>, centroid_history: <queue.Queue object at 0x11da31a90>, \n",
      "\n",
      "current label: 2, current_stats: [  429   122   379   263 62111], current_centroid: [599.31598268 239.4958864 ], stats_history: <queue.Queue object at 0x11eeafdd0>, centroid_history: <queue.Queue object at 0x11da31a90>, \n",
      "\n",
      "Current Stats \n",
      "\n",
      "[  429   122   379   263 62111]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Current Label \n",
      "\n",
      "2\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Current Centroid\n",
      "\n",
      "[599.31598268 239.4958864 ]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "worn index: 3, video_cap: < cv2.VideoCapture 0x11ee854b0>, label_history: <queue.Queue object at 0x11ef0d610>, stats_history: <queue.Queue object at 0x11eec3f10>, centroid_history: <queue.Queue object at 0x11ee25f90>, \n",
      "\n",
      "current label: 3, current_stats: [  194   552   109   125 13189], current_centroid: [248.10144818 613.3115475 ], stats_history: <queue.Queue object at 0x11eec3f10>, centroid_history: <queue.Queue object at 0x11ee25f90>, \n",
      "\n",
      "Current Stats \n",
      "\n",
      "[  194   552   109   125 13189]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Current Label \n",
      "\n",
      "3\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Current Centroid\n",
      "\n",
      "[248.10144818 613.3115475 ]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "worn index: 4, video_cap: < cv2.VideoCapture 0x11ee854b0>, label_history: <queue.Queue object at 0x11eec2750>, stats_history: <queue.Queue object at 0x11eec3f90>, centroid_history: <queue.Queue object at 0x11eec2810>, \n",
      "\n",
      "current label: 4, current_stats: [ 1890   649   162   208 28281], current_centroid: [1970.73109155  743.5184753 ], stats_history: <queue.Queue object at 0x11eec3f90>, centroid_history: <queue.Queue object at 0x11eec2810>, \n",
      "\n",
      "Current Stats \n",
      "\n",
      "[ 1890   649   162   208 28281]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Current Label \n",
      "\n",
      "4\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Current Centroid\n",
      "\n",
      "[1970.73109155  743.5184753 ]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "worn index: 5, video_cap: < cv2.VideoCapture 0x11ee854b0>, label_history: <queue.Queue object at 0x11edccd90>, stats_history: <queue.Queue object at 0x11eeefd50>, centroid_history: <queue.Queue object at 0x11eeece50>, \n",
      "\n",
      "current label: 5, current_stats: [  670   705   230   310 46125], current_centroid: [780.69910027 864.7581355 ], stats_history: <queue.Queue object at 0x11eeefd50>, centroid_history: <queue.Queue object at 0x11eeece50>, \n",
      "\n",
      "Current Stats \n",
      "\n",
      "[  670   705   230   310 46125]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Current Label \n",
      "\n",
      "5\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Current Centroid\n",
      "\n",
      "[780.69910027 864.7581355 ]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "worn index: 6, video_cap: < cv2.VideoCapture 0x11ee854b0>, label_history: <queue.Queue object at 0x11edcf190>, stats_history: <queue.Queue object at 0x11eee8a50>, centroid_history: <queue.Queue object at 0x11eeeb410>, \n",
      "\n",
      "current label: 6, current_stats: [   69   848   118   189 20841], current_centroid: [129.23065112 944.04692673], stats_history: <queue.Queue object at 0x11eee8a50>, centroid_history: <queue.Queue object at 0x11eeeb410>, \n",
      "\n",
      "Current Stats \n",
      "\n",
      "[   69   848   118   189 20841]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Current Label \n",
      "\n",
      "6\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Current Centroid\n",
      "\n",
      "[129.23065112 944.04692673]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "worn index: 7, video_cap: < cv2.VideoCapture 0x11ee854b0>, label_history: <queue.Queue object at 0x11eee3110>, stats_history: <queue.Queue object at 0x11eee1610>, centroid_history: <queue.Queue object at 0x11eee10d0>, \n",
      "\n",
      "current label: 7, current_stats: [ 1048   860   141   144 17929], current_centroid: [1120.69418261  931.31836689], stats_history: <queue.Queue object at 0x11eee1610>, centroid_history: <queue.Queue object at 0x11eee10d0>, \n",
      "\n",
      "Current Stats \n",
      "\n",
      "[ 1048   860   141   144 17929]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Current Label \n",
      "\n",
      "7\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Current Centroid\n",
      "\n",
      "[1120.69418261  931.31836689]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "worn index: 8, video_cap: < cv2.VideoCapture 0x11ee854b0>, label_history: <queue.Queue object at 0x11eee0050>, stats_history: <queue.Queue object at 0x11eee2b90>, centroid_history: <queue.Queue object at 0x11eee1c10>, \n",
      "\n",
      "current label: 8, current_stats: [ 1883   901   233   596 79554], current_centroid: [1988.06056264 1185.98996908], stats_history: <queue.Queue object at 0x11eee2b90>, centroid_history: <queue.Queue object at 0x11eee1c10>, \n",
      "\n",
      "Current Stats \n",
      "\n",
      "[ 1883   901   233   596 79554]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Current Label \n",
      "\n",
      "8\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Current Centroid\n",
      "\n",
      "[1988.06056264 1185.98996908]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "worn index: 9, video_cap: < cv2.VideoCapture 0x11ee854b0>, label_history: <queue.Queue object at 0x11eee3c10>, stats_history: <queue.Queue object at 0x11eee1750>, centroid_history: <queue.Queue object at 0x11eea6190>, \n",
      "\n",
      "current label: 9, current_stats: [  455   950   110    93 10011], current_centroid: [509.40485466 996.04844671], stats_history: <queue.Queue object at 0x11eee1750>, centroid_history: <queue.Queue object at 0x11eea6190>, \n",
      "\n",
      "Current Stats \n",
      "\n",
      "[  455   950   110    93 10011]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Current Label \n",
      "\n",
      "9\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Current Centroid\n",
      "\n",
      "[509.40485466 996.04844671]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Frame Number: 6 \n",
      "Number of Labels 17\n",
      "Labels [[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n",
      "stats [[      0       0    2160    2160 3845960]\n",
      " [    425     123     397     267   57098]\n",
      " [    327     278      85      82    6937]\n",
      " ...\n",
      " [    231    1580     148     128   17436]\n",
      " [    541    1591     337     359   78658]\n",
      " [    385    1627     223     232   41013]]\n",
      "centroid [[1074.00021087 1029.17049683]\n",
      " [ 617.63856878  248.42047007]\n",
      " [ 369.00562203  318.6055932 ]\n",
      " [ 241.00559399  447.52907405]\n",
      " [ 234.33637489  631.99857857]\n",
      " [1988.49712845 1061.91055517]\n",
      " [  94.92513654  719.95883842]\n",
      " [ 784.86353267  870.38376529]\n",
      " [1096.34045132  907.20630281]\n",
      " [ 131.14996755  945.50997891]\n",
      " [ 527.00341228 1074.64923725]\n",
      " [1552.81099578 1171.14885884]\n",
      " [1060.19597674 1300.80867515]\n",
      " [1414.45199885 1875.97230703]\n",
      " [ 306.00154852 1645.06509521]\n",
      " [ 711.7786112  1772.67215032]\n",
      " [ 487.40562748 1729.43593495]]\n",
      "worn index: 1, video_cap: < cv2.VideoCapture 0x11ee854b0>, label_history: <queue.Queue object at 0x11ef0f250>, stats_history: <queue.Queue object at 0x11eea4bd0>, centroid_history: <queue.Queue object at 0x11eea5e90>, \n",
      "\n",
      "current label: 1, current_stats: [  425   123   397   267 57098], current_centroid: [617.63856878 248.42047007], stats_history: <queue.Queue object at 0x11eea4bd0>, centroid_history: <queue.Queue object at 0x11eea5e90>, \n",
      "\n",
      "Current Stats \n",
      "\n",
      "[  425   123   397   267 57098]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Current Label \n",
      "\n",
      "1\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Current Centroid\n",
      "\n",
      "[617.63856878 248.42047007]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "worn index: 2, video_cap: < cv2.VideoCapture 0x11ee854b0>, label_history: <queue.Queue object at 0x11eea7250>, stats_history: <queue.Queue object at 0x11eea5ed0>, centroid_history: <queue.Queue object at 0x11eea6310>, \n",
      "\n",
      "current label: 2, current_stats: [ 327  278   85   82 6937], current_centroid: [369.00562203 318.6055932 ], stats_history: <queue.Queue object at 0x11eea5ed0>, centroid_history: <queue.Queue object at 0x11eea6310>, \n",
      "\n",
      "Current Stats \n",
      "\n",
      "[ 327  278   85   82 6937]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Current Label \n",
      "\n",
      "2\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Current Centroid\n",
      "\n",
      "[369.00562203 318.6055932 ]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "worn index: 3, video_cap: < cv2.VideoCapture 0x11ee854b0>, label_history: <queue.Queue object at 0x11da31a90>, stats_history: <queue.Queue object at 0x11eea7d50>, centroid_history: <queue.Queue object at 0x11eeafd10>, \n",
      "\n",
      "current label: 3, current_stats: [ 200  407   83   82 6793], current_centroid: [241.00559399 447.52907405], stats_history: <queue.Queue object at 0x11eea7d50>, centroid_history: <queue.Queue object at 0x11eeafd10>, \n",
      "\n",
      "Current Stats \n",
      "\n",
      "[ 200  407   83   82 6793]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Current Label \n",
      "\n",
      "3\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Current Centroid\n",
      "\n",
      "[241.00559399 447.52907405]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "worn index: 4, video_cap: < cv2.VideoCapture 0x11ee854b0>, label_history: <queue.Queue object at 0x11ee25f90>, stats_history: <queue.Queue object at 0x11eea5c50>, centroid_history: <queue.Queue object at 0x11eeeda90>, \n",
      "\n",
      "current label: 4, current_stats: [  164   545   142   176 21809], current_centroid: [234.33637489 631.99857857], stats_history: <queue.Queue object at 0x11eea5c50>, centroid_history: <queue.Queue object at 0x11eeeda90>, \n",
      "\n",
      "Current Stats \n",
      "\n",
      "[  164   545   142   176 21809]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Current Label \n",
      "\n",
      "4\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Current Centroid\n",
      "\n",
      "[234.33637489 631.99857857]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "worn index: 5, video_cap: < cv2.VideoCapture 0x11ee854b0>, label_history: <queue.Queue object at 0x11eeed510>, stats_history: <queue.Queue object at 0x11eec3f90>, centroid_history: <queue.Queue object at 0x11eec2750>, \n",
      "\n",
      "current label: 5, current_stats: [  1883    627    234    871 130069], current_centroid: [1988.49712845 1061.91055517], stats_history: <queue.Queue object at 0x11eec3f90>, centroid_history: <queue.Queue object at 0x11eec2750>, \n",
      "\n",
      "Current Stats \n",
      "\n",
      "[  1883    627    234    871 130069]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Current Label \n",
      "\n",
      "5\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Current Centroid\n",
      "\n",
      "[1988.49712845 1061.91055517]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "worn index: 6, video_cap: < cv2.VideoCapture 0x11ee854b0>, label_history: <queue.Queue object at 0x11edccd90>, stats_history: <queue.Queue object at 0x11eeeef90>, centroid_history: <queue.Queue object at 0x11eeef750>, \n",
      "\n",
      "current label: 6, current_stats: [  53  676   85   89 7507], current_centroid: [ 94.92513654 719.95883842], stats_history: <queue.Queue object at 0x11eeeef90>, centroid_history: <queue.Queue object at 0x11eeef750>, \n",
      "\n",
      "Current Stats \n",
      "\n",
      "[  53  676   85   89 7507]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Current Label \n",
      "\n",
      "6\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Current Centroid\n",
      "\n",
      "[ 94.92513654 719.95883842]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "worn index: 7, video_cap: < cv2.VideoCapture 0x11ee854b0>, label_history: <queue.Queue object at 0x11edcf190>, stats_history: <queue.Queue object at 0x11eeebf90>, centroid_history: <queue.Queue object at 0x11eee9a10>, \n",
      "\n",
      "current label: 7, current_stats: [  668   698   243   341 53478], current_centroid: [784.86353267 870.38376529], stats_history: <queue.Queue object at 0x11eeebf90>, centroid_history: <queue.Queue object at 0x11eee9a10>, \n",
      "\n",
      "Current Stats \n",
      "\n",
      "[  668   698   243   341 53478]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Current Label \n",
      "\n",
      "7\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Current Centroid\n",
      "\n",
      "[784.86353267 870.38376529]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "worn index: 8, video_cap: < cv2.VideoCapture 0x11ee854b0>, label_history: <queue.Queue object at 0x11eee10d0>, stats_history: <queue.Queue object at 0x11eee1610>, centroid_history: <queue.Queue object at 0x11eee3110>, \n",
      "\n",
      "current label: 8, current_stats: [  994   816   200   190 28051], current_centroid: [1096.34045132  907.20630281], stats_history: <queue.Queue object at 0x11eee1610>, centroid_history: <queue.Queue object at 0x11eee3110>, \n",
      "\n",
      "Current Stats \n",
      "\n",
      "[  994   816   200   190 28051]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Current Label \n",
      "\n",
      "8\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Current Centroid\n",
      "\n",
      "[1096.34045132  907.20630281]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "worn index: 9, video_cap: < cv2.VideoCapture 0x11ee854b0>, label_history: <queue.Queue object at 0x11eee1c10>, stats_history: <queue.Queue object at 0x11eee2b90>, centroid_history: <queue.Queue object at 0x11eee0050>, \n",
      "\n",
      "current label: 9, current_stats: [   70   832   122   224 24652], current_centroid: [131.14996755 945.50997891], stats_history: <queue.Queue object at 0x11eee2b90>, centroid_history: <queue.Queue object at 0x11eee0050>, \n",
      "\n",
      "Current Stats \n",
      "\n",
      "[   70   832   122   224 24652]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Current Label \n",
      "\n",
      "9\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Current Centroid\n",
      "\n",
      "[131.14996755 945.50997891]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Frame Number: 7 \n",
      "Number of Labels 16\n",
      "Labels [[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n",
      "stats [[      0       0    2160    2160 3903095]\n",
      " [    413     123     398     274   55236]\n",
      " [    180     538     130     163   19323]\n",
      " ...\n",
      " [    634    1588     240     286   51471]\n",
      " [    382    1620     311     320   68492]\n",
      " [    915    1755     345     337   65816]]\n",
      "centroid [[1071.56815476 1032.33061506]\n",
      " [ 607.82904265  257.47523354]\n",
      " [ 244.02556539  618.97241629]\n",
      " [1988.23989669 1069.53827796]\n",
      " [ 779.87781673  862.86021037]\n",
      " [ 133.2437233   918.97076168]\n",
      " [1099.36373314  908.39208659]\n",
      " [ 509.83905278 1011.52344638]\n",
      " [1554.94242959 1167.99728811]\n",
      " [ 551.41625338 1155.84603246]\n",
      " [1052.68359223 1294.22434289]\n",
      " [1599.20514004 1828.55403385]\n",
      " [ 303.66128189 1647.57972606]\n",
      " [ 755.46902139 1724.20953547]\n",
      " [ 522.46742685 1777.02797407]\n",
      " [1126.24864775 1942.31667072]]\n",
      "worn index: 1, video_cap: < cv2.VideoCapture 0x11ee854b0>, label_history: <queue.Queue object at 0x11eec3950>, stats_history: <queue.Queue object at 0x11eee2c10>, centroid_history: <queue.Queue object at 0x11eee0310>, \n",
      "\n",
      "current label: 1, current_stats: [  413   123   398   274 55236], current_centroid: [607.82904265 257.47523354], stats_history: <queue.Queue object at 0x11eee2c10>, centroid_history: <queue.Queue object at 0x11eee0310>, \n",
      "\n",
      "Current Stats \n",
      "\n",
      "[  413   123   398   274 55236]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Current Label \n",
      "\n",
      "1\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Current Centroid\n",
      "\n",
      "[607.82904265 257.47523354]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "worn index: 2, video_cap: < cv2.VideoCapture 0x11ee854b0>, label_history: <queue.Queue object at 0x11ef0f250>, stats_history: <queue.Queue object at 0x11eea6d10>, centroid_history: <queue.Queue object at 0x11eea6cd0>, \n",
      "\n",
      "current label: 2, current_stats: [  180   538   130   163 19323], current_centroid: [244.02556539 618.97241629], stats_history: <queue.Queue object at 0x11eea6d10>, centroid_history: <queue.Queue object at 0x11eea6cd0>, \n",
      "\n",
      "Current Stats \n",
      "\n",
      "[  180   538   130   163 19323]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Current Label \n",
      "\n",
      "2\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Current Centroid\n",
      "\n",
      "[244.02556539 618.97241629]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "worn index: 3, video_cap: < cv2.VideoCapture 0x11ee854b0>, label_history: <queue.Queue object at 0x11eea6310>, stats_history: <queue.Queue object at 0x11eea6e10>, centroid_history: <queue.Queue object at 0x11eea6910>, \n",
      "\n",
      "current label: 3, current_stats: [  1882    639    234    862 127384], current_centroid: [1988.23989669 1069.53827796], stats_history: <queue.Queue object at 0x11eea6e10>, centroid_history: <queue.Queue object at 0x11eea6910>, \n",
      "\n",
      "Current Stats \n",
      "\n",
      "[  1882    639    234    862 127384]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Current Label \n",
      "\n",
      "3\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Current Centroid\n",
      "\n",
      "[1988.23989669 1069.53827796]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "worn index: 4, video_cap: < cv2.VideoCapture 0x11ee854b0>, label_history: <queue.Queue object at 0x11eea7d50>, stats_history: <queue.Queue object at 0x11eeaf450>, centroid_history: <queue.Queue object at 0x11da31a90>, \n",
      "\n",
      "current label: 4, current_stats: [  664   690   241   341 52765], current_centroid: [779.87781673 862.86021037], stats_history: <queue.Queue object at 0x11eeaf450>, centroid_history: <queue.Queue object at 0x11da31a90>, \n",
      "\n",
      "Current Stats \n",
      "\n",
      "[  664   690   241   341 52765]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Current Label \n",
      "\n",
      "4\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Current Centroid\n",
      "\n",
      "[779.87781673 862.86021037]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "worn index: 5, video_cap: < cv2.VideoCapture 0x11ee854b0>, label_history: <queue.Queue object at 0x11eeeda90>, stats_history: <queue.Queue object at 0x11ef0d610>, centroid_history: <queue.Queue object at 0x11eea70d0>, \n",
      "\n",
      "current label: 5, current_stats: [   71   784   121   263 28319], current_centroid: [133.2437233  918.97076168], stats_history: <queue.Queue object at 0x11ef0d610>, centroid_history: <queue.Queue object at 0x11eea70d0>, \n",
      "\n",
      "Current Stats \n",
      "\n",
      "[   71   784   121   263 28319]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Current Label \n",
      "\n",
      "5\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Current Centroid\n",
      "\n",
      "[133.2437233  918.97076168]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "worn index: 6, video_cap: < cv2.VideoCapture 0x11ee854b0>, label_history: <queue.Queue object at 0x11eec0f10>, stats_history: <queue.Queue object at 0x11eec3e90>, centroid_history: <queue.Queue object at 0x11eec3c50>, \n",
      "\n",
      "current label: 6, current_stats: [  998   811   201   196 28180], current_centroid: [1099.36373314  908.39208659], stats_history: <queue.Queue object at 0x11eec3e90>, centroid_history: <queue.Queue object at 0x11eec3c50>, \n",
      "\n",
      "Current Stats \n",
      "\n",
      "[  998   811   201   196 28180]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Current Label \n",
      "\n",
      "6\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Current Centroid\n",
      "\n",
      "[1099.36373314  908.39208659]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "worn index: 7, video_cap: < cv2.VideoCapture 0x11ee854b0>, label_history: <queue.Queue object at 0x11d5f0f90>, stats_history: <queue.Queue object at 0x11eeef750>, centroid_history: <queue.Queue object at 0x11eeeed50>, \n",
      "\n",
      "current label: 7, current_stats: [  444   948   128   125 14949], current_centroid: [ 509.83905278 1011.52344638], stats_history: <queue.Queue object at 0x11eeef750>, centroid_history: <queue.Queue object at 0x11eeeed50>, \n",
      "\n",
      "Current Stats \n",
      "\n",
      "[  444   948   128   125 14949]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Current Label \n",
      "\n",
      "7\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Current Centroid\n",
      "\n",
      "[ 509.83905278 1011.52344638]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "worn index: 8, video_cap: < cv2.VideoCapture 0x11ee854b0>, label_history: <queue.Queue object at 0x11edcf190>, stats_history: <queue.Queue object at 0x11eeea290>, centroid_history: <queue.Queue object at 0x11eeead10>, \n",
      "\n",
      "current label: 8, current_stats: [ 1486  1049   145   246 30606], current_centroid: [1554.94242959 1167.99728811], stats_history: <queue.Queue object at 0x11eeea290>, centroid_history: <queue.Queue object at 0x11eeead10>, \n",
      "\n",
      "Current Stats \n",
      "\n",
      "[ 1486  1049   145   246 30606]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Current Label \n",
      "\n",
      "8\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Current Centroid\n",
      "\n",
      "[1554.94242959 1167.99728811]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "worn index: 9, video_cap: < cv2.VideoCapture 0x11ee854b0>, label_history: <queue.Queue object at 0x11eee2690>, stats_history: <queue.Queue object at 0x11eee1490>, centroid_history: <queue.Queue object at 0x11eee3b90>, \n",
      "\n",
      "current label: 9, current_stats: [ 505 1108   94   96 8872], current_centroid: [ 551.41625338 1155.84603246], stats_history: <queue.Queue object at 0x11eee1490>, centroid_history: <queue.Queue object at 0x11eee3b90>, \n",
      "\n",
      "Current Stats \n",
      "\n",
      "[ 505 1108   94   96 8872]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Current Label \n",
      "\n",
      "9\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Current Centroid\n",
      "\n",
      "[ 551.41625338 1155.84603246]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Frame Number: 8 \n",
      "Number of Labels 15\n",
      "Labels [[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n",
      "stats [[      0       0    2160    2160 3906409]\n",
      " [    401     127     400     278   56442]\n",
      " [    182     528     134     167   20353]\n",
      " ...\n",
      " [    230    1571     145     145   19098]\n",
      " [    378    1586     491     361  122564]\n",
      " [   1004    1763     247     333   57243]]\n",
      "centroid [[1070.46422098 1033.15758104]\n",
      " [ 596.34206796  263.49448992]\n",
      " [ 248.00638726  610.1395863 ]\n",
      " [1980.04977359  748.17199701]\n",
      " [ 774.31737192  855.43009703]\n",
      " [1103.99304949  907.3333799 ]\n",
      " [ 130.58416382  944.58188309]\n",
      " [1994.75742224 1184.15445531]\n",
      " [ 530.00680633 1069.41505362]\n",
      " [1557.61188394 1164.18412162]\n",
      " [1046.10451853 1287.86965906]\n",
      " [1596.99876702 1826.90951177]\n",
      " [ 300.36794429 1642.66467693]\n",
      " [ 623.84358376 1754.53166509]\n",
      " [1145.9629125  1940.79934665]]\n",
      "worn index: 1, video_cap: < cv2.VideoCapture 0x11ee854b0>, label_history: <queue.Queue object at 0x11ef0cfd0>, stats_history: <queue.Queue object at 0x11eee2050>, centroid_history: <queue.Queue object at 0x11eee3790>, \n",
      "\n",
      "current label: 1, current_stats: [  401   127   400   278 56442], current_centroid: [596.34206796 263.49448992], stats_history: <queue.Queue object at 0x11eee2050>, centroid_history: <queue.Queue object at 0x11eee3790>, \n",
      "\n",
      "Current Stats \n",
      "\n",
      "[  401   127   400   278 56442]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Current Label \n",
      "\n",
      "1\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Current Centroid\n",
      "\n",
      "[596.34206796 263.49448992]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "worn index: 2, video_cap: < cv2.VideoCapture 0x11ee854b0>, label_history: <queue.Queue object at 0x10710d750>, stats_history: <queue.Queue object at 0x11eee1d10>, centroid_history: <queue.Queue object at 0x11eea6ad0>, \n",
      "\n",
      "current label: 2, current_stats: [  182   528   134   167 20353], current_centroid: [248.00638726 610.1395863 ], stats_history: <queue.Queue object at 0x11eee1d10>, centroid_history: <queue.Queue object at 0x11eea6ad0>, \n",
      "\n",
      "Current Stats \n",
      "\n",
      "[  182   528   134   167 20353]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Current Label \n",
      "\n",
      "2\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Current Centroid\n",
      "\n",
      "[248.00638726 610.1395863 ]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "worn index: 3, video_cap: < cv2.VideoCapture 0x11ee854b0>, label_history: <queue.Queue object at 0x11ef0f250>, stats_history: <queue.Queue object at 0x11eea5bd0>, centroid_history: <queue.Queue object at 0x11eea6790>, \n",
      "\n",
      "current label: 3, current_stats: [ 1898   657   170   190 28047], current_centroid: [1980.04977359  748.17199701], stats_history: <queue.Queue object at 0x11eea5bd0>, centroid_history: <queue.Queue object at 0x11eea6790>, \n",
      "\n",
      "Current Stats \n",
      "\n",
      "[ 1898   657   170   190 28047]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Current Label \n",
      "\n",
      "3\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Current Centroid\n",
      "\n",
      "[1980.04977359  748.17199701]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "worn index: 4, video_cap: < cv2.VideoCapture 0x11ee854b0>, label_history: <queue.Queue object at 0x11ee4c6d0>, stats_history: <queue.Queue object at 0x11eea7b10>, centroid_history: <queue.Queue object at 0x11eea7010>, \n",
      "\n",
      "current label: 4, current_stats: [  660   680   240   344 52251], current_centroid: [774.31737192 855.43009703], stats_history: <queue.Queue object at 0x11eea7b10>, centroid_history: <queue.Queue object at 0x11eea7010>, \n",
      "\n",
      "Current Stats \n",
      "\n",
      "[  660   680   240   344 52251]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Current Label \n",
      "\n",
      "4\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Current Centroid\n",
      "\n",
      "[774.31737192 855.43009703]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "worn index: 5, video_cap: < cv2.VideoCapture 0x11ee854b0>, label_history: <queue.Queue object at 0x11da31a90>, stats_history: <queue.Queue object at 0x11eeaf450>, centroid_history: <queue.Queue object at 0x11eeaf390>, \n",
      "\n",
      "current label: 5, current_stats: [ 1002   809   203   198 28631], current_centroid: [1103.99304949  907.3333799 ], stats_history: <queue.Queue object at 0x11eeaf450>, centroid_history: <queue.Queue object at 0x11eeaf390>, \n",
      "\n",
      "Current Stats \n",
      "\n",
      "[ 1002   809   203   198 28631]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Current Label \n",
      "\n",
      "5\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Current Centroid\n",
      "\n",
      "[1103.99304949  907.3333799 ]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "worn index: 6, video_cap: < cv2.VideoCapture 0x11ee854b0>, label_history: <queue.Queue object at 0x11eeb7590>, stats_history: <queue.Queue object at 0x11ee25f90>, centroid_history: <queue.Queue object at 0x11eeef210>, \n",
      "\n",
      "current label: 6, current_stats: [   73   853   117   184 20169], current_centroid: [130.58416382 944.58188309], stats_history: <queue.Queue object at 0x11ee25f90>, centroid_history: <queue.Queue object at 0x11eeef210>, \n",
      "\n",
      "Current Stats \n",
      "\n",
      "[   73   853   117   184 20169]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Current Label \n",
      "\n",
      "6\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Current Centroid\n",
      "\n",
      "[130.58416382 944.58188309]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "worn index: 7, video_cap: < cv2.VideoCapture 0x11ee854b0>, label_history: <queue.Queue object at 0x11eeeeb90>, stats_history: <queue.Queue object at 0x11eec3e90>, centroid_history: <queue.Queue object at 0x11eec0f10>, \n",
      "\n",
      "current label: 7, current_stats: [  1880    859    237    646 101654], current_centroid: [1994.75742224 1184.15445531], stats_history: <queue.Queue object at 0x11eec3e90>, centroid_history: <queue.Queue object at 0x11eec0f10>, \n",
      "\n",
      "Current Stats \n",
      "\n",
      "[  1880    859    237    646 101654]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Current Label \n",
      "\n",
      "7\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Current Centroid\n",
      "\n",
      "[1994.75742224 1184.15445531]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "worn index: 8, video_cap: < cv2.VideoCapture 0x11ee854b0>, label_history: <queue.Queue object at 0x11eeeed50>, stats_history: <queue.Queue object at 0x11eeef750>, centroid_history: <queue.Queue object at 0x11d5f0f90>, \n",
      "\n",
      "current label: 8, current_stats: [  444   947   164   257 30119], current_centroid: [ 530.00680633 1069.41505362], stats_history: <queue.Queue object at 0x11eeef750>, centroid_history: <queue.Queue object at 0x11d5f0f90>, \n",
      "\n",
      "Current Stats \n",
      "\n",
      "[  444   947   164   257 30119]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Current Label \n",
      "\n",
      "8\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Current Centroid\n",
      "\n",
      "[ 530.00680633 1069.41505362]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "worn index: 9, video_cap: < cv2.VideoCapture 0x11ee854b0>, label_history: <queue.Queue object at 0x11edcf190>, stats_history: <queue.Queue object at 0x11eeeb410>, centroid_history: <queue.Queue object at 0x11eeeb050>, \n",
      "\n",
      "current label: 9, current_stats: [ 1489  1045   144   247 30192], current_centroid: [1557.61188394 1164.18412162], stats_history: <queue.Queue object at 0x11eeeb410>, centroid_history: <queue.Queue object at 0x11eeeb050>, \n",
      "\n",
      "Current Stats \n",
      "\n",
      "[ 1489  1045   144   247 30192]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Current Label \n",
      "\n",
      "9\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Current Centroid\n",
      "\n",
      "[1557.61188394 1164.18412162]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Frame Number: 9 \n",
      "Number of Labels 16\n",
      "Labels [[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n",
      "stats [[      0       0    2160    2160 3908988]\n",
      " [    394     129     395     283   53268]\n",
      " [    190     519     132     168   20094]\n",
      " ...\n",
      " [    223    1567     142     142   18479]\n",
      " [    640    1586     225     265   49417]\n",
      " [    376    1605     303     327   67611]]\n",
      "centroid [[1072.80893853 1031.92111053]\n",
      " [ 586.18269881  268.10595479]\n",
      " [ 254.44889022  601.56718423]\n",
      " [1983.25660563  756.8313052 ]\n",
      " [ 769.29690046  846.39485135]\n",
      " [ 134.31331827  917.87278775]\n",
      " [1108.87182533  909.39744978]\n",
      " [1992.00398764 1179.1942983 ]\n",
      " [ 511.43531896 1015.96660369]\n",
      " [1558.76276005 1159.74727407]\n",
      " [ 551.69270426 1147.40017185]\n",
      " [1039.83845261 1282.02475449]\n",
      " [1430.29344331 1868.68855107]\n",
      " [ 292.83164673 1637.90334975]\n",
      " [ 754.44280308 1718.99856325]\n",
      " [ 517.12749405 1763.67420982]]\n",
      "worn index: 1, video_cap: < cv2.VideoCapture 0x11ee854b0>, label_history: <queue.Queue object at 0x11eea6050>, stats_history: <queue.Queue object at 0x11eee2250>, centroid_history: <queue.Queue object at 0x11eee3ed0>, \n",
      "\n",
      "current label: 1, current_stats: [  394   129   395   283 53268], current_centroid: [586.18269881 268.10595479], stats_history: <queue.Queue object at 0x11eee2250>, centroid_history: <queue.Queue object at 0x11eee3ed0>, \n",
      "\n",
      "Current Stats \n",
      "\n",
      "[  394   129   395   283 53268]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Current Label \n",
      "\n",
      "1\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Current Centroid\n",
      "\n",
      "[586.18269881 268.10595479]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "worn index: 2, video_cap: < cv2.VideoCapture 0x11ee854b0>, label_history: <queue.Queue object at 0x11eeefb90>, stats_history: <queue.Queue object at 0x11eee1c10>, centroid_history: <queue.Queue object at 0x11eee0b50>, \n",
      "\n",
      "current label: 2, current_stats: [  190   519   132   168 20094], current_centroid: [254.44889022 601.56718423], stats_history: <queue.Queue object at 0x11eee1c10>, centroid_history: <queue.Queue object at 0x11eee0b50>, \n",
      "\n",
      "Current Stats \n",
      "\n",
      "[  190   519   132   168 20094]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Current Label \n",
      "\n",
      "2\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Current Centroid\n",
      "\n",
      "[254.44889022 601.56718423]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "worn index: 3, video_cap: < cv2.VideoCapture 0x11ee854b0>, label_history: <queue.Queue object at 0x11ee2fe10>, stats_history: <queue.Queue object at 0x11eee11d0>, centroid_history: <queue.Queue object at 0x11eead1d0>, \n",
      "\n",
      "current label: 3, current_stats: [ 1902   669   168   180 26379], current_centroid: [1983.25660563  756.8313052 ], stats_history: <queue.Queue object at 0x11eee11d0>, centroid_history: <queue.Queue object at 0x11eead1d0>, \n",
      "\n",
      "Current Stats \n",
      "\n",
      "[ 1902   669   168   180 26379]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Current Label \n",
      "\n",
      "3\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Current Centroid\n",
      "\n",
      "[1983.25660563  756.8313052 ]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "worn index: 4, video_cap: < cv2.VideoCapture 0x11ee854b0>, label_history: <queue.Queue object at 0x11ef0f250>, stats_history: <queue.Queue object at 0x11eea4bd0>, centroid_history: <queue.Queue object at 0x11eea6990>, \n",
      "\n",
      "current label: 4, current_stats: [  657   670   237   345 52169], current_centroid: [769.29690046 846.39485135], stats_history: <queue.Queue object at 0x11eea4bd0>, centroid_history: <queue.Queue object at 0x11eea6990>, \n",
      "\n",
      "Current Stats \n",
      "\n",
      "[  657   670   237   345 52169]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Current Label \n",
      "\n",
      "4\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Current Centroid\n",
      "\n",
      "[769.29690046 846.39485135]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "worn index: 5, video_cap: < cv2.VideoCapture 0x11ee854b0>, label_history: <queue.Queue object at 0x11ee4c6d0>, stats_history: <queue.Queue object at 0x11eea5e50>, centroid_history: <queue.Queue object at 0x11eea73d0>, \n",
      "\n",
      "current label: 5, current_stats: [   73   805   118   220 23449], current_centroid: [134.31331827 917.87278775], stats_history: <queue.Queue object at 0x11eea5e50>, centroid_history: <queue.Queue object at 0x11eea73d0>, \n",
      "\n",
      "Current Stats \n",
      "\n",
      "[   73   805   118   220 23449]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Current Label \n",
      "\n",
      "5\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Current Centroid\n",
      "\n",
      "[134.31331827 917.87278775]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "worn index: 6, video_cap: < cv2.VideoCapture 0x11ee854b0>, label_history: <queue.Queue object at 0x11da31a90>, stats_history: <queue.Queue object at 0x11eeaf390>, centroid_history: <queue.Queue object at 0x11eeafdd0>, \n",
      "\n",
      "current label: 6, current_stats: [ 1007   811   204   196 28625], current_centroid: [1108.87182533  909.39744978], stats_history: <queue.Queue object at 0x11eeaf390>, centroid_history: <queue.Queue object at 0x11eeafdd0>, \n",
      "\n",
      "Current Stats \n",
      "\n",
      "[ 1007   811   204   196 28625]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Current Label \n",
      "\n",
      "6\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Current Centroid\n",
      "\n",
      "[1108.87182533  909.39744978]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "worn index: 7, video_cap: < cv2.VideoCapture 0x11ee854b0>, label_history: <queue.Queue object at 0x11ee25f90>, stats_history: <queue.Queue object at 0x11ef0d610>, centroid_history: <queue.Queue object at 0x11eec3750>, \n",
      "\n",
      "current label: 7, current_stats: [ 1879   865   237   642 95796], current_centroid: [1992.00398764 1179.1942983 ], stats_history: <queue.Queue object at 0x11ef0d610>, centroid_history: <queue.Queue object at 0x11eec3750>, \n",
      "\n",
      "Current Stats \n",
      "\n",
      "[ 1879   865   237   642 95796]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Current Label \n",
      "\n",
      "7\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Current Centroid\n",
      "\n",
      "[1992.00398764 1179.1942983 ]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "worn index: 8, video_cap: < cv2.VideoCapture 0x11ee854b0>, label_history: <queue.Queue object at 0x11eeeeb90>, stats_history: <queue.Queue object at 0x11eec3650>, centroid_history: <queue.Queue object at 0x11eec1810>, \n",
      "\n",
      "current label: 8, current_stats: [  444   946   134   141 17996], current_centroid: [ 511.43531896 1015.96660369], stats_history: <queue.Queue object at 0x11eec3650>, centroid_history: <queue.Queue object at 0x11eec1810>, \n",
      "\n",
      "Current Stats \n",
      "\n",
      "[  444   946   134   141 17996]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Current Label \n",
      "\n",
      "8\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Current Centroid\n",
      "\n",
      "[ 511.43531896 1015.96660369]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "worn index: 9, video_cap: < cv2.VideoCapture 0x11ee854b0>, label_history: <queue.Queue object at 0x11d5f0f90>, stats_history: <queue.Queue object at 0x11eeef750>, centroid_history: <queue.Queue object at 0x11eeeed50>, \n",
      "\n",
      "current label: 9, current_stats: [ 1490  1041   145   246 29898], current_centroid: [1558.76276005 1159.74727407], stats_history: <queue.Queue object at 0x11eeef750>, centroid_history: <queue.Queue object at 0x11eeeed50>, \n",
      "\n",
      "Current Stats \n",
      "\n",
      "[ 1490  1041   145   246 29898]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Current Label \n",
      "\n",
      "9\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Current Centroid\n",
      "\n",
      "[1558.76276005 1159.74727407]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Frame Number: 10 \n",
      "Number of Labels 13\n",
      "Labels [[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n",
      "stats [[      0       0    2160    2160 3887414]\n",
      " [    386     130     397     289   57361]\n",
      " [    196     512     131     163   19508]\n",
      " ...\n",
      " [    872    1106     346     327   63158]\n",
      " [    215    1563     654     347  148504]\n",
      " [   1025    1562     860     538  193154]]\n",
      "centroid [[1075.49046153 1032.08463673]\n",
      " [ 580.82118513  272.81585049]\n",
      " [ 260.01855649  591.86841296]\n",
      " [ 765.0929136   837.99144744]\n",
      " [1986.61520677  765.39697167]\n",
      " [ 138.0440687   891.3119092 ]\n",
      " [1116.28867918  916.18734103]\n",
      " [1991.20997511 1185.89515483]\n",
      " [ 527.65705365 1067.20731276]\n",
      " [1560.10025368 1156.93144006]\n",
      " [1035.6204598  1275.59335318]\n",
      " [ 570.30740586 1732.91262188]\n",
      " [1444.89824182 1861.32907939]]\n",
      "worn index: 1, video_cap: < cv2.VideoCapture 0x11ee854b0>, label_history: <queue.Queue object at 0x11edcf190>, stats_history: <queue.Queue object at 0x11eeeb050>, centroid_history: <queue.Queue object at 0x11eeea550>, \n",
      "\n",
      "current label: 1, current_stats: [  386   130   397   289 57361], current_centroid: [580.82118513 272.81585049], stats_history: <queue.Queue object at 0x11eeeb050>, centroid_history: <queue.Queue object at 0x11eeea550>, \n",
      "\n",
      "Current Stats \n",
      "\n",
      "[  386   130   397   289 57361]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Current Label \n",
      "\n",
      "1\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Current Centroid\n",
      "\n",
      "[580.82118513 272.81585049]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "worn index: 2, video_cap: < cv2.VideoCapture 0x11ee854b0>, label_history: <queue.Queue object at 0x11eee3ed0>, stats_history: <queue.Queue object at 0x11eee2250>, centroid_history: <queue.Queue object at 0x11eee1490>, \n",
      "\n",
      "current label: 2, current_stats: [  196   512   131   163 19508], current_centroid: [260.01855649 591.86841296], stats_history: <queue.Queue object at 0x11eee2250>, centroid_history: <queue.Queue object at 0x11eee1490>, \n",
      "\n",
      "Current Stats \n",
      "\n",
      "[  196   512   131   163 19508]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Current Label \n",
      "\n",
      "2\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Current Centroid\n",
      "\n",
      "[260.01855649 591.86841296]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "worn index: 3, video_cap: < cv2.VideoCapture 0x11ee854b0>, label_history: <queue.Queue object at 0x11eee6f10>, stats_history: <queue.Queue object at 0x11eee2b90>, centroid_history: <queue.Queue object at 0x11eee06d0>, \n",
      "\n",
      "current label: 3, current_stats: [  653   662   235   345 52382], current_centroid: [765.0929136  837.99144744], stats_history: <queue.Queue object at 0x11eee2b90>, centroid_history: <queue.Queue object at 0x11eee06d0>, \n",
      "\n",
      "Current Stats \n",
      "\n",
      "[  653   662   235   345 52382]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Current Label \n",
      "\n",
      "3\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Current Centroid\n",
      "\n",
      "[765.0929136  837.99144744]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "worn index: 4, video_cap: < cv2.VideoCapture 0x11ee854b0>, label_history: <queue.Queue object at 0x11eead1d0>, stats_history: <queue.Queue object at 0x11ee2fe10>, centroid_history: <queue.Queue object at 0x11eee3c10>, \n",
      "\n",
      "current label: 4, current_stats: [ 1903   668   170   203 30710], current_centroid: [1986.61520677  765.39697167], stats_history: <queue.Queue object at 0x11ee2fe10>, centroid_history: <queue.Queue object at 0x11eee3c10>, \n",
      "\n",
      "Current Stats \n",
      "\n",
      "[ 1903   668   170   203 30710]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Current Label \n",
      "\n",
      "4\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Current Centroid\n",
      "\n",
      "[1986.61520677  765.39697167]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "worn index: 5, video_cap: < cv2.VideoCapture 0x11ee854b0>, label_history: <queue.Queue object at 0x11ef0f250>, stats_history: <queue.Queue object at 0x11eee7090>, centroid_history: <queue.Queue object at 0x11eea6d10>, \n",
      "\n",
      "current label: 5, current_stats: [   69   759   137   261 30044], current_centroid: [138.0440687 891.3119092], stats_history: <queue.Queue object at 0x11eee7090>, centroid_history: <queue.Queue object at 0x11eea6d10>, \n",
      "\n",
      "Current Stats \n",
      "\n",
      "[   69   759   137   261 30044]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Current Label \n",
      "\n",
      "5\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Current Centroid\n",
      "\n",
      "[138.0440687 891.3119092]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "worn index: 6, video_cap: < cv2.VideoCapture 0x11ee854b0>, label_history: <queue.Queue object at 0x11ee4c6d0>, stats_history: <queue.Queue object at 0x11eea7250>, centroid_history: <queue.Queue object at 0x11eea5ed0>, \n",
      "\n",
      "current label: 6, current_stats: [ 1018   819   198   189 27127], current_centroid: [1116.28867918  916.18734103], stats_history: <queue.Queue object at 0x11eea7250>, centroid_history: <queue.Queue object at 0x11eea5ed0>, \n",
      "\n",
      "Current Stats \n",
      "\n",
      "[ 1018   819   198   189 27127]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Current Label \n",
      "\n",
      "6\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Current Centroid\n",
      "\n",
      "[1116.28867918  916.18734103]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "worn index: 7, video_cap: < cv2.VideoCapture 0x11ee854b0>, label_history: <queue.Queue object at 0x11eeafdd0>, stats_history: <queue.Queue object at 0x11eeaf250>, centroid_history: <queue.Queue object at 0x11d9a1e10>, \n",
      "\n",
      "current label: 7, current_stats: [ 1876   876   243   634 94816], current_centroid: [1991.20997511 1185.89515483], stats_history: <queue.Queue object at 0x11eeaf250>, centroid_history: <queue.Queue object at 0x11d9a1e10>, \n",
      "\n",
      "Current Stats \n",
      "\n",
      "[ 1876   876   243   634 94816]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Current Label \n",
      "\n",
      "7\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Current Centroid\n",
      "\n",
      "[1991.20997511 1185.89515483]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "worn index: 8, video_cap: < cv2.VideoCapture 0x11ee854b0>, label_history: <queue.Queue object at 0x11eec3750>, stats_history: <queue.Queue object at 0x11eeda990>, centroid_history: <queue.Queue object at 0x11eeedd90>, \n",
      "\n",
      "current label: 8, current_stats: [  446   945   160   256 31069], current_centroid: [ 527.65705365 1067.20731276], stats_history: <queue.Queue object at 0x11eeda990>, centroid_history: <queue.Queue object at 0x11eeedd90>, \n",
      "\n",
      "Current Stats \n",
      "\n",
      "[  446   945   160   256 31069]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Current Label \n",
      "\n",
      "8\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Current Centroid\n",
      "\n",
      "[ 527.65705365 1067.20731276]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "worn index: 9, video_cap: < cv2.VideoCapture 0x11ee854b0>, label_history: <queue.Queue object at 0x11eec3c50>, stats_history: <queue.Queue object at 0x11eec2590>, centroid_history: <queue.Queue object at 0x11eec0f10>, \n",
      "\n",
      "current label: 9, current_stats: [ 1490  1037   147   247 30353], current_centroid: [1560.10025368 1156.93144006], stats_history: <queue.Queue object at 0x11eec2590>, centroid_history: <queue.Queue object at 0x11eec0f10>, \n",
      "\n",
      "Current Stats \n",
      "\n",
      "[ 1490  1037   147   247 30353]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Current Label \n",
      "\n",
      "9\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Current Centroid\n",
      "\n",
      "[1560.10025368 1156.93144006]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Frame Number: 11 \n",
      "Number of Labels 14\n",
      "Labels [[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n",
      "stats [[      0       0    2160    2160 3859617]\n",
      " [    370     137     407     290   59871]\n",
      " [    200     499     136     164   20248]\n",
      " ...\n",
      " [    211    1559     151     161   21364]\n",
      " [    946    1561     975     546  214325]\n",
      " [    367    1586     508     356  131070]]\n",
      "centroid [[1076.2925982  1027.47270001]\n",
      " [ 566.44059728  280.09936363]\n",
      " [ 266.8726294   579.80630186]\n",
      " [ 760.015141    828.97264489]\n",
      " [1990.59945513  772.27326489]\n",
      " [ 142.28123055  881.13691315]\n",
      " [1116.13400683  915.01826886]\n",
      " [1990.45658272 1190.81964552]\n",
      " [ 528.54212513 1062.85292504]\n",
      " [1561.32162894 1153.10217147]\n",
      " [1029.451756   1267.97199046]\n",
      " [ 287.27532297 1636.14641453]\n",
      " [1433.85142657 1866.04122244]\n",
      " [ 610.63453117 1749.39996185]]\n",
      "worn index: 1, video_cap: < cv2.VideoCapture 0x11ee854b0>, label_history: <queue.Queue object at 0x11ee5f190>, stats_history: <queue.Queue object at 0x11eeece50>, centroid_history: <queue.Queue object at 0x11ee24b90>, \n",
      "\n",
      "current label: 1, current_stats: [  370   137   407   290 59871], current_centroid: [566.44059728 280.09936363], stats_history: <queue.Queue object at 0x11eeece50>, centroid_history: <queue.Queue object at 0x11ee24b90>, \n",
      "\n",
      "Current Stats \n",
      "\n",
      "[  370   137   407   290 59871]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Current Label \n",
      "\n",
      "1\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Current Centroid\n",
      "\n",
      "[566.44059728 280.09936363]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "worn index: 2, video_cap: < cv2.VideoCapture 0x11ee854b0>, label_history: <queue.Queue object at 0x11eeea550>, stats_history: <queue.Queue object at 0x11eeeb050>, centroid_history: <queue.Queue object at 0x11edcf190>, \n",
      "\n",
      "current label: 2, current_stats: [  200   499   136   164 20248], current_centroid: [266.8726294  579.80630186], stats_history: <queue.Queue object at 0x11eeeb050>, centroid_history: <queue.Queue object at 0x11edcf190>, \n",
      "\n",
      "Current Stats \n",
      "\n",
      "[  200   499   136   164 20248]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Current Label \n",
      "\n",
      "2\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Current Centroid\n",
      "\n",
      "[266.8726294  579.80630186]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "worn index: 3, video_cap: < cv2.VideoCapture 0x11ee854b0>, label_history: <queue.Queue object at 0x11eee4e50>, stats_history: <queue.Queue object at 0x11eee2250>, centroid_history: <queue.Queue object at 0x11eee3ed0>, \n",
      "\n",
      "current label: 3, current_stats: [  646   652   237   349 53299], current_centroid: [760.015141   828.97264489], stats_history: <queue.Queue object at 0x11eee2250>, centroid_history: <queue.Queue object at 0x11eee3ed0>, \n",
      "\n",
      "Current Stats \n",
      "\n",
      "[  646   652   237   349 53299]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Current Label \n",
      "\n",
      "3\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Current Centroid\n",
      "\n",
      "[760.015141   828.97264489]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "worn index: 4, video_cap: < cv2.VideoCapture 0x11ee854b0>, label_history: <queue.Queue object at 0x11ef0ee50>, stats_history: <queue.Queue object at 0x11eee3790>, centroid_history: <queue.Queue object at 0x11eee3f90>, \n",
      "\n",
      "current label: 4, current_stats: [ 1907   678   170   196 30099], current_centroid: [1990.59945513  772.27326489], stats_history: <queue.Queue object at 0x11eee3790>, centroid_history: <queue.Queue object at 0x11eee3f90>, \n",
      "\n",
      "Current Stats \n",
      "\n",
      "[ 1907   678   170   196 30099]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Current Label \n",
      "\n",
      "4\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Current Centroid\n",
      "\n",
      "[1990.59945513  772.27326489]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "worn index: 5, video_cap: < cv2.VideoCapture 0x11ee854b0>, label_history: <queue.Queue object at 0x11eec3950>, stats_history: <queue.Queue object at 0x11eee11d0>, centroid_history: <queue.Queue object at 0x11eee7790>, \n",
      "\n",
      "current label: 5, current_stats: [   73   753   142   258 30523], current_centroid: [142.28123055 881.13691315], stats_history: <queue.Queue object at 0x11eee11d0>, centroid_history: <queue.Queue object at 0x11eee7790>, \n",
      "\n",
      "Current Stats \n",
      "\n",
      "[   73   753   142   258 30523]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Current Label \n",
      "\n",
      "5\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Current Centroid\n",
      "\n",
      "[142.28123055 881.13691315]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "worn index: 6, video_cap: < cv2.VideoCapture 0x11ee854b0>, label_history: <queue.Queue object at 0x11eee7090>, stats_history: <queue.Queue object at 0x11eea6110>, centroid_history: <queue.Queue object at 0x11ef0f250>, \n",
      "\n",
      "current label: 6, current_stats: [ 1011   818   211   191 28409], current_centroid: [1116.13400683  915.01826886], stats_history: <queue.Queue object at 0x11eea6110>, centroid_history: <queue.Queue object at 0x11ef0f250>, \n",
      "\n",
      "Current Stats \n",
      "\n",
      "[ 1011   818   211   191 28409]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Current Label \n",
      "\n",
      "6\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Current Centroid\n",
      "\n",
      "[1116.13400683  915.01826886]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "worn index: 7, video_cap: < cv2.VideoCapture 0x11ee854b0>, label_history: <queue.Queue object at 0x11ee4c6d0>, stats_history: <queue.Queue object at 0x11eea6310>, centroid_history: <queue.Queue object at 0x11eea5e50>, \n",
      "\n",
      "current label: 7, current_stats: [ 1874   884   246   631 93039], current_centroid: [1990.45658272 1190.81964552], stats_history: <queue.Queue object at 0x11eea6310>, centroid_history: <queue.Queue object at 0x11eea5e50>, \n",
      "\n",
      "Current Stats \n",
      "\n",
      "[ 1874   884   246   631 93039]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Current Label \n",
      "\n",
      "7\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Current Centroid\n",
      "\n",
      "[1990.45658272 1190.81964552]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "worn index: 8, video_cap: < cv2.VideoCapture 0x11ee854b0>, label_history: <queue.Queue object at 0x11d9a1e10>, stats_history: <queue.Queue object at 0x11da31a90>, centroid_history: <queue.Queue object at 0x11eeafdd0>, \n",
      "\n",
      "current label: 8, current_stats: [  444   945   163   251 30991], current_centroid: [ 528.54212513 1062.85292504], stats_history: <queue.Queue object at 0x11da31a90>, centroid_history: <queue.Queue object at 0x11eeafdd0>, \n",
      "\n",
      "Current Stats \n",
      "\n",
      "[  444   945   163   251 30991]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Current Label \n",
      "\n",
      "8\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Current Centroid\n",
      "\n",
      "[ 528.54212513 1062.85292504]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "worn index: 9, video_cap: < cv2.VideoCapture 0x11ee854b0>, label_history: <queue.Queue object at 0x11eea5c50>, stats_history: <queue.Queue object at 0x11eeedd90>, centroid_history: <queue.Queue object at 0x11eec3750>, \n",
      "\n",
      "current label: 9, current_stats: [ 1491  1034   148   246 30302], current_centroid: [1561.32162894 1153.10217147], stats_history: <queue.Queue object at 0x11eeedd90>, centroid_history: <queue.Queue object at 0x11eec3750>, \n",
      "\n",
      "Current Stats \n",
      "\n",
      "[ 1491  1034   148   246 30302]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Current Label \n",
      "\n",
      "9\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Current Centroid\n",
      "\n",
      "[1561.32162894 1153.10217147]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Frame Number: 12 \n",
      "Number of Labels 13\n",
      "Labels [[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n",
      "stats [[      0       0    2160    2160 3863958]\n",
      " [    362     137     405     296   61503]\n",
      " [    207     493     135     161   19529]\n",
      " ...\n",
      " [    863    1091     340     330   59940]\n",
      " [    210    1556     670     347  148778]\n",
      " [    939    1561     981     548  213708]]\n",
      "centroid [[1075.50977288 1028.69940564]\n",
      " [ 558.80851341  284.07892298]\n",
      " [ 273.69890931  572.58200625]\n",
      " [ 755.13550549  820.46627892]\n",
      " [1993.34862385  779.74050289]\n",
      " [ 145.28791713  875.72514087]\n",
      " [1118.44604539  916.45522696]\n",
      " [1990.65533764 1203.48983084]\n",
      " [ 529.14574475 1059.54546347]\n",
      " [1562.80452585 1149.17842698]\n",
      " [1025.14117451 1260.40587254]\n",
      " [ 561.92321445 1724.44103967]\n",
      " [1429.7368559  1866.78910476]]\n",
      "worn index: 1, video_cap: < cv2.VideoCapture 0x11ee854b0>, label_history: <queue.Queue object at 0x11eed4d90>, stats_history: <queue.Queue object at 0x11eec3650>, centroid_history: <queue.Queue object at 0x11eec03d0>, \n",
      "\n",
      "current label: 1, current_stats: [  362   137   405   296 61503], current_centroid: [558.80851341 284.07892298], stats_history: <queue.Queue object at 0x11eec3650>, centroid_history: <queue.Queue object at 0x11eec03d0>, \n",
      "\n",
      "Current Stats \n",
      "\n",
      "[  362   137   405   296 61503]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Current Label \n",
      "\n",
      "1\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Current Centroid\n",
      "\n",
      "[558.80851341 284.07892298]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "worn index: 2, video_cap: < cv2.VideoCapture 0x11ee854b0>, label_history: <queue.Queue object at 0x11ee24b90>, stats_history: <queue.Queue object at 0x11eeece50>, centroid_history: <queue.Queue object at 0x11eeef7d0>, \n",
      "\n",
      "current label: 2, current_stats: [  207   493   135   161 19529], current_centroid: [273.69890931 572.58200625], stats_history: <queue.Queue object at 0x11eeece50>, centroid_history: <queue.Queue object at 0x11eeef7d0>, \n",
      "\n",
      "Current Stats \n",
      "\n",
      "[  207   493   135   161 19529]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Current Label \n",
      "\n",
      "2\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Current Centroid\n",
      "\n",
      "[273.69890931 572.58200625]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "worn index: 3, video_cap: < cv2.VideoCapture 0x11ee854b0>, label_history: <queue.Queue object at 0x11edcf190>, stats_history: <queue.Queue object at 0x11eeeb050>, centroid_history: <queue.Queue object at 0x11eeea550>, \n",
      "\n",
      "current label: 3, current_stats: [  641   644   235   350 53127], current_centroid: [755.13550549 820.46627892], stats_history: <queue.Queue object at 0x11eeeb050>, centroid_history: <queue.Queue object at 0x11eeea550>, \n",
      "\n",
      "Current Stats \n",
      "\n",
      "[  641   644   235   350 53127]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Current Label \n",
      "\n",
      "3\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Current Centroid\n",
      "\n",
      "[755.13550549 820.46627892]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "worn index: 4, video_cap: < cv2.VideoCapture 0x11ee854b0>, label_history: <queue.Queue object at 0x11eee4e50>, stats_history: <queue.Queue object at 0x11eee0ed0>, centroid_history: <queue.Queue object at 0x11eee2790>, \n",
      "\n",
      "current label: 4, current_stats: [ 1910   687   170   191 29430], current_centroid: [1993.34862385  779.74050289], stats_history: <queue.Queue object at 0x11eee0ed0>, centroid_history: <queue.Queue object at 0x11eee2790>, \n",
      "\n",
      "Current Stats \n",
      "\n",
      "[ 1910   687   170   191 29430]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Current Label \n",
      "\n",
      "4\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Current Centroid\n",
      "\n",
      "[1993.34862385  779.74050289]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "worn index: 5, video_cap: < cv2.VideoCapture 0x11ee854b0>, label_history: <queue.Queue object at 0x11eeeec50>, stats_history: <queue.Queue object at 0x11eee3cd0>, centroid_history: <queue.Queue object at 0x11eee1c10>, \n",
      "\n",
      "current label: 5, current_stats: [   75   751   145   251 29637], current_centroid: [145.28791713 875.72514087], stats_history: <queue.Queue object at 0x11eee3cd0>, centroid_history: <queue.Queue object at 0x11eee1c10>, \n",
      "\n",
      "Current Stats \n",
      "\n",
      "[   75   751   145   251 29637]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Current Label \n",
      "\n",
      "5\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Current Centroid\n",
      "\n",
      "[145.28791713 875.72514087]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "worn index: 6, video_cap: < cv2.VideoCapture 0x11ee854b0>, label_history: <queue.Queue object at 0x10710d750>, stats_history: <queue.Queue object at 0x11eee6f90>, centroid_history: <queue.Queue object at 0x11eee1810>, \n",
      "\n",
      "current label: 6, current_stats: [ 1014   819   212   191 29080], current_centroid: [1118.44604539  916.45522696], stats_history: <queue.Queue object at 0x11eee6f90>, centroid_history: <queue.Queue object at 0x11eee1810>, \n",
      "\n",
      "Current Stats \n",
      "\n",
      "[ 1014   819   212   191 29080]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Current Label \n",
      "\n",
      "6\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Current Centroid\n",
      "\n",
      "[1118.44604539  916.45522696]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "worn index: 7, video_cap: < cv2.VideoCapture 0x11ee854b0>, label_history: <queue.Queue object at 0x11ef0f250>, stats_history: <queue.Queue object at 0x11eee7090>, centroid_history: <queue.Queue object at 0x11eea6990>, \n",
      "\n",
      "current label: 7, current_stats: [ 1872   898   249   621 96419], current_centroid: [1990.65533764 1203.48983084], stats_history: <queue.Queue object at 0x11eee7090>, centroid_history: <queue.Queue object at 0x11eea6990>, \n",
      "\n",
      "Current Stats \n",
      "\n",
      "[ 1872   898   249   621 96419]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Current Label \n",
      "\n",
      "7\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Current Centroid\n",
      "\n",
      "[1990.65533764 1203.48983084]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "worn index: 8, video_cap: < cv2.VideoCapture 0x11ee854b0>, label_history: <queue.Queue object at 0x11ee26d50>, stats_history: <queue.Queue object at 0x11eea73d0>, centroid_history: <queue.Queue object at 0x11eea6e10>, \n",
      "\n",
      "current label: 8, current_stats: [  445   944   162   246 30574], current_centroid: [ 529.14574475 1059.54546347], stats_history: <queue.Queue object at 0x11eea73d0>, centroid_history: <queue.Queue object at 0x11eea6e10>, \n",
      "\n",
      "Current Stats \n",
      "\n",
      "[  445   944   162   246 30574]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Current Label \n",
      "\n",
      "8\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Current Centroid\n",
      "\n",
      "[ 529.14574475 1059.54546347]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "worn index: 9, video_cap: < cv2.VideoCapture 0x11ee854b0>, label_history: <queue.Queue object at 0x11da31a90>, stats_history: <queue.Queue object at 0x11d298a90>, centroid_history: <queue.Queue object at 0x11eeaf390>, \n",
      "\n",
      "current label: 9, current_stats: [ 1492  1032   149   243 29917], current_centroid: [1562.80452585 1149.17842698], stats_history: <queue.Queue object at 0x11d298a90>, centroid_history: <queue.Queue object at 0x11eeaf390>, \n",
      "\n",
      "Current Stats \n",
      "\n",
      "[ 1492  1032   149   243 29917]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Current Label \n",
      "\n",
      "9\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Current Centroid\n",
      "\n",
      "[1562.80452585 1149.17842698]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Frame Number: 13 \n",
      "Number of Labels 12\n",
      "Labels [[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n",
      "stats [[      0       0    2160    2160 3862271]\n",
      " [    213     140     544     506   91647]\n",
      " [    635     636     235     350   52884]\n",
      " ...\n",
      " [    853    1086     340     328   60341]\n",
      " [   1001    1538     904     565  203628]\n",
      " [    208    1553     677     356  155446]]\n",
      "centroid [[1078.75483802 1031.26050166]\n",
      " [ 474.7242572   364.63155368]\n",
      " [ 750.17806898  812.13879434]\n",
      " [1997.305077    791.13582027]\n",
      " [ 148.77749929  863.35339593]\n",
      " [1122.01365911  918.69164465]\n",
      " [ 530.51674395 1058.56072901]\n",
      " [1990.10320607 1237.40881669]\n",
      " [1563.31140483 1145.34658389]\n",
      " [1016.3079001  1253.03622744]\n",
      " [1459.08039661 1854.55168248]\n",
      " [ 560.38499543 1726.96784092]]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 1 is out of bounds for axis 0 with size 1",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mIndexError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[35], line 28\u001B[0m\n\u001B[1;32m     26\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;241m1\u001B[39m, \u001B[38;5;28mlen\u001B[39m(w_list)):\n\u001B[1;32m     27\u001B[0m     temp_Worm \u001B[38;5;241m=\u001B[39m Worm(i, original_cap, i, stats[i], centroids[i])\n\u001B[0;32m---> 28\u001B[0m     temp_Worm\u001B[38;5;241m.\u001B[39mtake_the_bundle_in(bundle)\n\u001B[1;32m     29\u001B[0m     temp_Worm\u001B[38;5;241m.\u001B[39mwrite_frame(iteration_Number)\n\u001B[1;32m     30\u001B[0m     w_list[i] \u001B[38;5;241m=\u001B[39m temp_Worm\n",
      "Cell \u001B[0;32mIn[18], line 77\u001B[0m, in \u001B[0;36mWorm.take_the_bundle_in\u001B[0;34m(self, bundle)\u001B[0m\n\u001B[1;32m     75\u001B[0m this_label_selected \u001B[38;5;241m=\u001B[39m get_nth_item(overlap_labels, j)\n\u001B[1;32m     76\u001B[0m this_centroid \u001B[38;5;241m=\u001B[39m centroids[this_label_selected]\n\u001B[0;32m---> 77\u001B[0m this_distance \u001B[38;5;241m=\u001B[39m distance_between_points(this_centroid[\u001B[38;5;241m0\u001B[39m], this_centroid[\u001B[38;5;241m1\u001B[39m], \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcurrent_centroid[\u001B[38;5;241m0\u001B[39m], \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcurrent_centroid[\u001B[38;5;241m1\u001B[39m])\n\u001B[1;32m     78\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m this_distance \u001B[38;5;241m<\u001B[39m centroid_distance:\n\u001B[1;32m     79\u001B[0m     centroid_distance \u001B[38;5;241m=\u001B[39m this_distance\n",
      "\u001B[0;31mIndexError\u001B[0m: index 1 is out of bounds for axis 0 with size 1"
     ]
    }
   ],
   "source": [
    "for iteration_Number in range(3, int(cap.get(cv2.CAP_PROP_FRAME_COUNT)-1)):\n",
    "\n",
    "    print(f'\\rFrame Number: {iteration_Number} \\n', end='')\n",
    "    # Read the second frame\n",
    "    ret2, frame2 = cap.read()\n",
    "    next = cv2.cvtColor(frame2, cv2.COLOR_BGR2GRAY)\n",
    "    # Read the original frame\n",
    "    ret3, ori_frame = original_cap.read()\n",
    "\n",
    "    # Calculate Optical Flow\n",
    "    flow = cv2.calcOpticalFlowFarneback(prvs, next, None, 0.5, 3, 15, 3, 5, 1.2, 0)\n",
    "    # Compute magnitude and angle of the flow\n",
    "    mag, ang = cv2.cartToPolar(flow[..., 0], flow[..., 1])\n",
    "    # Create motion mask\n",
    "    thresh = 1  # Set threshold for motion detection\n",
    "    motion_mask = cv2.threshold(mag, thresh, 255, cv2.THRESH_BINARY)[1]\n",
    "    eroded_frame = cv2.erode(motion_mask, erode_kernel, iterations=1)\n",
    "    dilated_frame = cv2.dilate(eroded_frame, dilate_kernel, iterations=1)\n",
    "    bundle = cv2.connectedComponentsWithStats(np.uint8(dilated_frame), connectivity=8)\n",
    "    num_labels, labels, stats, centroids = cv2.connectedComponentsWithStats(np.uint8(dilated_frame), connectivity=8)\n",
    "    print('Number of Labels', num_labels)\n",
    "    print('Labels', labels)\n",
    "    print('stats', stats)\n",
    "    print('centroid', centroids)\n",
    "    # process the worm list\n",
    "    for i in range(1, len(w_list)):\n",
    "        temp_Worm = Worm(i, original_cap, i, stats[i], centroids[i])\n",
    "        temp_Worm.take_the_bundle_in(bundle)\n",
    "        temp_Worm.write_frame(iteration_Number)\n",
    "        w_list[i] = temp_Worm\n",
    "\n",
    "    for i in range(1, num_labels):\n",
    "        x, y, w, h, area = stats[i]\n",
    "        if area > min_area:\n",
    "            cv2.rectangle(dilated_frame, (x, y), (x + w, y + h), (255, 255, 255), 2)\n",
    "    for i in range(1, num_labels):\n",
    "        x, y, w, h, area = stats[i]\n",
    "        if area > min_area:\n",
    "            cv2.rectangle(ori_frame, (x, y), (x + w, y + h), (250,128,114), 2)\n",
    "    temp_frame = cv2.normalize(dilated_frame, None, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX, dtype=cv2.CV_8U)\n",
    "    rgb_frame = cv2.cvtColor(temp_frame, cv2.COLOR_GRAY2RGB)\n",
    "    prvs = next\n",
    "    cv2.imwrite(os.path.join(monitor_folder, f\"frame_{iteration_Number}.jpg\"), rgb_frame)\n",
    "    cv2.imwrite(os.path.join(output_folder, f\"frame_{iteration_Number}.jpg\"), ori_frame)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
