{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Computer Vision Algorithm for Tracking Planarian Motion\n",
    "developed by Hokin Deng xueqiandeng@yahoo.com"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Import Dependencies and Data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Import packages and make sure of the python technicality"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python Version: 3.11.4 (main, Jul  5 2023, 08:41:25) [Clang 14.0.6 ]\n",
      "Python Executable: /Users/billdeng/anaconda3/envs/unlearning_Version1/bin/python\n",
      "Python Path: ['/Users/billdeng/PycharmProjects/unicellular', '/Users/billdeng/PycharmProjects/unicellular', '/Users/billdeng/anaconda3/envs/unlearning_Version1/lib/python311.zip', '/Users/billdeng/anaconda3/envs/unlearning_Version1/lib/python3.11', '/Users/billdeng/anaconda3/envs/unlearning_Version1/lib/python3.11/lib-dynload', '', '/Users/billdeng/anaconda3/envs/unlearning_Version1/lib/python3.11/site-packages']\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "print(\"Python Version:\", sys.version)\n",
    "print(\"Python Executable:\", sys.executable)\n",
    "print(\"Python Path:\", sys.path)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "import cv2\n",
    "import math\n",
    "import copy\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import numpy as np\n",
    "import argparse\n",
    "import imutils\n",
    "import matplotlib.pyplot as plt\n",
    "np.set_printoptions(threshold=sys.maxsize)\n",
    "np.set_printoptions(threshold=50)\n",
    "import queue\n",
    "import math"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The video data should be named as \"sample.avi\" and put in the folder as the notebook."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Open a video file (replace 'sample.avi' with the path)\n",
    "cap = cv2.VideoCapture('sample.avi')\n",
    "\n",
    "# Check if the video file was opened successfully\n",
    "if not cap.isOpened():\n",
    "    print(\"Error: Could not open video file.\")\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Display the video if want to have a look at it"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "#cv2.startWindowThread()\n",
    "# Loop to read and display frames\n",
    "#while True:\n",
    "    # Read a frame from the video\n",
    "#    ret, frame = cap.read()\n",
    "    # If the video has ended, break out of the loop\n",
    "#    if not ret:\n",
    "#        break\n",
    "    # Display the frame in a window\n",
    "#    cv2.imshow('Video', frame)\n",
    "    # Exit the loop if the 'q' key is pressed\n",
    "#    if cv2.waitKey(25) & 0xFF == ord('q'):\n",
    "#        break\n",
    "\n",
    "# Release the video capture object and close the window"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "First, get basic properties about our video"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frame Count: 1202\n",
      "Frame Width: 2160\n",
      "Frame Height: 2160\n",
      "Frame Rate: 10.0 frames per second\n",
      "Video Duration: 120.20 seconds\n"
     ]
    }
   ],
   "source": [
    "# Get basic video properties\n",
    "frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "frame_rate = cap.get(cv2.CAP_PROP_FPS)\n",
    "video_duration_sec = frame_count / frame_rate if frame_rate > 0 else 0\n",
    "\n",
    "# Print the video properties\n",
    "print(f\"Frame Count: {frame_count}\")\n",
    "print(f\"Frame Width: {frame_width}\")\n",
    "print(f\"Frame Height: {frame_height}\")\n",
    "print(f\"Frame Rate: {frame_rate} frames per second\")\n",
    "print(f\"Video Duration: {video_duration_sec:.2f} seconds\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ret <class 'bool'>\n",
      "frame <class 'numpy.ndarray'>\n",
      "ret True\n",
      "frame [[[255 255 255]\n",
      "  [255 255 255]\n",
      "  [255 255 255]\n",
      "  ...\n",
      "  [255 255 255]\n",
      "  [255 255 255]\n",
      "  [255 255 255]]\n",
      "\n",
      " [[255 255 255]\n",
      "  [255 255 255]\n",
      "  [255 255 255]\n",
      "  ...\n",
      "  [255 255 255]\n",
      "  [255 255 255]\n",
      "  [255 255 255]]\n",
      "\n",
      " [[255 255 255]\n",
      "  [255 255 255]\n",
      "  [255 255 255]\n",
      "  ...\n",
      "  [255 255 255]\n",
      "  [255 255 255]\n",
      "  [255 255 255]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[255 255 255]\n",
      "  [255 255 255]\n",
      "  [255 255 255]\n",
      "  ...\n",
      "  [255 255 255]\n",
      "  [255 255 255]\n",
      "  [255 255 255]]\n",
      "\n",
      " [[255 255 255]\n",
      "  [255 255 255]\n",
      "  [255 255 255]\n",
      "  ...\n",
      "  [255 255 255]\n",
      "  [255 255 255]\n",
      "  [255 255 255]]\n",
      "\n",
      " [[255 255 255]\n",
      "  [255 255 255]\n",
      "  [255 255 255]\n",
      "  ...\n",
      "  [255 255 255]\n",
      "  [255 255 255]\n",
      "  [255 255 255]]]\n"
     ]
    }
   ],
   "source": [
    "example_ret, example_frame = cap.read()\n",
    "print( 'ret', type(example_ret))\n",
    "print( 'frame', type(example_frame))\n",
    "print('ret', example_ret)\n",
    "print('frame', example_frame)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "# frame_number = 100  # The frame you want to access\n",
    "# Set the video position to the desired frame\n",
    "# cap.set(cv2.CAP_PROP_POS_FRAMES, frame_number - 1)\n",
    "# Read the frame\n",
    "# ret, frame = cap.read()\n",
    "# if ret:\n",
    "    # Process the frame\n",
    "#     cv2.imshow('Frame 100', frame)\n",
    "#    cv2.waitKey(0)  # Wait for a key press to close the image window\n",
    "# else:\n",
    "#    print(\"Error: Unable to read the frame\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Inspect function for a frame"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "def inspect_frame_from_video(cap_for_here, frame_number):\n",
    "    \"\"\"\n",
    "    Inspects a specific frame in a video.\n",
    "\n",
    "    :param cap_for_here: video reference\n",
    "    :param frame_number: The frame number to inspect (1-based index).\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    # Open the video file\n",
    "    if not cap_for_here.isOpened():\n",
    "        print(\"Error: Unable to open video file\")\n",
    "        return\n",
    "    # Check if the frame number is valid\n",
    "    total_frames = cap_for_here.get(cv2.CAP_PROP_FRAME_COUNT)\n",
    "    if frame_number < 1 or frame_number > total_frames:\n",
    "        print(f\"Frame number should be between 1 and {int(total_frames)}\")\n",
    "        return\n",
    "    # Set the video position to the desired frame (0-based index for frame_number)\n",
    "    cap_for_here.set(cv2.CAP_PROP_POS_FRAMES, frame_number - 1)\n",
    "    # Read the frame\n",
    "    ret_temp, frame_temp = cap_for_here.read()\n",
    "    if ret_temp:\n",
    "        # Display the frame\n",
    "        # cv2.imshow(f'Frame {frame_number}', frame)\n",
    "        # cv2.waitKey(0)  # Wait for a key press to close the image window\n",
    "        # cv2.destroyAllWindows()\n",
    "        frame_h, frame_w = frame_temp.shape[:2]\n",
    "        print(f\"Frame Width: {frame_h}\")\n",
    "        print(f\"Frame Height: {frame_w}\")\n",
    "        # Color Channels\n",
    "        channels = frame_temp.shape[2] if len(frame_temp.shape) == 3 else 1\n",
    "        print(f\"Color Channels: {channels}\")\n",
    "        # Data Type\n",
    "        data_type = frame_temp.dtype\n",
    "        print(f\"Data Type: {data_type}\")\n",
    "        # Aspect Ratio\n",
    "        aspect_ratio = frame_w / frame_h\n",
    "        print(f\"Aspect Ratio: {aspect_ratio}\")\n",
    "        # Resolution (assuming a standard display resolution of 96 PPI)\n",
    "        # Color Space (assuming default BGR)\n",
    "        color_space = \"BGR\" if channels == 3 else \"Grayscale\"\n",
    "        print(f\"Color Space: {color_space}\")\n",
    "        # Histogram for each channel\n",
    "        if channels > 1:\n",
    "            for i, col in enumerate(['Blue', 'Green', 'Red']):\n",
    "                hist = cv2.calcHist([frame_temp], [i], None, [256], [0, 256])\n",
    "                print(f\"Histogram for {col} channel: {np.array(hist).flatten()}\")\n",
    "        else:\n",
    "            hist = cv2.calcHist([frame_temp], [0], None, [256], [0, 256])\n",
    "            print(f\"Histogram for Grayscale: {np.array(hist).flatten()}\")\n",
    "    else:\n",
    "        print(\"Error: Unable to read the frame\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "def inspect_given_a_frame(this_frame):\n",
    "    frame_h, frame_w = this_frame.shape[:2]\n",
    "    print(f\"Frame Width: {frame_h}\")\n",
    "    print(f\"Frame Height: {frame_w}\")\n",
    "    channels = this_frame.shape[2] if len(this_frame.shape) == 3 else 1\n",
    "    print(f\"Color Channels: {channels}\")\n",
    "    data_type = this_frame.dtype\n",
    "    print(f\"Data Type: {data_type}\")\n",
    "    aspect_ratio = frame_w / frame_h\n",
    "    print(f\"Aspect Ratio: {aspect_ratio}\")\n",
    "    color_space = \"BGR\" if channels == 3 else \"Grayscale\"\n",
    "    print(f\"Color Space: {color_space}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frame Width: 2160\n",
      "Frame Height: 2160\n",
      "Color Channels: 3\n",
      "Data Type: uint8\n",
      "Aspect Ratio: 1.0\n",
      "Color Space: BGR\n",
      "Histogram for Blue channel: [      0.       0.       0. ...   31073.   57326. 3775011.]\n",
      "Histogram for Green channel: [      0.       0.       0. ...   31073.   57326. 3775011.]\n",
      "Histogram for Red channel: [      0.       0.       0. ...   31073.   57326. 3775011.]\n"
     ]
    }
   ],
   "source": [
    "# Usage example\n",
    "video_file = 'sample.avi'\n",
    "frame_to_inspect = 100  # Adjust the frame number as needed\n",
    "inspect_frame_from_video(cap, frame_to_inspect)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frame Width: 2160\n",
      "Frame Height: 2160\n",
      "Color Channels: 3\n",
      "Data Type: uint8\n",
      "Aspect Ratio: 1.0\n",
      "Color Space: BGR\n"
     ]
    }
   ],
   "source": [
    "ret, frame_for_use = cap.read()\n",
    "inspect_given_a_frame(frame_for_use)\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Pre-Processing\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Background subtraction"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "i = 0\n",
    "# Initialize video capture\n",
    "cap = cv2.VideoCapture('sample.avi')\n",
    "# Get video properties\n",
    "frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "frame_rate = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "# Define codec and create VideoWriter object\n",
    "fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "out = cv2.VideoWriter('background_remove.avi', fourcc, frame_rate, (frame_width, frame_height), False)\n",
    "backSub = cv2.createBackgroundSubtractorMOG2()\n",
    "while True:\n",
    "    i += 1\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    fgMask = backSub.apply(frame)\n",
    "    out.write(fgMask)\n",
    "    print(f'\\rProgress: {i}', end='')\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "cap.release()\n",
    "out.release()\n",
    "cv2.destroyAllWindows()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Spatial Smoothing"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "i = 0\n",
    "cap = cv2.VideoCapture('background_remove.avi')\n",
    "frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "frame_rate = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "out = cv2.VideoWriter('background_remove_spatial_smoothed.avi', fourcc, frame_rate, (frame_width, frame_height), False)\n",
    "while True:\n",
    "    i += 1\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    blurred_frame = cv2.GaussianBlur(frame, (5, 5), 0)\n",
    "    out.write(blurred_frame)\n",
    "    print(f'\\rProgress: {i}', end='')\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "cap.release()\n",
    "out.release()\n",
    "cv2.destroyAllWindows()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Temporal Smoothing"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture('background_remove_spatial_smooth.avi')\n",
    "fourcc = cv2.VideoWriter_fourcc(*'XVID') # You can also use 'XVID'\n",
    "out = cv2.VideoWriter('b_r_s_s_t_s.avi', fourcc, frame_rate, (frame_width, frame_height), False)\n",
    "buffer_size = 5\n",
    "frame_buffer = []\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    frame_buffer.append(frame)\n",
    "    if len(frame_buffer) > buffer_size:\n",
    "        frame_buffer.pop(0)\n",
    "    # Temporal smoothing (average of frames in buffer)\n",
    "    temp_smoothed = np.mean(frame_buffer, axis=0).astype(np.uint8)\n",
    "    # Write frame to video\n",
    "    out.write(temp_smoothed)\n",
    "    # Break the loop\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "cap.release()\n",
    "out.release()\n",
    "cv2.destroyAllWindows()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "do everything all at once"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Initialize video capture\n",
    "cap = cv2.VideoCapture('sample.avi')\n",
    "# Get video properties\n",
    "frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "frame_rate = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "# Define codec and create VideoWriter object\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v') # You can also use 'XVID'\n",
    "out = cv2.VideoWriter('pre_dense.mp4', fourcc, frame_rate, (frame_width, frame_height), False)\n",
    "# Background subtractor\n",
    "backSub = cv2.createBackgroundSubtractorMOG2()\n",
    "# Buffer for temporal smoothing\n",
    "buffer_size = 5\n",
    "frame_buffer = []\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    # Background subtraction\n",
    "    fgMask = backSub.apply(frame)\n",
    "    # Spatial smoothing (Gaussian blur)\n",
    "    blurred = cv2.GaussianBlur(fgMask, (5, 5), 0)\n",
    "    # Add frame to buffer for temporal smoothing\n",
    "    frame_buffer.append(blurred)\n",
    "    if len(frame_buffer) > buffer_size:\n",
    "        frame_buffer.pop(0)\n",
    "    # Temporal smoothing (average of frames in buffer)\n",
    "    temp_smoothed = np.mean(frame_buffer, axis=0).astype(np.uint8)\n",
    "    # Write frame to video\n",
    "    out.write(temp_smoothed)\n",
    "    # Display result\n",
    "    # cv2.imshow('Frame', frame)\n",
    "    # cv2.imshow('FG Mask', fgMask)\n",
    "    # cv2.imshow('Blurred', blurred)\n",
    "    # cv2.imshow('Temporally Smoothed', temp_smoothed)\n",
    "    # Break the loop\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "# Release everything\n",
    "cap.release()\n",
    "out.release()\n",
    "cv2.destroyAllWindows()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "dense opti flow\n",
    "this takes a very very long time"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(\"pre_dense.mp4\")\n",
    "if not cap.isOpened():\n",
    "    print(\"Error opening video file\")\n",
    "    exit()\n",
    "frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "frame_rate = cap.get(cv2.CAP_PROP_FPS)\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')  # Try 'XVID' if 'mp4v' does not work\n",
    "out = cv2.VideoWriter('dense_opti_flow_v2.mp4', fourcc, frame_rate, (frame_width, frame_height), True)\n",
    "ret, first_frame = cap.read()\n",
    "if not ret:\n",
    "    print(\"Error reading first frame\")\n",
    "    cap.release()\n",
    "    exit()\n",
    "prev_gray = cv2.cvtColor(first_frame, cv2.COLOR_BGR2GRAY)\n",
    "mask = np.zeros_like(first_frame)\n",
    "mask[..., 1] = 255\n",
    "i = 0\n",
    "while True:\n",
    "    i += 1\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    flow = cv2.calcOpticalFlowFarneback(prev_gray, gray, None, 0.5, 3, 15, 3, 5, 1.2, 0)\n",
    "    magnitude, angle = cv2.cartToPolar(flow[..., 0], flow[..., 1])\n",
    "    mask[..., 0] = angle * 180 / np.pi / 2\n",
    "    mask[..., 2] = cv2.normalize(magnitude, None, 0, 255, cv2.NORM_MINMAX)\n",
    "    rgb = cv2.cvtColor(mask, cv2.COLOR_HSV2BGR)\n",
    "    out.write(rgb)\n",
    "    print(f'\\rProgress: {i}', end='')\n",
    "    prev_gray = gray\n",
    "cap.release()\n",
    "out.release()\n",
    "cv2.destroyAllWindows()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "segment each worm in dense optical flow video"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture('dense_opti_flow_v2.mp4')\n",
    "frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "frame_rate = cap.get(cv2.CAP_PROP_FPS)\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')  # Try 'XVID' if 'mp4v' does not work\n",
    "out = cv2.VideoWriter('dense_opt_segmented.mp4', fourcc, frame_rate, (frame_width, frame_height), True)\n",
    "# Read the first frame\n",
    "ret, frame1 = cap.read()\n",
    "if not ret:\n",
    "    print(\"Failed to read video\")\n",
    "    cap.release()\n",
    "prvs = cv2.cvtColor(frame1, cv2.COLOR_BGR2GRAY)\n",
    "j = 0\n",
    "while True:\n",
    "    j += 1\n",
    "    ret, frame2 = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    next = cv2.cvtColor(frame2, cv2.COLOR_BGR2GRAY)\n",
    "    # Calculate Optical Flow\n",
    "    flow = cv2.calcOpticalFlowFarneback(prvs, next, None, 0.5, 3, 15, 3, 5, 1.2, 0)\n",
    "    # Compute magnitude and angle of the flow\n",
    "    mag, ang = cv2.cartToPolar(flow[..., 0], flow[..., 1])\n",
    "    # Create motion mask\n",
    "    thresh = 3  # Set threshold for motion detection\n",
    "    motion_mask = cv2.threshold(mag, thresh, 255, cv2.THRESH_BINARY)[1]\n",
    "    # Segment mask based on connected components\n",
    "    num_labels, labels, stats, centroids = cv2.connectedComponentsWithStats(np.uint8(motion_mask), connectivity=8)\n",
    "    min_area = 50  # Minimum area for connected components\n",
    "    # Draw bounding boxes around components\n",
    "    for i in range(1, num_labels):\n",
    "        x, y, w, h, area = stats[i]\n",
    "        if area > min_area:\n",
    "            cv2.rectangle(frame2, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "    # Display the result\n",
    "    out.write(frame2)\n",
    "    # cv2.imshow('Segmented Frame', frame2)\n",
    "    # Update previous frame\n",
    "    print(f'\\rProgress: {j}', end='')\n",
    "    prvs = next\n",
    "    # Break the loop if 'q' is pressed\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "cap.release()\n",
    "out.release()\n",
    "cv2.destroyAllWindows()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "segment each worm without optical flow processing"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture('pre_dense.mp4')\n",
    "frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "frame_rate = cap.get(cv2.CAP_PROP_FPS)\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')  # Try 'XVID' if 'mp4v' does not work\n",
    "out = cv2.VideoWriter('predense_degmented.mp4', fourcc, frame_rate, (frame_width, frame_height), True)\n",
    "# Read the first frame\n",
    "ret, frame1 = cap.read()\n",
    "if not ret:\n",
    "    print(\"Failed to read video\")\n",
    "    cap.release()\n",
    "prvs = cv2.cvtColor(frame1, cv2.COLOR_BGR2GRAY)\n",
    "j = 0\n",
    "while True:\n",
    "    j += 1\n",
    "    ret, frame2 = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    next = cv2.cvtColor(frame2, cv2.COLOR_BGR2GRAY)\n",
    "    # Calculate Optical Flow\n",
    "    flow = cv2.calcOpticalFlowFarneback(prvs, next, None, 0.5, 3, 15, 3, 5, 1.2, 0)\n",
    "    # Compute magnitude and angle of the flow\n",
    "    mag, ang = cv2.cartToPolar(flow[..., 0], flow[..., 1])\n",
    "    # Create motion mask\n",
    "    thresh = 3  # Set threshold for motion detection\n",
    "    motion_mask = cv2.threshold(mag, thresh, 255, cv2.THRESH_BINARY)[1]\n",
    "    # Segment mask based on connected components\n",
    "    num_labels, labels, stats, centroids = cv2.connectedComponentsWithStats(np.uint8(motion_mask), connectivity=8)\n",
    "    min_area = 50  # Minimum area for connected components\n",
    "    # Draw bounding boxes around components\n",
    "    for i in range(1, num_labels):\n",
    "        x, y, w, h, area = stats[i]\n",
    "        if area > min_area:\n",
    "            cv2.rectangle(frame2, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "    # Display the result\n",
    "    out.write(frame2)\n",
    "    # cv2.imshow('Segmented Frame', frame2)\n",
    "    # Update previous frame\n",
    "    print(f'\\rProgress: {j}', end='')\n",
    "    prvs = next\n",
    "    # Break the loop if 'q' is pressed\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "cap.release()\n",
    "out.release()\n",
    "cv2.destroyAllWindows()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Define a function for video to image and use it"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Define a function to extract and save frames from a video file\n",
    "current_location = os.getcwd();\n",
    "output_folder = current_location + '/raw_images'\n",
    "video_path = current_location + '/sample.avi'\n",
    "\n",
    "def video2image(video_path, output_folder):\n",
    "    # Create the output folder if it doesn't exist\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "\n",
    "    # Load the video\n",
    "    video = cv2.VideoCapture(video_path)\n",
    "\n",
    "    # Initialize frame count\n",
    "    count = 0\n",
    "\n",
    "    # Iterate through video frames\n",
    "    while True:\n",
    "        # Read a frame\n",
    "        success, frame = video.read()\n",
    "        # Break if no frame is read (end of video)\n",
    "        if not success:\n",
    "            break\n",
    "        # Save the frame as an image\n",
    "        cv2.imwrite(os.path.join(output_folder, f\"frame_{count}.jpg\"), frame)\n",
    "        # Increment frame count\n",
    "        count += 1\n",
    "\n",
    "    # Release the video object\n",
    "    video.release()\n",
    "\n",
    "    return count"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Save the video into images"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "frame_count = video2image(video_path, output_folder)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Design a crop function that remove the dish"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def crop_circle(image_path):\n",
    "    # Read the image\n",
    "    image = cv2.imread(image_path)\n",
    "    # Create a mask with the same dimensions as the image\n",
    "    mask = np.zeros_like(image)\n",
    "    rows, cols, _ = mask.shape\n",
    "    # Compute the center and radius of the circle\n",
    "    center = (cols // 2, rows // 2)\n",
    "    radius = min(center[0], center[1], rows - center[1], cols - center[0])\n",
    "    # Draw the circular mask\n",
    "    cv2.circle(mask, center, radius, (255, 255, 255), -1)\n",
    "    # Apply the mask\n",
    "    circular_image = cv2.bitwise_and(image, mask)\n",
    "    # Optionally, you can remove the black background\n",
    "    masked_data = cv2.cvtColor(circular_image, cv2.COLOR_BGR2BGRA)\n",
    "    masked_data[mask == 0] = [0, 0, 0, 0]\n",
    "    # Save or display the result\n",
    "    cv2.imwrite('circular_image.png', masked_data)\n",
    "    # cv2.imshow('Circular Image', masked_data)\n",
    "    # cv2.waitKey(0)\n",
    "    # cv2.destroyAllWindows()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "threshold the original video and set the threshold"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "thresh = 127"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "apply to original video"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Load the video\n",
    "cap = cv2.VideoCapture('sample.avi')\n",
    "frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "frame_rate = cap.get(cv2.CAP_PROP_FPS)\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')  # Try 'XVID' if 'mp4v' does not work\n",
    "out = cv2.VideoWriter('threshold_original.mp4', fourcc, frame_rate, (frame_width, frame_height), True)\n",
    "# Check if video opened successfully\n",
    "if not cap.isOpened():\n",
    "    print(\"Error opening video file\")\n",
    "# Read until video is completed\n",
    "j = 0\n",
    "while cap.isOpened():\n",
    "    j += 1\n",
    "    # Capture frame-by-frame\n",
    "    ret, frame = cap.read()\n",
    "    if ret:\n",
    "        # Convert to grayscale\n",
    "        # Apply threshold for each channel\n",
    "        _, red_channel = cv2.threshold(frame[:,:,0], thresh, 255, cv2.THRESH_BINARY)\n",
    "        _, green_channel = cv2.threshold(frame[:,:,1], thresh, 255, cv2.THRESH_BINARY)\n",
    "        _, blue_channel = cv2.threshold(frame[:,:,2], thresh, 255, cv2.THRESH_BINARY)\n",
    "    # Combine the channels back\n",
    "        thresh_frame = cv2.merge([red_channel, green_channel, blue_channel])\n",
    "        # Display the resulting frame\n",
    "        out.write(thresh_frame)\n",
    "        # Press Q on keyboard to exit\n",
    "        if cv2.waitKey(25) & 0xFF == ord('q'):\n",
    "            break\n",
    "        print(f'\\rProgress: {j}', end='')\n",
    "    else:\n",
    "        break\n",
    "# When everything done, release the video capture object\n",
    "cap.release()\n",
    "out.release()\n",
    "cv2.destroyAllWindows()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "apply erode and dilate"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Load the video\n",
    "cap = cv2.VideoCapture('pre_dense.mp4')\n",
    "kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (7, 7))\n",
    "frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "frame_rate = cap.get(cv2.CAP_PROP_FPS)\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')  # Try 'XVID' if 'mp4v' does not work\n",
    "out = cv2.VideoWriter('pre_dense_erode_dilate.mp4', fourcc, frame_rate, (frame_width, frame_height), True)\n",
    "j = 0\n",
    "while True:\n",
    "    j += 1\n",
    "    # Read each frame\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    # Apply erosion and then dilation\n",
    "    eroded_frame = cv2.erode(frame, kernel, iterations=1)\n",
    "    dilated_frame = cv2.dilate(eroded_frame, kernel, iterations=1)\n",
    "    # Display the processed frame\n",
    "    out.write(dilated_frame)\n",
    "    print(f'\\rProgress: {j}', end='')\n",
    "    # Break the loop with a key press\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release resources\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "out.release()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "motion segmentation using connectivity after erode and dilate"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture('pre_dense_erode_dilate.mp4')\n",
    "frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "frame_rate = cap.get(cv2.CAP_PROP_FPS)\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "out = cv2.VideoWriter('pre_dense_erode_dilate_segmented.mp4', fourcc, frame_rate, (frame_width, frame_height), True)\n",
    "# Read the first frame\n",
    "ret, frame1 = cap.read()\n",
    "if not ret:\n",
    "    print(\"Failed to read video\")\n",
    "    cap.release()\n",
    "prvs = cv2.cvtColor(frame1, cv2.COLOR_BGR2GRAY)\n",
    "j = 0\n",
    "while True:\n",
    "    j += 1\n",
    "    ret, frame2 = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    next = cv2.cvtColor(frame2, cv2.COLOR_BGR2GRAY)\n",
    "    # Calculate Optical Flow\n",
    "    flow = cv2.calcOpticalFlowFarneback(prvs, next, None, 0.5, 3, 15, 3, 5, 1.2, 0)\n",
    "    # Compute magnitude and angle of the flow\n",
    "    mag, ang = cv2.cartToPolar(flow[..., 0], flow[..., 1])\n",
    "    # Create motion mask\n",
    "    thresh = 1  # Set threshold for motion detection\n",
    "    motion_mask = cv2.threshold(mag, thresh, 255, cv2.THRESH_BINARY)[1]\n",
    "    cv2.imshow('Segmented Frame', motion_mask)\n",
    "    # Segment mask based on connected components\n",
    "    num_labels, labels, stats, centroids = cv2.connectedComponentsWithStats(np.uint8(motion_mask), connectivity=8)\n",
    "    min_area = 300  # Minimum area for connected components\n",
    "    # Draw bounding boxes around components\n",
    "    for i in range(1, num_labels):\n",
    "        x, y, w, h, area = stats[i]\n",
    "        if area > min_area:\n",
    "            cv2.rectangle(frame2, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "    # cv2.imshow('Segmented Frame', frame2)\n",
    "    # Display the result\n",
    "    out.write(frame2)\n",
    "    # Update previous frame\n",
    "    print(f'\\rProgress: {j}', end='')\n",
    "    prvs = next\n",
    "    # Break the loop if 'q' is pressed\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "cap.release()\n",
    "out.release()\n",
    "cv2.destroyAllWindows()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "erode and dilate also motion mask, save motion mask actually"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture('pre_dense_erode_dilate.mp4')\n",
    "frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "frame_rate = cap.get(cv2.CAP_PROP_FPS)\n",
    "erode_kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (2, 2))\n",
    "dilate_kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (15, 15))\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "out = cv2.VideoWriter('motion_mask_segmented.mp4', fourcc, frame_rate, (frame_width, frame_height), True)\n",
    "# Read the first frame\n",
    "ret, frame1 = cap.read()\n",
    "if not ret:\n",
    "    print(\"Failed to read video\")\n",
    "    cap.release()\n",
    "prvs = cv2.cvtColor(frame1, cv2.COLOR_BGR2GRAY)\n",
    "j = 0\n",
    "while True:\n",
    "    j += 1\n",
    "    ret, frame2 = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    next = cv2.cvtColor(frame2, cv2.COLOR_BGR2GRAY)\n",
    "    # Calculate Optical Flow\n",
    "    flow = cv2.calcOpticalFlowFarneback(prvs, next, None, 0.5, 3, 15, 3, 5, 1.2, 0)\n",
    "    # Compute magnitude and angle of the flow\n",
    "    mag, ang = cv2.cartToPolar(flow[..., 0], flow[..., 1])\n",
    "    # Create motion mask\n",
    "    thresh = 1  # Set threshold for motion detection\n",
    "    motion_mask = cv2.threshold(mag, thresh, 255, cv2.THRESH_BINARY)[1]\n",
    "    eroded_frame = cv2.erode(motion_mask, erode_kernel, iterations=1)\n",
    "    dilated_frame = cv2.dilate(eroded_frame, dilate_kernel, iterations=1)\n",
    "    # cv2.imshow('Segmented Frame', dilated_frame)\n",
    "    # Segment mask based on connected components\n",
    "    num_labels, labels, stats, centroids = cv2.connectedComponentsWithStats(np.uint8(dilated_frame), connectivity=8)\n",
    "    min_area = 300  # Minimum area for connected components\n",
    "    # Draw bounding boxes around components\n",
    "    for i in range(1, num_labels):\n",
    "        x, y, w, h, area = stats[i]\n",
    "        if area > min_area:\n",
    "            cv2.rectangle(frame2, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "    for i in range(1, num_labels):\n",
    "        x, y, w, h, area = stats[i]\n",
    "        if area > min_area:\n",
    "            cv2.rectangle(dilated_frame, (x, y), (x + w, y + h), (255, 255, 255), 2)\n",
    "    save_frame = cv2.normalize(dilated_frame, None, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX, dtype=cv2.CV_8U)\n",
    "    # cv2.imshow('Segmented Frame', dilated_frame)\n",
    "    # cv2.imshow('Segmented Frame', frame2)\n",
    "    # Display the result\n",
    "    rgb_frame = cv2.cvtColor(save_frame, cv2.COLOR_GRAY2RGB)\n",
    "    # inspect_given_a_frame(frame2)\n",
    "    # inspect_given_a_frame(rgb_frame)\n",
    "    out.write(save_frame)\n",
    "    # Update previous frame\n",
    "    print(f'\\rProgress: {j}', end='')\n",
    "    prvs = next\n",
    "    # Break the loop if 'q' is pressed\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "cap.release()\n",
    "out.release()\n",
    "cv2.destroyAllWindows()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture('pre_dense_erode_dilate.mp4')\n",
    "frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "frame_rate = cap.get(cv2.CAP_PROP_FPS)\n",
    "erode_kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (2, 2))\n",
    "dilate_kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (15, 15))\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "out = cv2.VideoWriter('motion_mask_segmented_v2.mp4', fourcc, frame_rate, (frame_width, frame_height), True)\n",
    "# Read the first frame\n",
    "ret, frame1 = cap.read()\n",
    "if not ret:\n",
    "    print(\"Failed to read video\")\n",
    "    cap.release()\n",
    "prvs = cv2.cvtColor(frame1, cv2.COLOR_BGR2GRAY)\n",
    "j = 0\n",
    "while True:\n",
    "    j += 1\n",
    "    ret, frame2 = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    next = cv2.cvtColor(frame2, cv2.COLOR_BGR2GRAY)\n",
    "    # Calculate Optical Flow\n",
    "    flow = cv2.calcOpticalFlowFarneback(prvs, next, None, 0.5, 3, 15, 3, 5, 1.2, 0)\n",
    "    # Compute magnitude and angle of the flow\n",
    "    mag, ang = cv2.cartToPolar(flow[..., 0], flow[..., 1])\n",
    "    # Create motion mask\n",
    "    thresh = 1  # Set threshold for motion detection\n",
    "    motion_mask = cv2.threshold(mag, thresh, 255, cv2.THRESH_BINARY)[1]\n",
    "    eroded_frame = cv2.erode(motion_mask, erode_kernel, iterations=1)\n",
    "    dilated_frame = cv2.dilate(eroded_frame, dilate_kernel, iterations=1)\n",
    "    # cv2.imshow('Segmented Frame', dilated_frame)\n",
    "    # Segment mask based on connected components\n",
    "    num_labels, labels, stats, centroids = cv2.connectedComponentsWithStats(np.uint8(dilated_frame), connectivity=8)\n",
    "    min_area = 300  # Minimum area for connected components\n",
    "    # Draw bounding boxes around components\n",
    "    for i in range(1, num_labels):\n",
    "        x, y, w, h, area = stats[i]\n",
    "        if area > min_area:\n",
    "            cv2.rectangle(frame2, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "    for i in range(1, num_labels):\n",
    "        x, y, w, h, area = stats[i]\n",
    "        if area > min_area:\n",
    "            cv2.rectangle(dilated_frame, (x, y), (x + w, y + h), (255, 255, 255), 2)\n",
    "    t_frame = cv2.normalize(dilated_frame, None, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX, dtype=cv2.CV_8U)\n",
    "    # cv2.imshow('Segmented Frame', dilated_frame)\n",
    "    # cv2.imshow('Segmented Frame', frame2)\n",
    "    # Display the result\n",
    "    rgb_frame = cv2.cvtColor(t_frame, cv2.COLOR_GRAY2RGB)\n",
    "    # inspect_given_a_frame(frame2)\n",
    "    # inspect_given_a_frame(rgb_frame)\n",
    "    # inspect_given_a_frame(rgb_frame)\n",
    "    out.write(rgb_frame)\n",
    "    # Update previous frame\n",
    "    print(f'\\rProgress: {j}', end='')\n",
    "    prvs = next\n",
    "    # Break the loop if 'q' is pressed\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "cap.release()\n",
    "out.release()\n",
    "cv2.destroyAllWindows()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "dilate even more for segmentation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Overlay"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "original_cap = cv2.VideoCapture('sample.avi')\n",
    "cap = cv2.VideoCapture('pre_dense_erode_dilate.mp4')\n",
    "frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "frame_rate = cap.get(cv2.CAP_PROP_FPS)\n",
    "erode_kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (2, 2))\n",
    "dilate_kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (45, 45))\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "out = cv2.VideoWriter('overlay.mp4', fourcc, frame_rate, (frame_width, frame_height), True)\n",
    "# Read the first frame\n",
    "ret, frame1 = cap.read()\n",
    "not_useful, ori_frame = original_cap.read()\n",
    "if not ret:\n",
    "    print(\"Failed to read video\")\n",
    "    cap.release()\n",
    "prvs = cv2.cvtColor(frame1, cv2.COLOR_BGR2GRAY)\n",
    "j = 0\n",
    "while True:\n",
    "    j += 1\n",
    "    ret, frame2 = cap.read()\n",
    "    not_useful, ori_frame = original_cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    next = cv2.cvtColor(frame2, cv2.COLOR_BGR2GRAY)\n",
    "    # Calculate Optical Flow\n",
    "    flow = cv2.calcOpticalFlowFarneback(prvs, next, None, 0.5, 3, 15, 3, 5, 1.2, 0)\n",
    "    # Compute magnitude and angle of the flow\n",
    "    mag, ang = cv2.cartToPolar(flow[..., 0], flow[..., 1])\n",
    "    # Create motion mask\n",
    "    thresh = 1  # Set threshold for motion detection\n",
    "    motion_mask = cv2.threshold(mag, thresh, 255, cv2.THRESH_BINARY)[1]\n",
    "    eroded_frame = cv2.erode(motion_mask, erode_kernel, iterations=1)\n",
    "    dilated_frame = cv2.dilate(eroded_frame, dilate_kernel, iterations=1)\n",
    "    num_labels, labels, stats, centroids = cv2.connectedComponentsWithStats(np.uint8(dilated_frame), connectivity=8)\n",
    "    min_area = 300\n",
    "    for i in range(1, num_labels):\n",
    "        x, y, w, h, area = stats[i]\n",
    "        if area > min_area:\n",
    "            cv2.rectangle(frame2, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "    for i in range(1, num_labels):\n",
    "        x, y, w, h, area = stats[i]\n",
    "        if area > min_area:\n",
    "            cv2.rectangle(dilated_frame, (x, y), (x + w, y + h), (255, 255, 255), 2)\n",
    "    for i in range(1, num_labels):\n",
    "        x, y, w, h, area = stats[i]\n",
    "        if area > min_area:\n",
    "            cv2.rectangle(ori_frame, (x, y), (x + w, y + h), (250,128,114), 2)\n",
    "    temp_frame = cv2.normalize(dilated_frame, None, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX, dtype=cv2.CV_8U)\n",
    "    rgb_frame = cv2.cvtColor(temp_frame, cv2.COLOR_GRAY2RGB)\n",
    "    out.write(ori_frame)\n",
    "    print(f'\\rProgress: {j}', end='')\n",
    "    prvs = next\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "cap.release()\n",
    "out.release()\n",
    "cv2.destroyAllWindows()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Segmentation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Initialize Paths"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "current_location = os.getcwd();\n",
    "output_folder = current_location + '/worm_segmentation'\n",
    "video_path = current_location + '/sample.avi'"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Initialize input videos and output videos"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "motion_mask_cap = cv2.VideoCapture('pre_dense_erode_dilate.mp4')\n",
    "original_video_cap = cv2.VideoCapture('sample.avi')\n",
    "frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "frame_rate = cap.get(cv2.CAP_PROP_FPS)\n",
    "erode_kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (2, 2))\n",
    "dilate_kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (45, 45))\n",
    "f_worm_1 = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "f_worm_2 = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "f_worm_3 = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "f_worm_4 = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "f_worm_5 = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "f_worm_6 = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "f_worm_7 = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "f_worm_8 = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "f_worm_9 = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "f_worm_10 = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "worm_1_out = cv2.VideoWriter(output_folder + '/worm_1_out.mp4', f_worm_1, frame_rate, (frame_width, frame_height), True)\n",
    "worm_2_out = cv2.VideoWriter(output_folder + '/worm_2_out.mp4', f_worm_2, frame_rate, (frame_width, frame_height), True)\n",
    "worm_3_out = cv2.VideoWriter(output_folder + '/worm_3_out.mp4', f_worm_3, frame_rate, (frame_width, frame_height), True)\n",
    "worm_4_out = cv2.VideoWriter(output_folder + '/worm_4_out.mp4', f_worm_4, frame_rate, (frame_width, frame_height), True)\n",
    "worm_5_out = cv2.VideoWriter(output_folder + '/worm_5_out.mp4', f_worm_5, frame_rate, (frame_width, frame_height), True)\n",
    "worm_6_out = cv2.VideoWriter(output_folder + '/worm_6_out.mp4', f_worm_6, frame_rate, (frame_width, frame_height), True)\n",
    "worm_7_out = cv2.VideoWriter(output_folder + '/worm_7_out.mp4', f_worm_7, frame_rate, (frame_width, frame_height), True)\n",
    "worm_8_out = cv2.VideoWriter(output_folder + '/worm_8_out.mp4', f_worm_8, frame_rate, (frame_width, frame_height), True)\n",
    "worm_9_out = cv2.VideoWriter(output_folder + '/worm_9_out.mp4', f_worm_9, frame_rate, (frame_width, frame_height), True)\n",
    "worm_10_out = cv2.VideoWriter(output_folder + '/worm_10_out.mp4', f_worm_10, frame_rate, (frame_width, frame_height), True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "clean up make sure all number is good"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "total_it_number = 0\n",
    "\n",
    "if not motion_mask_cap.isOpened():\n",
    "    print(\"Error: Could not open motion mask.\")\n",
    "else:\n",
    "    # Get the total number of frames in the video\n",
    "    total_it_number = int(motion_mask_cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    print(f\"Total number of frames in the motion mask: {total_it_number}\")\n",
    "\n",
    "if not original_video_cap.isOpened():\n",
    "    print(\"Error: Could not open motion mask.\")\n",
    "else:\n",
    "    # Get the total number of frames in the video\n",
    "    total_it_number = int(original_video_cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    print(f\"Total number of frames in the original video: {total_it_number}\")\n",
    "\n",
    "print(f\"to process the video need to iterate over: {total_it_number}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Read the first motion\n",
    "ret, motion_frame_init = motion_mask_cap.read()\n",
    "if not ret:\n",
    "    print(\"Failed to read the motion ask\")\n",
    "    cap.release()\n",
    "else:\n",
    "    print(\"motion frame read\")\n",
    "print(\"Shape of Frame\", motion_frame_init.shape)\n",
    "print(\"First Row\", motion_frame_init[:,1])\n",
    "previous_motion_frame = cv2.cvtColor(motion_frame_init, cv2.COLOR_BGR2GRAY)\n",
    "inspect_given_a_frame(previous_motion_frame)\n",
    "print(\"Shape of Frame\", previous_motion_frame.shape)\n",
    "print(\"First Row\", previous_motion_frame[:,1].shape)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "pilot the code a bit"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for j in range(total_it_number):\n",
    "    ret, temporal_motion_frame = motion_mask_cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    current_motion_frame = cv2.cvtColor(temporal_motion_frame, cv2.COLOR_BGR2GRAY)\n",
    "    # Calculate Optical Flow\n",
    "    flow = cv2.calcOpticalFlowFarneback(previous_motion_frame, current_motion_frame, None, 0.5, 3, 15, 3, 5, 1.2, 0)\n",
    "    # Compute magnitude and angle of the flow\n",
    "    mag, ang = cv2.cartToPolar(flow[..., 0], flow[..., 1])\n",
    "    # Create motion mask\n",
    "    thresh = 1  # Set threshold for motion detection\n",
    "    motion_mask = cv2.threshold(mag, thresh, 255, cv2.THRESH_BINARY)[1]\n",
    "    eroded_frame = cv2.erode(motion_mask, erode_kernel, iterations=1)\n",
    "    dilated_frame = cv2.dilate(eroded_frame, dilate_kernel, iterations=1)\n",
    "    num_labels, labels, stats, centroids = cv2.connectedComponentsWithStats(np.uint8(dilated_frame), connectivity=8)\n",
    "    print('Number of labels', num_labels, '\\n')\n",
    "    print('labels', labels, '\\n')\n",
    "    print('stats', stats, '\\n')\n",
    "    print('centroids', centroids, '\\n')\n",
    "    min_area = 5000\n",
    "    for i in range(1, num_labels):\n",
    "        x, y, w, h, area = stats[i]\n",
    "        if area > min_area:\n",
    "            cv2.rectangle(frame2, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "    for i in range(1, num_labels):\n",
    "        x, y, w, h, area = stats[i]\n",
    "        if area > min_area:\n",
    "            cv2.rectangle(dilated_frame, (x, y), (x + w, y + h), (255, 255, 255), 2)\n",
    "    temp_frame = cv2.normalize(dilated_frame, None, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX, dtype=cv2.CV_8U)\n",
    "    rgb_frame = cv2.cvtColor(temp_frame, cv2.COLOR_GRAY2RGB)\n",
    "    cv2.imshow('Segmented Frame', rgb_frame)\n",
    "    out.write(rgb_frame)\n",
    "    print(f'\\rProgress: {j}', end='')\n",
    "    prvs = next\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tye = num_labels, labels, stats, centroids"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "num_la, la, st, cent = tye"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(type(num_la))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(centroids[2][0])\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(st)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(st[1][0])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(centroids.shape)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "stats_2 = stats\n",
    "print(stats_2)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "hum = stats"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "hum = np.append(hum, stats_2)\n",
    "print(hum)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "make an object array and work with that"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "hum = np.array(None,dtype=object)\n",
    "print(hum)\n",
    "hum = np.append(hum, stats_2)\n",
    "print(hum)\n",
    "hum = np.append(hum, stats_2)\n",
    "print(hum)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(tye[0])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(tye[1])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(tye[2])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(tye[3])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Construction of a Worm Object\n",
    "## the idea is that, to make a worm object, this object should be able to take each frame as input,\n",
    "## find the motion mask to correspond to the worm, and append to the worm object"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class Worm:\n",
    "    def __init__(self, worm_index, original_cap, label_history, stats_history, centroid_history, another_variable_in_case=None):\n",
    "        self.worm_index = worm_index\n",
    "        self.original_cap = original_cap\n",
    "        self.label_history = label_history\n",
    "        self.stats_history = stats_history\n",
    "        self.centroid_history = centroid_history\n",
    "        self.another_variable_in_case = another_variable_in_case\n",
    "\n",
    "    def display_info(self):\n",
    "        info = f\"worn index: {self.worm_index}, video_cap: {self.original_cap}, label_history: {self.label_history}, stats_history: {self.stats_history}, centroid_history: {self.centroid_history}\"\n",
    "        print(info)\n",
    "\n",
    "    def update_another_variable(self, another_variable_in_case):\n",
    "        \"\"\"Update the car's mileage.\"\"\"\n",
    "        if another_variable_in_case >= self.another_variable_in_case:\n",
    "            self.another_variable_in_case = another_variable_in_case\n",
    "        else:\n",
    "            print(\"Error: another_variable_in_case cannot be reduced.\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Pilot with Single Worm first before doing 10"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Basic set up"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "current_location = os.getcwd();\n",
    "output_folder = current_location + '/worm_segmentation'\n",
    "video_path = current_location + '/sample.avi'\n",
    "if not os.path.exists(output_folder):\n",
    "    os.makedirs(output_folder)\n",
    "motion_mask_cap = cv2.VideoCapture('pre_dense_erode_dilate.mp4')\n",
    "original_video_cap = cv2.VideoCapture('sample.avi')\n",
    "frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "frame_rate = cap.get(cv2.CAP_PROP_FPS)\n",
    "erode_kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (2, 2))\n",
    "dilate_kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (45, 45))\n",
    "f_worm_1 = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "worm_1_out = cv2.VideoWriter(output_folder + '/worm_1_out.mp4', f_worm_1, frame_rate, (frame_width, frame_height), True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "loop through the first 1/10 of the video"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "total_it_number = 0\n",
    "\n",
    "if not motion_mask_cap.isOpened():\n",
    "    print(\"Error: Could not open motion mask.\")\n",
    "else:\n",
    "    # Get the total number of frames in the video\n",
    "    total_it_number = int(motion_mask_cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    print(f\"Total number of frames in the motion mask: {total_it_number}\")\n",
    "\n",
    "if not original_video_cap.isOpened():\n",
    "    print(\"Error: Could not open motion mask.\")\n",
    "else:\n",
    "    # Get the total number of frames in the video\n",
    "    total_it_number = int(original_video_cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    print(f\"Total number of frames in the original video: {total_it_number}\")\n",
    "\n",
    "print(f\"to process the video need to iterate over: {total_it_number}\")\n",
    "\n",
    "total_it_number = int(total_it_number / 10)\n",
    "\n",
    "print(f\"to process the video need to iterate over: {total_it_number}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "actually, construct the worm object before looping, that means, I need to make the first frame"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Read the first motion\n",
    "ret, motion_frame_init = motion_mask_cap.read()\n",
    "if not ret:\n",
    "    print(\"Failed to read the motion ask\")\n",
    "    cap.release()\n",
    "else:\n",
    "    print(\"motion frame read\")\n",
    "print(\"Shape of Frame\", motion_frame_init.shape)\n",
    "print(\"First Row\", motion_frame_init[:,1])\n",
    "previous_motion_frame = cv2.cvtColor(motion_frame_init, cv2.COLOR_BGR2GRAY)\n",
    "inspect_given_a_frame(previous_motion_frame)\n",
    "print(\"Shape of Frame\", previous_motion_frame.shape)\n",
    "print(\"First Row\", previous_motion_frame[:,1].shape)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "for worm initialization, let us analyze frame by frame"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "ret, current_motion_frame = motion_mask_cap.read()\n",
    "current_motion_frame = cv2.cvtColor(current_motion_frame, cv2.COLOR_BGR2GRAY)\n",
    "# Calculate Optical Flow\n",
    "flow = cv2.calcOpticalFlowFarneback(previous_motion_frame, current_motion_frame, None, 0.5, 3, 15, 3, 5, 1.2, 0)\n",
    "# Compute magnitude and angle of the flow\n",
    "mag, ang = cv2.cartToPolar(flow[..., 0], flow[..., 1])\n",
    "# Create motion mask\n",
    "thresh = 1  # Set threshold for motion detection\n",
    "motion_mask = cv2.threshold(mag, thresh, 255, cv2.THRESH_BINARY)[1]\n",
    "eroded_frame = cv2.erode(motion_mask, erode_kernel, iterations=1)\n",
    "dilated_frame = cv2.dilate(eroded_frame, dilate_kernel, iterations=1)\n",
    "num_labels, labels, stats, centroids = cv2.connectedComponentsWithStats(np.uint8(dilated_frame), connectivity=8)\n",
    "print('Number of labels', num_labels, '\\n')\n",
    "print('labels', labels, '\\n')\n",
    "print('stats', stats, '\\n')\n",
    "print('centroids', centroids, '\\n')\n",
    "min_area = 5000\n",
    "for i in range(1, num_labels):\n",
    "    x, y, w, h, area = stats[i]\n",
    "    if area > min_area:\n",
    "        cv2.rectangle(frame2, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "for i in range(1, num_labels):\n",
    "    x, y, w, h, area = stats[i]\n",
    "    if area > min_area:\n",
    "        cv2.rectangle(dilated_frame, (x, y), (x + w, y + h), (255, 255, 255), 2)\n",
    "previous_motion_frame = current_motion_frame"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(sum(sum(labels)))\n",
    "print('Number of labels', num_labels)\n",
    "\n",
    "# Create the heatmap\n",
    "plt.imshow(labels, cmap='hot', interpolation='nearest')\n",
    "\n",
    "# Adding a colorbar\n",
    "plt.colorbar()\n",
    "\n",
    "# Adding titles and labels (optional)\n",
    "plt.title('Heatmap of a 2D NumPy Array')\n",
    "plt.xlabel('X-axis Label')\n",
    "plt.ylabel('Y-axis Label')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "let us look at next frame"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "ret, current_motion_frame = motion_mask_cap.read()\n",
    "current_motion_frame = cv2.cvtColor(current_motion_frame, cv2.COLOR_BGR2GRAY)\n",
    "# Calculate Optical Flow\n",
    "flow = cv2.calcOpticalFlowFarneback(previous_motion_frame, current_motion_frame, None, 0.5, 3, 15, 3, 5, 1.2, 0)\n",
    "# Compute magnitude and angle of the flow\n",
    "mag, ang = cv2.cartToPolar(flow[..., 0], flow[..., 1])\n",
    "# Create motion mask\n",
    "thresh = 1  # Set threshold for motion detection\n",
    "motion_mask = cv2.threshold(mag, thresh, 255, cv2.THRESH_BINARY)[1]\n",
    "eroded_frame = cv2.erode(motion_mask, erode_kernel, iterations=1)\n",
    "dilated_frame = cv2.dilate(eroded_frame, dilate_kernel, iterations=1)\n",
    "num_labels, labels, stats, centroids = cv2.connectedComponentsWithStats(np.uint8(dilated_frame), connectivity=8)\n",
    "print('Number of labels', num_labels, '\\n')\n",
    "print('labels', labels, '\\n')\n",
    "print('stats', stats, '\\n')\n",
    "print('centroids', centroids, '\\n')\n",
    "min_area = 5000\n",
    "for i in range(1, num_labels):\n",
    "    x, y, w, h, area = stats[i]\n",
    "    if area > min_area:\n",
    "        cv2.rectangle(frame2, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "for i in range(1, num_labels):\n",
    "    x, y, w, h, area = stats[i]\n",
    "    if area > min_area:\n",
    "        cv2.rectangle(dilated_frame, (x, y), (x + w, y + h), (255, 255, 255), 2)\n",
    "previous_motion_frame = current_motion_frame"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(sum(sum(labels)))\n",
    "print('Number of labels', num_labels)\n",
    "\n",
    "# Create the heatmap\n",
    "plt.imshow(labels, cmap='hot', interpolation='nearest')\n",
    "\n",
    "# Adding a colorbar\n",
    "plt.colorbar()\n",
    "\n",
    "# Adding titles and labels (optional)\n",
    "plt.title('Heatmap of a 2D NumPy Array')\n",
    "plt.xlabel('X-axis Label')\n",
    "plt.ylabel('Y-axis Label')\n",
    "\n",
    "# Show\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "standard initialization"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# reset cap\n",
    "cap.set(cv2.CAP_PROP_POS_FRAMES, 0)\n",
    "# Read the first motion\n",
    "ret, motion_frame_init = motion_mask_cap.read()\n",
    "if not ret:\n",
    "    print(\"Failed to read the motion ask\")\n",
    "    cap.release()\n",
    "else:\n",
    "    print(\"motion frame read\")\n",
    "print(\"Shape of Frame\", motion_frame_init.shape)\n",
    "print(\"First Row\", motion_frame_init[:,1])\n",
    "previous_motion_frame = cv2.cvtColor(motion_frame_init, cv2.COLOR_BGR2GRAY)\n",
    "inspect_given_a_frame(previous_motion_frame)\n",
    "print(\"Shape of Frame\", previous_motion_frame.shape)\n",
    "print(\"First Row\", previous_motion_frame[:,1].shape)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "loop through the video"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for j in range(total_it_number):\n",
    "    ret, temporal_motion_frame = motion_mask_cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    current_motion_frame = cv2.cvtColor(temporal_motion_frame, cv2.COLOR_BGR2GRAY)\n",
    "    # Calculate Optical Flow\n",
    "    flow = cv2.calcOpticalFlowFarneback(previous_motion_frame, current_motion_frame, None, 0.5, 3, 15, 3, 5, 1.2, 0)\n",
    "    # Compute magnitude and angle of the flow\n",
    "    mag, ang = cv2.cartToPolar(flow[..., 0], flow[..., 1])\n",
    "    # Create motion mask\n",
    "    thresh = 1  # Set threshold for motion detection\n",
    "    motion_mask = cv2.threshold(mag, thresh, 255, cv2.THRESH_BINARY)[1]\n",
    "    eroded_frame = cv2.erode(motion_mask, erode_kernel, iterations=1)\n",
    "    dilated_frame = cv2.dilate(eroded_frame, dilate_kernel, iterations=1)\n",
    "    num_labels, labels, stats, centroids = cv2.connectedComponentsWithStats(np.uint8(dilated_frame), connectivity=8)\n",
    "    print('Number of labels', num_labels, '\\n')\n",
    "    print('labels', labels, '\\n')\n",
    "    print('stats', stats, '\\n')\n",
    "    print('centroids', centroids, '\\n')\n",
    "    min_area = 5000\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    for i in range(1, num_labels):\n",
    "        x, y, w, h, area = stats[i]\n",
    "        if area > min_area:\n",
    "            cv2.rectangle(frame2, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "    for i in range(1, num_labels):\n",
    "        x, y, w, h, area = stats[i]\n",
    "        if area > min_area:\n",
    "            cv2.rectangle(dilated_frame, (x, y), (x + w, y + h), (255, 255, 255), 2)\n",
    "    temp_frame = cv2.normalize(dilated_frame, None, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX, dtype=cv2.CV_8U)\n",
    "    rgb_frame = cv2.cvtColor(temp_frame, cv2.COLOR_GRAY2RGB)\n",
    "    cv2.imshow('Segmented Frame', rgb_frame)\n",
    "    out.write(rgb_frame)\n",
    "    print(f'\\rProgress: {j}', end='')\n",
    "    prvs = next\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Cut to 1/4 of the original for easy processing"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Initialize video capture\n",
    "cap = cv2.VideoCapture('sample.avi')\n",
    "# Get video properties\n",
    "frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "frame_rate = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "# Define codec and create VideoWriter object\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "out = cv2.VideoWriter('shorter.mp4', fourcc, frame_rate, (frame_width, frame_height), True)\n",
    "for i in range(int(frame_count/4)):\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    out.write(frame)\n",
    "    print(f'\\rProgress: {i}', end='')\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "cap.release()\n",
    "out.release()\n",
    "cv2.destroyAllWindows()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Background Subtraction, Spatial Smoothing, Temporal Smoothing"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Initialize video capture\n",
    "cap = cv2.VideoCapture('shorter.mp4')\n",
    "# Get video properties\n",
    "frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "frame_rate = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "# Define codec and create VideoWriter object\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v') # You can also use 'XVID'\n",
    "out = cv2.VideoWriter('processed_v1.mp4', fourcc, frame_rate, (frame_width, frame_height), False)\n",
    "# Background subtractor\n",
    "backSub = cv2.createBackgroundSubtractorMOG2()\n",
    "# Buffer for temporal smoothing\n",
    "buffer_size = 5\n",
    "frame_buffer = []\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    # Background subtraction\n",
    "    fgMask = backSub.apply(frame)\n",
    "    # Spatial smoothing (Gaussian blur)\n",
    "    blurred = cv2.GaussianBlur(fgMask, (5, 5), 0)\n",
    "    # Add frame to buffer for temporal smoothing\n",
    "    frame_buffer.append(blurred)\n",
    "    if len(frame_buffer) > buffer_size:\n",
    "        frame_buffer.pop(0)\n",
    "    # Temporal smoothing (average of frames in buffer)\n",
    "    temp_smoothed = np.mean(frame_buffer, axis=0).astype(np.uint8)\n",
    "    # Write frame to video\n",
    "    out.write(temp_smoothed)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "# Release everything\n",
    "cap.release()\n",
    "out.release()\n",
    "cv2.destroyAllWindows()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Erode and Dilate"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Load the video\n",
    "cap = cv2.VideoCapture('processed_v1.mp4')\n",
    "kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (7, 7))\n",
    "frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "frame_rate = cap.get(cv2.CAP_PROP_FPS)\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "out = cv2.VideoWriter('processed_v2.mp4', fourcc, frame_rate, (frame_width, frame_height), True)\n",
    "j = 0\n",
    "while True:\n",
    "    j += 1\n",
    "    # Read each frame\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    # Apply erosion and then dilation\n",
    "    eroded_frame = cv2.erode(frame, kernel, iterations=1)\n",
    "    dilated_frame = cv2.dilate(eroded_frame, kernel, iterations=1)\n",
    "    # Display the processed frame\n",
    "    out.write(dilated_frame)\n",
    "    print(f'\\rProgress: {j}', end='')\n",
    "    # Break the loop with a key press\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release resources\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "out.release()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Overlay"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "original_cap = cv2.VideoCapture('shorter.mp4')\n",
    "cap = cv2.VideoCapture('processed_v2.mp4')\n",
    "frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "frame_rate = cap.get(cv2.CAP_PROP_FPS)\n",
    "erode_kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (2, 2))\n",
    "dilate_kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (75, 75))\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "out = cv2.VideoWriter('processed_v3.mp4', fourcc, frame_rate, (frame_width, frame_height), True)\n",
    "# Read the first frame\n",
    "ret, frame1 = cap.read()\n",
    "not_useful, ori_frame = original_cap.read()\n",
    "if not ret:\n",
    "    print(\"Failed to read video\")\n",
    "    cap.release()\n",
    "prvs = cv2.cvtColor(frame1, cv2.COLOR_BGR2GRAY)\n",
    "j = 0\n",
    "while True:\n",
    "    j += 1\n",
    "    ret, frame2 = cap.read()\n",
    "    not_useful, ori_frame = original_cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    next = cv2.cvtColor(frame2, cv2.COLOR_BGR2GRAY)\n",
    "    # Calculate Optical Flow\n",
    "    flow = cv2.calcOpticalFlowFarneback(prvs, next, None, 0.5, 3, 15, 3, 5, 1.2, 0)\n",
    "    # Compute magnitude and angle of the flow\n",
    "    mag, ang = cv2.cartToPolar(flow[..., 0], flow[..., 1])\n",
    "    # Create motion mask\n",
    "    thresh = 1  # Set threshold for motion detection\n",
    "    motion_mask = cv2.threshold(mag, thresh, 255, cv2.THRESH_BINARY)[1]\n",
    "    eroded_frame = cv2.erode(motion_mask, erode_kernel, iterations=1)\n",
    "    dilated_frame = cv2.dilate(eroded_frame, dilate_kernel, iterations=1)\n",
    "    num_labels, labels, stats, centroids = cv2.connectedComponentsWithStats(np.uint8(dilated_frame), connectivity=8)\n",
    "    min_area = 300\n",
    "    for i in range(1, num_labels):\n",
    "        x, y, w, h, area = stats[i]\n",
    "        if area > min_area:\n",
    "            cv2.rectangle(frame2, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "    for i in range(1, num_labels):\n",
    "        x, y, w, h, area = stats[i]\n",
    "        if area > min_area:\n",
    "            cv2.rectangle(dilated_frame, (x, y), (x + w, y + h), (255, 255, 255), 2)\n",
    "    for i in range(1, num_labels):\n",
    "        x, y, w, h, area = stats[i]\n",
    "        if area > min_area:\n",
    "            cv2.rectangle(ori_frame, (x, y), (x + w, y + h), (250,128,114), 2)\n",
    "    temp_frame = cv2.normalize(dilated_frame, None, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX, dtype=cv2.CV_8U)\n",
    "    rgb_frame = cv2.cvtColor(temp_frame, cv2.COLOR_GRAY2RGB)\n",
    "    out.write(ori_frame)\n",
    "    print(f'\\rProgress: {j}', end='')\n",
    "    prvs = next\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "cap.release()\n",
    "out.release()\n",
    "cv2.destroyAllWindows()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Worm Object"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Make Minimum Area a Global Parameter"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [],
   "source": [
    "min_area = 2000"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Helper Function"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "def create_list_of_n_objects(n):\n",
    "    \"\"\"\n",
    "    Create a list of n objects, each object is its index in the list.\n",
    "    \"\"\"\n",
    "    return [i for i in range(n)]\n",
    "\n",
    "# Example usage\n",
    "n = 10\n",
    "list_of_n_objects = create_list_of_n_objects(n)\n",
    "\n",
    "def iterate_and_print_list(input_list):\n",
    "    \"\"\"\n",
    "    Iteratively list all objects in the provided list.\n",
    "    \"\"\"\n",
    "    for item in input_list:\n",
    "        print(item)\n",
    "\n",
    "# Example usage with a list of 10 items\n",
    "iterate_and_print_list(list_of_n_objects)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Do the squares overlap? True\n"
     ]
    }
   ],
   "source": [
    "def check_overlap(x1, y1, width1, height1, x2, y2, width2, height2):\n",
    "    # Check if one square is to the left of the other\n",
    "    if x1 + width1 <= x2 or x2 + width2 <= x1:\n",
    "        return False\n",
    "    # Check if one square is above the other\n",
    "    if y1 + height1 <= y2 or y2 + height2 <= y1:\n",
    "        return False\n",
    "    # If neither condition is true, squares must be overlapping\n",
    "    return True\n",
    "\n",
    "# Example usage\n",
    "x1, y1, width1, height1 = 10, 10, 30, 30  # Square 1\n",
    "x2, y2, width2, height2 = 20, 20, 30, 30  # Square 2\n",
    "\n",
    "overlap = check_overlap(x1, y1, width1, height1, x2, y2, width2, height2)\n",
    "print(\"Do the squares overlap?\", overlap)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 2nd item in the queue is: item2\n"
     ]
    }
   ],
   "source": [
    "def get_nth_item(q, n):\n",
    "    if n >= q.qsize():\n",
    "        return None  # or raise an exception if you prefer\n",
    "\n",
    "    for _ in range(n - 1):\n",
    "        q.get()\n",
    "\n",
    "    nth_item = q.get()\n",
    "    return nth_item\n",
    "\n",
    "# Example\n",
    "q = queue.Queue()\n",
    "q.put('item1')\n",
    "q.put('item2')\n",
    "q.put('item3')\n",
    "\n",
    "nth_item = get_nth_item(q, 2)  # Get the 2nd item\n",
    "print(\"The 2nd item in the queue is:\", nth_item)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The distance between the points is: 5.0\n"
     ]
    }
   ],
   "source": [
    "def distance_between_points(x1, y1, x2, y2):\n",
    "    return math.sqrt((x2 - x1) ** 2 + (y2 - y1) ** 2)\n",
    "\n",
    "# Example usage\n",
    "x1, y1 = 1, 2\n",
    "x2, y2 = 4, 6\n",
    "distance = distance_between_points(x1, y1, x2, y2)\n",
    "print(\"The distance between the points is:\", distance)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [],
   "source": [
    "def white_outside_rectangle(fra, x, y, width, height):\n",
    "    \"\"\"\n",
    "    Makes everything outside a specified rectangle in the frame purely white.\n",
    "    :param fra: The original image/frame.\n",
    "    :param x: The x-coordinate of the top-left corner of the rectangle.\n",
    "    :param y: The y-coordinate of the top-left corner of the rectangle.\n",
    "    :param width: The width of the rectangle.\n",
    "    :param height: The height of the rectangle.\n",
    "    :return: Modified frame with white outside the specified rectangle.\n",
    "    \"\"\"\n",
    "    # Make a copy of the frame\n",
    "    modified_frame = np.copy(fra)\n",
    "    # Draw white rectangles over the areas outside the specified rectangle\n",
    "    # Top\n",
    "    modified_frame[0:y, :] = 255\n",
    "    # Bottom\n",
    "    modified_frame[y+height:fra.shape[0], :] = 255\n",
    "    # Left\n",
    "    modified_frame[:, 0:x] = 255\n",
    "    # Right\n",
    "    modified_frame[:, x+width:fra.shape[1]] = 255\n",
    "    return modified_frame\n",
    "# Example usage\n",
    "# frame = cv2.imread('path_to_your_image.jpg')  # Load your frame or image\n",
    "# modified_frame = white_outside_rectangle(frame, 50, 50, 100, 100)\n",
    "# cv2.imshow('Modified Frame', modified_frame)\n",
    "# cv2.waitKey(0)\n",
    "# cv2.destroyAllWindows()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "item 1\n",
      "item 2\n",
      "item 3\n"
     ]
    }
   ],
   "source": [
    "def print_queue(q):\n",
    "    \"\"\"\n",
    "    Print the items of a queue without modifying its state.\n",
    "\n",
    "    :param q: A queue.Queue object\n",
    "    \"\"\"\n",
    "    # Temporary list to store queue items\n",
    "    temp_list = []\n",
    "\n",
    "    # Dequeue all items and add them to the temp_list\n",
    "    while not q.empty():\n",
    "        item = q.get()\n",
    "        print(item)\n",
    "        temp_list.append(item)\n",
    "\n",
    "    # Re-enqueue items back to the queue\n",
    "    for item in temp_list:\n",
    "        q.put(item)\n",
    "\n",
    "# Create a queue and enqueue some items\n",
    "my_queue = queue.Queue()\n",
    "my_queue.put(\"item 1\")\n",
    "my_queue.put(\"item 2\")\n",
    "my_queue.put(\"item 3\")\n",
    "# Print items in the queue\n",
    "print_queue(my_queue)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Main Code"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "outputs": [],
   "source": [
    "current_location = os.getcwd();\n",
    "output_folder = current_location + '/pilot_images'\n",
    "monitor_folder = current_location + '/monitor_images'\n",
    "video_path = current_location + '/sample.avi'\n",
    "if not os.path.exists(output_folder):\n",
    "    os.makedirs(output_folder)\n",
    "if not os.path.exists(monitor_folder):\n",
    "    os.makedirs(monitor_folder)\n",
    "\n",
    "class Worm:\n",
    "    def __init__(self, worm_index, original_Cap, init_label, init_stats,init_centroid,another_variable_in_case=None):\n",
    "        self.worm_index = worm_index\n",
    "        self.original_cap = original_Cap\n",
    "        self.current_label = init_label\n",
    "        self.current_stats = init_stats\n",
    "        self.current_centroid = init_centroid\n",
    "        self.label_history = queue.Queue()\n",
    "        self.stats_history = queue.Queue()\n",
    "        self.centroid_history = queue.Queue()\n",
    "        self.another_variable_in_case = another_variable_in_case\n",
    "        # also establish the output flow\n",
    "        self.frame_width = int(original_Cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "        self.frame_height = int(original_Cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "        self.frame_rate = original_Cap.get(cv2.CAP_PROP_FPS)\n",
    "        cur_location = os.getcwd()\n",
    "        text = '/worm_' + str(worm_index)\n",
    "        self.output_folder = cur_location + text\n",
    "        if not os.path.exists(self.output_folder):\n",
    "            os.makedirs(self.output_folder)\n",
    "\n",
    "    def display_info(self):\n",
    "        info = f\"worn index: {self.worm_index}, video_cap: {self.original_cap}, label_history: {self.label_history}, stats_history: {self.stats_history}, centroid_history: {self.centroid_history}, \\n\"\n",
    "        print(info)\n",
    "        info = f\"current label: {self.current_label}, current_stats: {self.current_stats}, current_centroid: {self.current_centroid}, stats_history: {self.stats_history}, centroid_history: {self.centroid_history}, \\n\"\n",
    "        print(info)\n",
    "        print('Current Stats')\n",
    "        print(self.current_stats)\n",
    "        print('Current Label')\n",
    "        print(self.current_label)\n",
    "        print('Current Centroid')\n",
    "        print(self.current_centroid)\n",
    "\n",
    "    def update_another_variable(self, another_variable_in_case):\n",
    "        \"\"\"Update the car's mileage.\"\"\"\n",
    "        if another_variable_in_case >= self.another_variable_in_case:\n",
    "            self.another_variable_in_case = another_variable_in_case\n",
    "        else:\n",
    "            print(\"Error: another_variable_in_case cannot be reduced.\")\n",
    "\n",
    "    def take_the_bundle_in(self, bundle):\n",
    "        print('\\n \\n \\n bundle in')\n",
    "        num_labels, labels, stats, centroids = bundle\n",
    "        print('bundle', bundle)\n",
    "        self.display_info()\n",
    "        # save the previous iteration\n",
    "        labels_modified = np.where(labels == self.current_label, self.current_label, 0)\n",
    "        self.label_history.put(labels_modified)\n",
    "        self.stats_history.put(self.current_stats)\n",
    "        self.centroid_history.put(self.current_centroid)\n",
    "        overlap_labels = queue.Queue()\n",
    "        # determine which one is the next label\n",
    "        for i in range(1, num_labels):\n",
    "            x, y, w, h, area = stats[i]\n",
    "            print(\"stats,\" , stats[i])\n",
    "            if area > min_area:\n",
    "                overlap_yes = check_overlap(x, y, w, h, self.current_stats[0], self.current_stats[1], self.current_stats[2], self.current_stats[3])\n",
    "                if overlap_yes:\n",
    "                    overlap_labels.put(i)\n",
    "                    print('Overlapped Label', i)\n",
    "        print('Print the Queue')\n",
    "        print_queue(overlap_labels)\n",
    "        # find the closest centroid\n",
    "        self.display_info()\n",
    "        centroid_distance = 10000000000 # first assume that the two centroids are very far away\n",
    "        best_label = -1 # let us not know which one is the best label at the beginning\n",
    "        for j in range(overlap_labels.qsize()):\n",
    "            print('J',j)\n",
    "            this_label_selected = get_nth_item(overlap_labels, j)\n",
    "            print('Selected Label', this_label_selected)\n",
    "            this_centroid = centroids[this_label_selected]\n",
    "            print('Selected Centroid', this_centroid)\n",
    "            this_distance = distance_between_points(this_centroid[0], this_centroid[1], self.current_centroid[0], self.current_centroid[1])\n",
    "            print('Selected Distance', this_distance)\n",
    "            if this_distance < centroid_distance:\n",
    "                centroid_distance = this_distance\n",
    "                print('New Distance', centroid_distance)\n",
    "                best_label = this_label_selected\n",
    "                print('New Label')\n",
    "        # update the current frame\n",
    "        print('Let us see what is the label now', best_label)\n",
    "        self.display_info()\n",
    "        if best_label == -1: # make sure some labels are selected, if  no overlapping frames exist, jump to the nea\n",
    "            print('No Criterion satisfied, jump to the nearest frame')\n",
    "            for l in range(1, num_labels):\n",
    "                this_centroid = centroids[l]\n",
    "                this_distance = distance_between_points(this_centroid[0], this_centroid[1], self.current_centroid[0], self.current_centroid[1])\n",
    "                if this_distance < centroid_distance:\n",
    "                    centroid_distance = this_distance\n",
    "                    print('L label', l)\n",
    "                    best_label = l\n",
    "        print('Let us update')\n",
    "        self.current_label = best_label\n",
    "        self.current_stats = stats[best_label]\n",
    "        self.current_centroid = centroids[best_label]\n",
    "        self.display_info()\n",
    "\n",
    "    def write_frame(self, n_th_frame):\n",
    "        self.original_cap.set(cv2.CAP_PROP_POS_FRAMES, n_th_frame)\n",
    "        get, this_frame = original_cap.read()\n",
    "        if not get:\n",
    "            print('something wrong happened')\n",
    "        x, y, w, h, area = self.current_stats\n",
    "        cv2.rectangle(this_frame, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "        this_frame = white_outside_rectangle(this_frame, x, y, w, h)\n",
    "        cv2.imwrite(os.path.join(self.output_folder, f\"frame_{n_th_frame}.jpg\"), this_frame)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Frame-by-frame processing"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Process the First Frame"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "outputs": [
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original_cap = cv2.VideoCapture('shorter.mp4')\n",
    "\n",
    "cap = cv2.VideoCapture('processed_v2.mp4')\n",
    "\n",
    "frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "frame_rate = cap.get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "erode_kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (2, 2))\n",
    "dilate_kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (75, 75))\n",
    "\n",
    "# Read the first frame\n",
    "ret1, frame1 = cap.read()\n",
    "prvs = cv2.cvtColor(frame1, cv2.COLOR_BGR2GRAY)\n",
    "# Read the second frame\n",
    "ret2, frame2 = cap.read()\n",
    "next = cv2.cvtColor(frame2, cv2.COLOR_BGR2GRAY)\n",
    "# Read the original frame\n",
    "ret3, ori_frame = original_cap.read()\n",
    "\n",
    "# Calculate Optical Flow\n",
    "flow = cv2.calcOpticalFlowFarneback(prvs, next, None, 0.5, 3, 15, 3, 5, 1.2, 0)\n",
    "# Compute magnitude and angle of the flow\n",
    "mag, ang = cv2.cartToPolar(flow[..., 0], flow[..., 1])\n",
    "# Create motion mask\n",
    "thresh = 1  # Set threshold for motion detection\n",
    "motion_mask = cv2.threshold(mag, thresh, 255, cv2.THRESH_BINARY)[1]\n",
    "eroded_frame = cv2.erode(motion_mask, erode_kernel, iterations=1)\n",
    "dilated_frame = cv2.dilate(eroded_frame, dilate_kernel, iterations=1)\n",
    "num_labels, labels, stats, centroids = cv2.connectedComponentsWithStats(np.uint8(dilated_frame), connectivity=8)\n",
    "for i in range(1, num_labels):\n",
    "    x, y, w, h, area = stats[i]\n",
    "    if area > min_area:\n",
    "        cv2.rectangle(dilated_frame, (x, y), (x + w, y + h), (255, 255, 255), 2)\n",
    "for i in range(1, num_labels):\n",
    "    x, y, w, h, area = stats[i]\n",
    "    if area > min_area:\n",
    "        cv2.rectangle(ori_frame, (x, y), (x + w, y + h), (250,128,114), 2)\n",
    "temp_frame = cv2.normalize(dilated_frame, None, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX, dtype=cv2.CV_8U)\n",
    "rgb_frame = cv2.cvtColor(temp_frame, cv2.COLOR_GRAY2RGB)\n",
    "prvs = next\n",
    "cv2.imwrite(os.path.join(monitor_folder, f\"frame_{1}.jpg\"), rgb_frame)\n",
    "cv2.imwrite(os.path.join(output_folder, f\"frame_{1}.jpg\"), ori_frame)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Process the Second Frame"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "outputs": [
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read the second frame\n",
    "ret2, frame2 = cap.read()\n",
    "next = cv2.cvtColor(frame2, cv2.COLOR_BGR2GRAY)\n",
    "# Read the original frame\n",
    "ret3, ori_frame = original_cap.read()\n",
    "\n",
    "# Calculate Optical Flow\n",
    "flow = cv2.calcOpticalFlowFarneback(prvs, next, None, 0.5, 3, 15, 3, 5, 1.2, 0)\n",
    "# Compute magnitude and angle of the flow\n",
    "mag, ang = cv2.cartToPolar(flow[..., 0], flow[..., 1])\n",
    "# Create motion mask\n",
    "thresh = 1  # Set threshold for motion detection\n",
    "motion_mask = cv2.threshold(mag, thresh, 255, cv2.THRESH_BINARY)[1]\n",
    "eroded_frame = cv2.erode(motion_mask, erode_kernel, iterations=1)\n",
    "dilated_frame = cv2.dilate(eroded_frame, dilate_kernel, iterations=1)\n",
    "num_labels, labels, stats, centroids = cv2.connectedComponentsWithStats(np.uint8(dilated_frame), connectivity=8)\n",
    "for i in range(1, num_labels):\n",
    "    x, y, w, h, area = stats[i]\n",
    "    if area > min_area:\n",
    "        cv2.rectangle(dilated_frame, (x, y), (x + w, y + h), (255, 255, 255), 2)\n",
    "for i in range(1, num_labels):\n",
    "    x, y, w, h, area = stats[i]\n",
    "    if area > min_area:\n",
    "        cv2.rectangle(ori_frame, (x, y), (x + w, y + h), (250,128,114), 2)\n",
    "temp_frame = cv2.normalize(dilated_frame, None, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX, dtype=cv2.CV_8U)\n",
    "rgb_frame = cv2.cvtColor(temp_frame, cv2.COLOR_GRAY2RGB)\n",
    "prvs = next\n",
    "cv2.imwrite(os.path.join(monitor_folder, f\"frame_{2}.jpg\"), rgb_frame)\n",
    "cv2.imwrite(os.path.join(output_folder, f\"frame_{2}.jpg\"), ori_frame)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Process the Third Frame"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "outputs": [
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read the second frame\n",
    "ret2, frame2 = cap.read()\n",
    "next = cv2.cvtColor(frame2, cv2.COLOR_BGR2GRAY)\n",
    "# Read the original frame\n",
    "ret3, ori_frame = original_cap.read()\n",
    "\n",
    "# Calculate Optical Flow\n",
    "flow = cv2.calcOpticalFlowFarneback(prvs, next, None, 0.5, 3, 15, 3, 5, 1.2, 0)\n",
    "# Compute magnitude and angle of the flow\n",
    "mag, ang = cv2.cartToPolar(flow[..., 0], flow[..., 1])\n",
    "# Create motion mask\n",
    "thresh = 1  # Set threshold for motion detection\n",
    "motion_mask = cv2.threshold(mag, thresh, 255, cv2.THRESH_BINARY)[1]\n",
    "eroded_frame = cv2.erode(motion_mask, erode_kernel, iterations=1)\n",
    "dilated_frame = cv2.dilate(eroded_frame, dilate_kernel, iterations=1)\n",
    "num_labels, labels, stats, centroids = cv2.connectedComponentsWithStats(np.uint8(dilated_frame), connectivity=8)\n",
    "for i in range(1, num_labels):\n",
    "    x, y, w, h, area = stats[i]\n",
    "    if area > min_area:\n",
    "        cv2.rectangle(dilated_frame, (x, y), (x + w, y + h), (255, 255, 255), 2)\n",
    "for i in range(1, num_labels):\n",
    "    x, y, w, h, area = stats[i]\n",
    "    if area > min_area:\n",
    "        cv2.rectangle(ori_frame, (x, y), (x + w, y + h), (250,128,114), 2)\n",
    "temp_frame = cv2.normalize(dilated_frame, None, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX, dtype=cv2.CV_8U)\n",
    "rgb_frame = cv2.cvtColor(temp_frame, cv2.COLOR_GRAY2RGB)\n",
    "prvs = next\n",
    "cv2.imwrite(os.path.join(monitor_folder, f\"frame_{3}.jpg\"), rgb_frame)\n",
    "cv2.imwrite(os.path.join(output_folder, f\"frame_{3}.jpg\"), ori_frame)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Set Up the First Worm Object: Worm_1"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "worn index: 4, video_cap: < cv2.VideoCapture 0x12cb74570>, label_history: <queue.Queue object at 0x12c3c3b10>, stats_history: <queue.Queue object at 0x12c486410>, centroid_history: <queue.Queue object at 0x12c485650>, \n",
      "\n",
      "current label: 4, current_stats: [1010  120   86   87 7454], current_centroid: [1052.49946338  163.04292997], stats_history: <queue.Queue object at 0x12c486410>, centroid_history: <queue.Queue object at 0x12c485650>, \n",
      "\n",
      "Current Stats\n",
      "[1010  120   86   87 7454]\n",
      "Current Label\n",
      "4\n",
      "Current Centroid\n",
      "[1052.49946338  163.04292997]\n"
     ]
    }
   ],
   "source": [
    "worm_1 = Worm(4, original_cap, 4, stats[4], centroids[4])\n",
    "worm_1.display_info()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Set up the Worm List"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "outputs": [],
   "source": [
    "# set up the number that I want to track for worms\n",
    "track_number = 10\n",
    "\n",
    "# create the list of n tracking number\n",
    "w_list = create_list_of_n_objects(track_number)\n",
    "\n",
    "for i in range(1, len(w_list)):\n",
    "    temp_Worm = Worm(i, original_cap, i, stats[i], centroids[i])\n",
    "    w_list[i] = temp_Worm"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Check everything in the list"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "worn index: 1, video_cap: < cv2.VideoCapture 0x12cb74570>, label_history: <queue.Queue object at 0x12c43a210>, stats_history: <queue.Queue object at 0x1265f7810>, centroid_history: <queue.Queue object at 0x12c48e650>, \n",
      "\n",
      "current label: 1, current_stats: [ 935    0   78   77 6003], current_centroid: [973.48075962  37.98150925], stats_history: <queue.Queue object at 0x1265f7810>, centroid_history: <queue.Queue object at 0x12c48e650>, \n",
      "\n",
      "Current Stats\n",
      "[ 935    0   78   77 6003]\n",
      "Current Label\n",
      "1\n",
      "Current Centroid\n",
      "[973.48075962  37.98150925]\n",
      "worn index: 2, video_cap: < cv2.VideoCapture 0x12cb74570>, label_history: <queue.Queue object at 0x12659ee90>, stats_history: <queue.Queue object at 0x12c46c910>, centroid_history: <queue.Queue object at 0x12c46f410>, \n",
      "\n",
      "current label: 2, current_stats: [1053   12   79   79 6233], current_centroid: [1092.04925397   50.97561367], stats_history: <queue.Queue object at 0x12c46c910>, centroid_history: <queue.Queue object at 0x12c46f410>, \n",
      "\n",
      "Current Stats\n",
      "[1053   12   79   79 6233]\n",
      "Current Label\n",
      "2\n",
      "Current Centroid\n",
      "[1092.04925397   50.97561367]\n",
      "worn index: 3, video_cap: < cv2.VideoCapture 0x12cb74570>, label_history: <queue.Queue object at 0x12b0d4b50>, stats_history: <queue.Queue object at 0x12c3c0710>, centroid_history: <queue.Queue object at 0x12c3c3f90>, \n",
      "\n",
      "current label: 3, current_stats: [1122  102   82  104 8225], current_centroid: [1162.04863222  153.05471125], stats_history: <queue.Queue object at 0x12c3c0710>, centroid_history: <queue.Queue object at 0x12c3c3f90>, \n",
      "\n",
      "Current Stats\n",
      "[1122  102   82  104 8225]\n",
      "Current Label\n",
      "3\n",
      "Current Centroid\n",
      "[1162.04863222  153.05471125]\n",
      "worn index: 4, video_cap: < cv2.VideoCapture 0x12cb74570>, label_history: <queue.Queue object at 0x12c502810>, stats_history: <queue.Queue object at 0x126713c50>, centroid_history: <queue.Queue object at 0x12c5b7950>, \n",
      "\n",
      "current label: 4, current_stats: [1010  120   86   87 7454], current_centroid: [1052.49946338  163.04292997], stats_history: <queue.Queue object at 0x126713c50>, centroid_history: <queue.Queue object at 0x12c5b7950>, \n",
      "\n",
      "Current Stats\n",
      "[1010  120   86   87 7454]\n",
      "Current Label\n",
      "4\n",
      "Current Centroid\n",
      "[1052.49946338  163.04292997]\n",
      "worn index: 5, video_cap: < cv2.VideoCapture 0x12cb74570>, label_history: <queue.Queue object at 0x12c5b7610>, stats_history: <queue.Queue object at 0x12c5b51d0>, centroid_history: <queue.Queue object at 0x12c5b4710>, \n",
      "\n",
      "current label: 5, current_stats: [  359   124   443   377 67112], current_centroid: [574.51002801 280.78412206], stats_history: <queue.Queue object at 0x12c5b51d0>, centroid_history: <queue.Queue object at 0x12c5b4710>, \n",
      "\n",
      "Current Stats\n",
      "[  359   124   443   377 67112]\n",
      "Current Label\n",
      "5\n",
      "Current Centroid\n",
      "[574.51002801 280.78412206]\n",
      "worn index: 6, video_cap: < cv2.VideoCapture 0x12cb74570>, label_history: <queue.Queue object at 0x12c5b4590>, stats_history: <queue.Queue object at 0x12c5b6690>, centroid_history: <queue.Queue object at 0x12c5b4750>, \n",
      "\n",
      "current label: 6, current_stats: [1277  158   76   76 5776], current_centroid: [1314.5  195.5], stats_history: <queue.Queue object at 0x12c5b6690>, centroid_history: <queue.Queue object at 0x12c5b4750>, \n",
      "\n",
      "Current Stats\n",
      "[1277  158   76   76 5776]\n",
      "Current Label\n",
      "6\n",
      "Current Centroid\n",
      "[1314.5  195.5]\n",
      "worn index: 7, video_cap: < cv2.VideoCapture 0x12cb74570>, label_history: <queue.Queue object at 0x12c5b6750>, stats_history: <queue.Queue object at 0x12c5b7690>, centroid_history: <queue.Queue object at 0x12c5b5a90>, \n",
      "\n",
      "current label: 7, current_stats: [ 156  463   78   76 5925], current_centroid: [194.49367089 500.51898734], stats_history: <queue.Queue object at 0x12c5b7690>, centroid_history: <queue.Queue object at 0x12c5b5a90>, \n",
      "\n",
      "Current Stats\n",
      "[ 156  463   78   76 5925]\n",
      "Current Label\n",
      "7\n",
      "Current Centroid\n",
      "[194.49367089 500.51898734]\n",
      "worn index: 8, video_cap: < cv2.VideoCapture 0x12cb74570>, label_history: <queue.Queue object at 0x12c5b6550>, stats_history: <queue.Queue object at 0x12c5b5d50>, centroid_history: <queue.Queue object at 0x12c5b4350>, \n",
      "\n",
      "current label: 8, current_stats: [  172   564   122   141 16269], current_centroid: [231.24426824 633.56358719], stats_history: <queue.Queue object at 0x12c5b5d50>, centroid_history: <queue.Queue object at 0x12c5b4350>, \n",
      "\n",
      "Current Stats\n",
      "[  172   564   122   141 16269]\n",
      "Current Label\n",
      "8\n",
      "Current Centroid\n",
      "[231.24426824 633.56358719]\n",
      "worn index: 9, video_cap: < cv2.VideoCapture 0x12cb74570>, label_history: <queue.Queue object at 0x12c5a4b90>, stats_history: <queue.Queue object at 0x12c427950>, centroid_history: <queue.Queue object at 0x12c425a10>, \n",
      "\n",
      "current label: 9, current_stats: [ 1887   653   156   195 25521], current_centroid: [1964.71074801  741.59680263], stats_history: <queue.Queue object at 0x12c427950>, centroid_history: <queue.Queue object at 0x12c425a10>, \n",
      "\n",
      "Current Stats\n",
      "[ 1887   653   156   195 25521]\n",
      "Current Label\n",
      "9\n",
      "Current Centroid\n",
      "[1964.71074801  741.59680263]\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, len(w_list)):\n",
    "    temp_Worm = Worm(i, original_cap, i, stats[i], centroids[i])\n",
    "    temp_Worm.display_info()\n",
    "    w_list[i] = temp_Worm"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Process the Fourth Frame and take the bundle in by the Worm"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " \n",
      " \n",
      " bundle in\n",
      "bundle (19, array([[0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       ...,\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0]], dtype=int32), array([[      0,       0,    2160,    2160, 3920757],\n",
      "       [   1026,     117,     210,      97,   18763],\n",
      "       [    435,     124,     386,     256,   51413],\n",
      "       ...,\n",
      "       [    236,    1587,     140,     116,   15035],\n",
      "       [    379,    1588,     493,     363,  114735],\n",
      "       [    736,    1959,      75,      78,    5850]], dtype=int32), array([[1076.20338062, 1036.87229379],\n",
      "       [1129.28305708,  165.21739594],\n",
      "       [ 620.73300527,  248.3179546 ],\n",
      "       [ 378.49350649,  322.49350649],\n",
      "       [ 407.39727754,  451.01222586],\n",
      "       [ 234.27232116,  627.71403562],\n",
      "       [1966.45045875,  744.73146762],\n",
      "       [ 785.38512205,  871.66690403],\n",
      "       [ 141.5       ,  792.5       ],\n",
      "       [1094.37245013,  902.39397817],\n",
      "       [ 130.57567881,  956.6339103 ],\n",
      "       [1992.73415299, 1193.00398268],\n",
      "       [ 525.52005468, 1075.33799777],\n",
      "       [1553.25298932, 1172.01448399],\n",
      "       [1064.24870001, 1308.38552617],\n",
      "       [1407.79749064, 1875.67034953],\n",
      "       [ 305.81935484, 1645.51659461],\n",
      "       [ 622.79866649, 1757.73856278],\n",
      "       [ 773.        , 1997.5       ]]))\n",
      "worn index: 4, video_cap: < cv2.VideoCapture 0x12cb74570>, label_history: <queue.Queue object at 0x12c3c3b10>, stats_history: <queue.Queue object at 0x12c486410>, centroid_history: <queue.Queue object at 0x12c485650>, \n",
      "\n",
      "current label: 4, current_stats: [1010  120   86   87 7454], current_centroid: [1052.49946338  163.04292997], stats_history: <queue.Queue object at 0x12c486410>, centroid_history: <queue.Queue object at 0x12c485650>, \n",
      "\n",
      "Current Stats\n",
      "[1010  120   86   87 7454]\n",
      "Current Label\n",
      "4\n",
      "Current Centroid\n",
      "[1052.49946338  163.04292997]\n",
      "stats, [ 1026   117   210    97 18763]\n",
      "Overlapped Label 1\n",
      "stats, [  435   124   386   256 51413]\n",
      "stats, [ 341  285   76   76 5775]\n",
      "stats, [ 363  407   90   89 7934]\n",
      "stats, [  171   557   127   142 16565]\n",
      "stats, [ 1887   657   160   196 26049]\n",
      "stats, [  676   714   229   312 44939]\n",
      "stats, [ 103  755   78   76 5926]\n",
      "stats, [  999   809   186   192 26570]\n",
      "stats, [   72   866   114   175 18083]\n",
      "stats, [ 1884   905   230   588 83612]\n",
      "stats, [  447   952   160   254 27799]\n",
      "stats, [ 1486  1056   138   238 28100]\n",
      "stats, [  900  1165   340   283 53462]\n",
      "stats, [   903   1566    996    527 194233]\n",
      "stats, [  236  1587   140   116 15035]\n",
      "stats, [   379   1588    493    363 114735]\n",
      "stats, [ 736 1959   75   78 5850]\n",
      "Print the Queue\n",
      "1\n",
      "worn index: 4, video_cap: < cv2.VideoCapture 0x12cb74570>, label_history: <queue.Queue object at 0x12c3c3b10>, stats_history: <queue.Queue object at 0x12c486410>, centroid_history: <queue.Queue object at 0x12c485650>, \n",
      "\n",
      "current label: 4, current_stats: [1010  120   86   87 7454], current_centroid: [1052.49946338  163.04292997], stats_history: <queue.Queue object at 0x12c486410>, centroid_history: <queue.Queue object at 0x12c485650>, \n",
      "\n",
      "Current Stats\n",
      "[1010  120   86   87 7454]\n",
      "Current Label\n",
      "4\n",
      "Current Centroid\n",
      "[1052.49946338  163.04292997]\n",
      "J 0\n",
      "Selected Label 1\n",
      "Selected Centroid [1129.28305708  165.21739594]\n",
      "Selected Distance 76.8143773294454\n",
      "New Distance 76.8143773294454\n",
      "New Label\n",
      "Let us see what is the label now 1\n",
      "worn index: 4, video_cap: < cv2.VideoCapture 0x12cb74570>, label_history: <queue.Queue object at 0x12c3c3b10>, stats_history: <queue.Queue object at 0x12c486410>, centroid_history: <queue.Queue object at 0x12c485650>, \n",
      "\n",
      "current label: 4, current_stats: [1010  120   86   87 7454], current_centroid: [1052.49946338  163.04292997], stats_history: <queue.Queue object at 0x12c486410>, centroid_history: <queue.Queue object at 0x12c485650>, \n",
      "\n",
      "Current Stats\n",
      "[1010  120   86   87 7454]\n",
      "Current Label\n",
      "4\n",
      "Current Centroid\n",
      "[1052.49946338  163.04292997]\n",
      "Let us update\n",
      "worn index: 4, video_cap: < cv2.VideoCapture 0x12cb74570>, label_history: <queue.Queue object at 0x12c3c3b10>, stats_history: <queue.Queue object at 0x12c486410>, centroid_history: <queue.Queue object at 0x12c485650>, \n",
      "\n",
      "current label: 1, current_stats: [ 1026   117   210    97 18763], current_centroid: [1129.28305708  165.21739594], stats_history: <queue.Queue object at 0x12c486410>, centroid_history: <queue.Queue object at 0x12c485650>, \n",
      "\n",
      "Current Stats\n",
      "[ 1026   117   210    97 18763]\n",
      "Current Label\n",
      "1\n",
      "Current Centroid\n",
      "[1129.28305708  165.21739594]\n",
      "\n",
      " \n",
      " \n",
      " bundle in\n",
      "bundle (19, array([[0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       ...,\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0]], dtype=int32), array([[      0,       0,    2160,    2160, 3920757],\n",
      "       [   1026,     117,     210,      97,   18763],\n",
      "       [    435,     124,     386,     256,   51413],\n",
      "       ...,\n",
      "       [    236,    1587,     140,     116,   15035],\n",
      "       [    379,    1588,     493,     363,  114735],\n",
      "       [    736,    1959,      75,      78,    5850]], dtype=int32), array([[1076.20338062, 1036.87229379],\n",
      "       [1129.28305708,  165.21739594],\n",
      "       [ 620.73300527,  248.3179546 ],\n",
      "       [ 378.49350649,  322.49350649],\n",
      "       [ 407.39727754,  451.01222586],\n",
      "       [ 234.27232116,  627.71403562],\n",
      "       [1966.45045875,  744.73146762],\n",
      "       [ 785.38512205,  871.66690403],\n",
      "       [ 141.5       ,  792.5       ],\n",
      "       [1094.37245013,  902.39397817],\n",
      "       [ 130.57567881,  956.6339103 ],\n",
      "       [1992.73415299, 1193.00398268],\n",
      "       [ 525.52005468, 1075.33799777],\n",
      "       [1553.25298932, 1172.01448399],\n",
      "       [1064.24870001, 1308.38552617],\n",
      "       [1407.79749064, 1875.67034953],\n",
      "       [ 305.81935484, 1645.51659461],\n",
      "       [ 622.79866649, 1757.73856278],\n",
      "       [ 773.        , 1997.5       ]]))\n",
      "worn index: 1, video_cap: < cv2.VideoCapture 0x12cb74570>, label_history: <queue.Queue object at 0x12c43a210>, stats_history: <queue.Queue object at 0x1265f7810>, centroid_history: <queue.Queue object at 0x12c48e650>, \n",
      "\n",
      "current label: 1, current_stats: [ 935    0   78   77 6003], current_centroid: [973.48075962  37.98150925], stats_history: <queue.Queue object at 0x1265f7810>, centroid_history: <queue.Queue object at 0x12c48e650>, \n",
      "\n",
      "Current Stats\n",
      "[ 935    0   78   77 6003]\n",
      "Current Label\n",
      "1\n",
      "Current Centroid\n",
      "[973.48075962  37.98150925]\n",
      "stats, [ 1026   117   210    97 18763]\n",
      "stats, [  435   124   386   256 51413]\n",
      "stats, [ 341  285   76   76 5775]\n",
      "stats, [ 363  407   90   89 7934]\n",
      "stats, [  171   557   127   142 16565]\n",
      "stats, [ 1887   657   160   196 26049]\n",
      "stats, [  676   714   229   312 44939]\n",
      "stats, [ 103  755   78   76 5926]\n",
      "stats, [  999   809   186   192 26570]\n",
      "stats, [   72   866   114   175 18083]\n",
      "stats, [ 1884   905   230   588 83612]\n",
      "stats, [  447   952   160   254 27799]\n",
      "stats, [ 1486  1056   138   238 28100]\n",
      "stats, [  900  1165   340   283 53462]\n",
      "stats, [   903   1566    996    527 194233]\n",
      "stats, [  236  1587   140   116 15035]\n",
      "stats, [   379   1588    493    363 114735]\n",
      "stats, [ 736 1959   75   78 5850]\n",
      "Print the Queue\n",
      "worn index: 1, video_cap: < cv2.VideoCapture 0x12cb74570>, label_history: <queue.Queue object at 0x12c43a210>, stats_history: <queue.Queue object at 0x1265f7810>, centroid_history: <queue.Queue object at 0x12c48e650>, \n",
      "\n",
      "current label: 1, current_stats: [ 935    0   78   77 6003], current_centroid: [973.48075962  37.98150925], stats_history: <queue.Queue object at 0x1265f7810>, centroid_history: <queue.Queue object at 0x12c48e650>, \n",
      "\n",
      "Current Stats\n",
      "[ 935    0   78   77 6003]\n",
      "Current Label\n",
      "1\n",
      "Current Centroid\n",
      "[973.48075962  37.98150925]\n",
      "Let us see what is the label now -1\n",
      "worn index: 1, video_cap: < cv2.VideoCapture 0x12cb74570>, label_history: <queue.Queue object at 0x12c43a210>, stats_history: <queue.Queue object at 0x1265f7810>, centroid_history: <queue.Queue object at 0x12c48e650>, \n",
      "\n",
      "current label: 1, current_stats: [ 935    0   78   77 6003], current_centroid: [973.48075962  37.98150925], stats_history: <queue.Queue object at 0x1265f7810>, centroid_history: <queue.Queue object at 0x12c48e650>, \n",
      "\n",
      "Current Stats\n",
      "[ 935    0   78   77 6003]\n",
      "Current Label\n",
      "1\n",
      "Current Centroid\n",
      "[973.48075962  37.98150925]\n",
      "No Criterion satisfied, jump to the nearest frame\n",
      "L label 1\n",
      "Let us update\n",
      "worn index: 1, video_cap: < cv2.VideoCapture 0x12cb74570>, label_history: <queue.Queue object at 0x12c43a210>, stats_history: <queue.Queue object at 0x1265f7810>, centroid_history: <queue.Queue object at 0x12c48e650>, \n",
      "\n",
      "current label: 1, current_stats: [ 1026   117   210    97 18763], current_centroid: [1129.28305708  165.21739594], stats_history: <queue.Queue object at 0x1265f7810>, centroid_history: <queue.Queue object at 0x12c48e650>, \n",
      "\n",
      "Current Stats\n",
      "[ 1026   117   210    97 18763]\n",
      "Current Label\n",
      "1\n",
      "Current Centroid\n",
      "[1129.28305708  165.21739594]\n",
      "\n",
      " \n",
      " \n",
      " bundle in\n",
      "bundle (19, array([[0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       ...,\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0]], dtype=int32), array([[      0,       0,    2160,    2160, 3920757],\n",
      "       [   1026,     117,     210,      97,   18763],\n",
      "       [    435,     124,     386,     256,   51413],\n",
      "       ...,\n",
      "       [    236,    1587,     140,     116,   15035],\n",
      "       [    379,    1588,     493,     363,  114735],\n",
      "       [    736,    1959,      75,      78,    5850]], dtype=int32), array([[1076.20338062, 1036.87229379],\n",
      "       [1129.28305708,  165.21739594],\n",
      "       [ 620.73300527,  248.3179546 ],\n",
      "       [ 378.49350649,  322.49350649],\n",
      "       [ 407.39727754,  451.01222586],\n",
      "       [ 234.27232116,  627.71403562],\n",
      "       [1966.45045875,  744.73146762],\n",
      "       [ 785.38512205,  871.66690403],\n",
      "       [ 141.5       ,  792.5       ],\n",
      "       [1094.37245013,  902.39397817],\n",
      "       [ 130.57567881,  956.6339103 ],\n",
      "       [1992.73415299, 1193.00398268],\n",
      "       [ 525.52005468, 1075.33799777],\n",
      "       [1553.25298932, 1172.01448399],\n",
      "       [1064.24870001, 1308.38552617],\n",
      "       [1407.79749064, 1875.67034953],\n",
      "       [ 305.81935484, 1645.51659461],\n",
      "       [ 622.79866649, 1757.73856278],\n",
      "       [ 773.        , 1997.5       ]]))\n",
      "worn index: 2, video_cap: < cv2.VideoCapture 0x12cb74570>, label_history: <queue.Queue object at 0x12659ee90>, stats_history: <queue.Queue object at 0x12c46c910>, centroid_history: <queue.Queue object at 0x12c46f410>, \n",
      "\n",
      "current label: 2, current_stats: [1053   12   79   79 6233], current_centroid: [1092.04925397   50.97561367], stats_history: <queue.Queue object at 0x12c46c910>, centroid_history: <queue.Queue object at 0x12c46f410>, \n",
      "\n",
      "Current Stats\n",
      "[1053   12   79   79 6233]\n",
      "Current Label\n",
      "2\n",
      "Current Centroid\n",
      "[1092.04925397   50.97561367]\n",
      "stats, [ 1026   117   210    97 18763]\n",
      "stats, [  435   124   386   256 51413]\n",
      "stats, [ 341  285   76   76 5775]\n",
      "stats, [ 363  407   90   89 7934]\n",
      "stats, [  171   557   127   142 16565]\n",
      "stats, [ 1887   657   160   196 26049]\n",
      "stats, [  676   714   229   312 44939]\n",
      "stats, [ 103  755   78   76 5926]\n",
      "stats, [  999   809   186   192 26570]\n",
      "stats, [   72   866   114   175 18083]\n",
      "stats, [ 1884   905   230   588 83612]\n",
      "stats, [  447   952   160   254 27799]\n",
      "stats, [ 1486  1056   138   238 28100]\n",
      "stats, [  900  1165   340   283 53462]\n",
      "stats, [   903   1566    996    527 194233]\n",
      "stats, [  236  1587   140   116 15035]\n",
      "stats, [   379   1588    493    363 114735]\n",
      "stats, [ 736 1959   75   78 5850]\n",
      "Print the Queue\n",
      "worn index: 2, video_cap: < cv2.VideoCapture 0x12cb74570>, label_history: <queue.Queue object at 0x12659ee90>, stats_history: <queue.Queue object at 0x12c46c910>, centroid_history: <queue.Queue object at 0x12c46f410>, \n",
      "\n",
      "current label: 2, current_stats: [1053   12   79   79 6233], current_centroid: [1092.04925397   50.97561367], stats_history: <queue.Queue object at 0x12c46c910>, centroid_history: <queue.Queue object at 0x12c46f410>, \n",
      "\n",
      "Current Stats\n",
      "[1053   12   79   79 6233]\n",
      "Current Label\n",
      "2\n",
      "Current Centroid\n",
      "[1092.04925397   50.97561367]\n",
      "Let us see what is the label now -1\n",
      "worn index: 2, video_cap: < cv2.VideoCapture 0x12cb74570>, label_history: <queue.Queue object at 0x12659ee90>, stats_history: <queue.Queue object at 0x12c46c910>, centroid_history: <queue.Queue object at 0x12c46f410>, \n",
      "\n",
      "current label: 2, current_stats: [1053   12   79   79 6233], current_centroid: [1092.04925397   50.97561367], stats_history: <queue.Queue object at 0x12c46c910>, centroid_history: <queue.Queue object at 0x12c46f410>, \n",
      "\n",
      "Current Stats\n",
      "[1053   12   79   79 6233]\n",
      "Current Label\n",
      "2\n",
      "Current Centroid\n",
      "[1092.04925397   50.97561367]\n",
      "No Criterion satisfied, jump to the nearest frame\n",
      "L label 1\n",
      "Let us update\n",
      "worn index: 2, video_cap: < cv2.VideoCapture 0x12cb74570>, label_history: <queue.Queue object at 0x12659ee90>, stats_history: <queue.Queue object at 0x12c46c910>, centroid_history: <queue.Queue object at 0x12c46f410>, \n",
      "\n",
      "current label: 1, current_stats: [ 1026   117   210    97 18763], current_centroid: [1129.28305708  165.21739594], stats_history: <queue.Queue object at 0x12c46c910>, centroid_history: <queue.Queue object at 0x12c46f410>, \n",
      "\n",
      "Current Stats\n",
      "[ 1026   117   210    97 18763]\n",
      "Current Label\n",
      "1\n",
      "Current Centroid\n",
      "[1129.28305708  165.21739594]\n",
      "\n",
      " \n",
      " \n",
      " bundle in\n",
      "bundle (19, array([[0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       ...,\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0]], dtype=int32), array([[      0,       0,    2160,    2160, 3920757],\n",
      "       [   1026,     117,     210,      97,   18763],\n",
      "       [    435,     124,     386,     256,   51413],\n",
      "       ...,\n",
      "       [    236,    1587,     140,     116,   15035],\n",
      "       [    379,    1588,     493,     363,  114735],\n",
      "       [    736,    1959,      75,      78,    5850]], dtype=int32), array([[1076.20338062, 1036.87229379],\n",
      "       [1129.28305708,  165.21739594],\n",
      "       [ 620.73300527,  248.3179546 ],\n",
      "       [ 378.49350649,  322.49350649],\n",
      "       [ 407.39727754,  451.01222586],\n",
      "       [ 234.27232116,  627.71403562],\n",
      "       [1966.45045875,  744.73146762],\n",
      "       [ 785.38512205,  871.66690403],\n",
      "       [ 141.5       ,  792.5       ],\n",
      "       [1094.37245013,  902.39397817],\n",
      "       [ 130.57567881,  956.6339103 ],\n",
      "       [1992.73415299, 1193.00398268],\n",
      "       [ 525.52005468, 1075.33799777],\n",
      "       [1553.25298932, 1172.01448399],\n",
      "       [1064.24870001, 1308.38552617],\n",
      "       [1407.79749064, 1875.67034953],\n",
      "       [ 305.81935484, 1645.51659461],\n",
      "       [ 622.79866649, 1757.73856278],\n",
      "       [ 773.        , 1997.5       ]]))\n",
      "worn index: 3, video_cap: < cv2.VideoCapture 0x12cb74570>, label_history: <queue.Queue object at 0x12b0d4b50>, stats_history: <queue.Queue object at 0x12c3c0710>, centroid_history: <queue.Queue object at 0x12c3c3f90>, \n",
      "\n",
      "current label: 3, current_stats: [1122  102   82  104 8225], current_centroid: [1162.04863222  153.05471125], stats_history: <queue.Queue object at 0x12c3c0710>, centroid_history: <queue.Queue object at 0x12c3c3f90>, \n",
      "\n",
      "Current Stats\n",
      "[1122  102   82  104 8225]\n",
      "Current Label\n",
      "3\n",
      "Current Centroid\n",
      "[1162.04863222  153.05471125]\n",
      "stats, [ 1026   117   210    97 18763]\n",
      "Overlapped Label 1\n",
      "stats, [  435   124   386   256 51413]\n",
      "stats, [ 341  285   76   76 5775]\n",
      "stats, [ 363  407   90   89 7934]\n",
      "stats, [  171   557   127   142 16565]\n",
      "stats, [ 1887   657   160   196 26049]\n",
      "stats, [  676   714   229   312 44939]\n",
      "stats, [ 103  755   78   76 5926]\n",
      "stats, [  999   809   186   192 26570]\n",
      "stats, [   72   866   114   175 18083]\n",
      "stats, [ 1884   905   230   588 83612]\n",
      "stats, [  447   952   160   254 27799]\n",
      "stats, [ 1486  1056   138   238 28100]\n",
      "stats, [  900  1165   340   283 53462]\n",
      "stats, [   903   1566    996    527 194233]\n",
      "stats, [  236  1587   140   116 15035]\n",
      "stats, [   379   1588    493    363 114735]\n",
      "stats, [ 736 1959   75   78 5850]\n",
      "Print the Queue\n",
      "1\n",
      "worn index: 3, video_cap: < cv2.VideoCapture 0x12cb74570>, label_history: <queue.Queue object at 0x12b0d4b50>, stats_history: <queue.Queue object at 0x12c3c0710>, centroid_history: <queue.Queue object at 0x12c3c3f90>, \n",
      "\n",
      "current label: 3, current_stats: [1122  102   82  104 8225], current_centroid: [1162.04863222  153.05471125], stats_history: <queue.Queue object at 0x12c3c0710>, centroid_history: <queue.Queue object at 0x12c3c3f90>, \n",
      "\n",
      "Current Stats\n",
      "[1122  102   82  104 8225]\n",
      "Current Label\n",
      "3\n",
      "Current Centroid\n",
      "[1162.04863222  153.05471125]\n",
      "J 0\n",
      "Selected Label 1\n",
      "Selected Centroid [1129.28305708  165.21739594]\n",
      "Selected Distance 34.95016184631289\n",
      "New Distance 34.95016184631289\n",
      "New Label\n",
      "Let us see what is the label now 1\n",
      "worn index: 3, video_cap: < cv2.VideoCapture 0x12cb74570>, label_history: <queue.Queue object at 0x12b0d4b50>, stats_history: <queue.Queue object at 0x12c3c0710>, centroid_history: <queue.Queue object at 0x12c3c3f90>, \n",
      "\n",
      "current label: 3, current_stats: [1122  102   82  104 8225], current_centroid: [1162.04863222  153.05471125], stats_history: <queue.Queue object at 0x12c3c0710>, centroid_history: <queue.Queue object at 0x12c3c3f90>, \n",
      "\n",
      "Current Stats\n",
      "[1122  102   82  104 8225]\n",
      "Current Label\n",
      "3\n",
      "Current Centroid\n",
      "[1162.04863222  153.05471125]\n",
      "Let us update\n",
      "worn index: 3, video_cap: < cv2.VideoCapture 0x12cb74570>, label_history: <queue.Queue object at 0x12b0d4b50>, stats_history: <queue.Queue object at 0x12c3c0710>, centroid_history: <queue.Queue object at 0x12c3c3f90>, \n",
      "\n",
      "current label: 1, current_stats: [ 1026   117   210    97 18763], current_centroid: [1129.28305708  165.21739594], stats_history: <queue.Queue object at 0x12c3c0710>, centroid_history: <queue.Queue object at 0x12c3c3f90>, \n",
      "\n",
      "Current Stats\n",
      "[ 1026   117   210    97 18763]\n",
      "Current Label\n",
      "1\n",
      "Current Centroid\n",
      "[1129.28305708  165.21739594]\n",
      "\n",
      " \n",
      " \n",
      " bundle in\n",
      "bundle (19, array([[0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       ...,\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0]], dtype=int32), array([[      0,       0,    2160,    2160, 3920757],\n",
      "       [   1026,     117,     210,      97,   18763],\n",
      "       [    435,     124,     386,     256,   51413],\n",
      "       ...,\n",
      "       [    236,    1587,     140,     116,   15035],\n",
      "       [    379,    1588,     493,     363,  114735],\n",
      "       [    736,    1959,      75,      78,    5850]], dtype=int32), array([[1076.20338062, 1036.87229379],\n",
      "       [1129.28305708,  165.21739594],\n",
      "       [ 620.73300527,  248.3179546 ],\n",
      "       [ 378.49350649,  322.49350649],\n",
      "       [ 407.39727754,  451.01222586],\n",
      "       [ 234.27232116,  627.71403562],\n",
      "       [1966.45045875,  744.73146762],\n",
      "       [ 785.38512205,  871.66690403],\n",
      "       [ 141.5       ,  792.5       ],\n",
      "       [1094.37245013,  902.39397817],\n",
      "       [ 130.57567881,  956.6339103 ],\n",
      "       [1992.73415299, 1193.00398268],\n",
      "       [ 525.52005468, 1075.33799777],\n",
      "       [1553.25298932, 1172.01448399],\n",
      "       [1064.24870001, 1308.38552617],\n",
      "       [1407.79749064, 1875.67034953],\n",
      "       [ 305.81935484, 1645.51659461],\n",
      "       [ 622.79866649, 1757.73856278],\n",
      "       [ 773.        , 1997.5       ]]))\n",
      "worn index: 4, video_cap: < cv2.VideoCapture 0x12cb74570>, label_history: <queue.Queue object at 0x12c502810>, stats_history: <queue.Queue object at 0x126713c50>, centroid_history: <queue.Queue object at 0x12c5b7950>, \n",
      "\n",
      "current label: 4, current_stats: [1010  120   86   87 7454], current_centroid: [1052.49946338  163.04292997], stats_history: <queue.Queue object at 0x126713c50>, centroid_history: <queue.Queue object at 0x12c5b7950>, \n",
      "\n",
      "Current Stats\n",
      "[1010  120   86   87 7454]\n",
      "Current Label\n",
      "4\n",
      "Current Centroid\n",
      "[1052.49946338  163.04292997]\n",
      "stats, [ 1026   117   210    97 18763]\n",
      "Overlapped Label 1\n",
      "stats, [  435   124   386   256 51413]\n",
      "stats, [ 341  285   76   76 5775]\n",
      "stats, [ 363  407   90   89 7934]\n",
      "stats, [  171   557   127   142 16565]\n",
      "stats, [ 1887   657   160   196 26049]\n",
      "stats, [  676   714   229   312 44939]\n",
      "stats, [ 103  755   78   76 5926]\n",
      "stats, [  999   809   186   192 26570]\n",
      "stats, [   72   866   114   175 18083]\n",
      "stats, [ 1884   905   230   588 83612]\n",
      "stats, [  447   952   160   254 27799]\n",
      "stats, [ 1486  1056   138   238 28100]\n",
      "stats, [  900  1165   340   283 53462]\n",
      "stats, [   903   1566    996    527 194233]\n",
      "stats, [  236  1587   140   116 15035]\n",
      "stats, [   379   1588    493    363 114735]\n",
      "stats, [ 736 1959   75   78 5850]\n",
      "Print the Queue\n",
      "1\n",
      "worn index: 4, video_cap: < cv2.VideoCapture 0x12cb74570>, label_history: <queue.Queue object at 0x12c502810>, stats_history: <queue.Queue object at 0x126713c50>, centroid_history: <queue.Queue object at 0x12c5b7950>, \n",
      "\n",
      "current label: 4, current_stats: [1010  120   86   87 7454], current_centroid: [1052.49946338  163.04292997], stats_history: <queue.Queue object at 0x126713c50>, centroid_history: <queue.Queue object at 0x12c5b7950>, \n",
      "\n",
      "Current Stats\n",
      "[1010  120   86   87 7454]\n",
      "Current Label\n",
      "4\n",
      "Current Centroid\n",
      "[1052.49946338  163.04292997]\n",
      "J 0\n",
      "Selected Label 1\n",
      "Selected Centroid [1129.28305708  165.21739594]\n",
      "Selected Distance 76.8143773294454\n",
      "New Distance 76.8143773294454\n",
      "New Label\n",
      "Let us see what is the label now 1\n",
      "worn index: 4, video_cap: < cv2.VideoCapture 0x12cb74570>, label_history: <queue.Queue object at 0x12c502810>, stats_history: <queue.Queue object at 0x126713c50>, centroid_history: <queue.Queue object at 0x12c5b7950>, \n",
      "\n",
      "current label: 4, current_stats: [1010  120   86   87 7454], current_centroid: [1052.49946338  163.04292997], stats_history: <queue.Queue object at 0x126713c50>, centroid_history: <queue.Queue object at 0x12c5b7950>, \n",
      "\n",
      "Current Stats\n",
      "[1010  120   86   87 7454]\n",
      "Current Label\n",
      "4\n",
      "Current Centroid\n",
      "[1052.49946338  163.04292997]\n",
      "Let us update\n",
      "worn index: 4, video_cap: < cv2.VideoCapture 0x12cb74570>, label_history: <queue.Queue object at 0x12c502810>, stats_history: <queue.Queue object at 0x126713c50>, centroid_history: <queue.Queue object at 0x12c5b7950>, \n",
      "\n",
      "current label: 1, current_stats: [ 1026   117   210    97 18763], current_centroid: [1129.28305708  165.21739594], stats_history: <queue.Queue object at 0x126713c50>, centroid_history: <queue.Queue object at 0x12c5b7950>, \n",
      "\n",
      "Current Stats\n",
      "[ 1026   117   210    97 18763]\n",
      "Current Label\n",
      "1\n",
      "Current Centroid\n",
      "[1129.28305708  165.21739594]\n",
      "\n",
      " \n",
      " \n",
      " bundle in\n",
      "bundle (19, array([[0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       ...,\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0]], dtype=int32), array([[      0,       0,    2160,    2160, 3920757],\n",
      "       [   1026,     117,     210,      97,   18763],\n",
      "       [    435,     124,     386,     256,   51413],\n",
      "       ...,\n",
      "       [    236,    1587,     140,     116,   15035],\n",
      "       [    379,    1588,     493,     363,  114735],\n",
      "       [    736,    1959,      75,      78,    5850]], dtype=int32), array([[1076.20338062, 1036.87229379],\n",
      "       [1129.28305708,  165.21739594],\n",
      "       [ 620.73300527,  248.3179546 ],\n",
      "       [ 378.49350649,  322.49350649],\n",
      "       [ 407.39727754,  451.01222586],\n",
      "       [ 234.27232116,  627.71403562],\n",
      "       [1966.45045875,  744.73146762],\n",
      "       [ 785.38512205,  871.66690403],\n",
      "       [ 141.5       ,  792.5       ],\n",
      "       [1094.37245013,  902.39397817],\n",
      "       [ 130.57567881,  956.6339103 ],\n",
      "       [1992.73415299, 1193.00398268],\n",
      "       [ 525.52005468, 1075.33799777],\n",
      "       [1553.25298932, 1172.01448399],\n",
      "       [1064.24870001, 1308.38552617],\n",
      "       [1407.79749064, 1875.67034953],\n",
      "       [ 305.81935484, 1645.51659461],\n",
      "       [ 622.79866649, 1757.73856278],\n",
      "       [ 773.        , 1997.5       ]]))\n",
      "worn index: 5, video_cap: < cv2.VideoCapture 0x12cb74570>, label_history: <queue.Queue object at 0x12c5b7610>, stats_history: <queue.Queue object at 0x12c5b51d0>, centroid_history: <queue.Queue object at 0x12c5b4710>, \n",
      "\n",
      "current label: 5, current_stats: [  359   124   443   377 67112], current_centroid: [574.51002801 280.78412206], stats_history: <queue.Queue object at 0x12c5b51d0>, centroid_history: <queue.Queue object at 0x12c5b4710>, \n",
      "\n",
      "Current Stats\n",
      "[  359   124   443   377 67112]\n",
      "Current Label\n",
      "5\n",
      "Current Centroid\n",
      "[574.51002801 280.78412206]\n",
      "stats, [ 1026   117   210    97 18763]\n",
      "stats, [  435   124   386   256 51413]\n",
      "Overlapped Label 2\n",
      "stats, [ 341  285   76   76 5775]\n",
      "Overlapped Label 3\n",
      "stats, [ 363  407   90   89 7934]\n",
      "Overlapped Label 4\n",
      "stats, [  171   557   127   142 16565]\n",
      "stats, [ 1887   657   160   196 26049]\n",
      "stats, [  676   714   229   312 44939]\n",
      "stats, [ 103  755   78   76 5926]\n",
      "stats, [  999   809   186   192 26570]\n",
      "stats, [   72   866   114   175 18083]\n",
      "stats, [ 1884   905   230   588 83612]\n",
      "stats, [  447   952   160   254 27799]\n",
      "stats, [ 1486  1056   138   238 28100]\n",
      "stats, [  900  1165   340   283 53462]\n",
      "stats, [   903   1566    996    527 194233]\n",
      "stats, [  236  1587   140   116 15035]\n",
      "stats, [   379   1588    493    363 114735]\n",
      "stats, [ 736 1959   75   78 5850]\n",
      "Print the Queue\n",
      "2\n",
      "3\n",
      "4\n",
      "worn index: 5, video_cap: < cv2.VideoCapture 0x12cb74570>, label_history: <queue.Queue object at 0x12c5b7610>, stats_history: <queue.Queue object at 0x12c5b51d0>, centroid_history: <queue.Queue object at 0x12c5b4710>, \n",
      "\n",
      "current label: 5, current_stats: [  359   124   443   377 67112], current_centroid: [574.51002801 280.78412206], stats_history: <queue.Queue object at 0x12c5b51d0>, centroid_history: <queue.Queue object at 0x12c5b4710>, \n",
      "\n",
      "Current Stats\n",
      "[  359   124   443   377 67112]\n",
      "Current Label\n",
      "5\n",
      "Current Centroid\n",
      "[574.51002801 280.78412206]\n",
      "J 0\n",
      "Selected Label 2\n",
      "Selected Centroid [620.73300527 248.3179546 ]\n",
      "Selected Distance 56.48553492938935\n",
      "New Distance 56.48553492938935\n",
      "New Label\n",
      "J 1\n",
      "Selected Label 3\n",
      "Selected Centroid [378.49350649 322.49350649]\n",
      "Selected Distance 200.40496365606884\n",
      "J 2\n",
      "Selected Label None\n",
      "Selected Centroid [[[1076.20338062 1036.87229379]\n",
      "  [1129.28305708  165.21739594]\n",
      "  [ 620.73300527  248.3179546 ]\n",
      "  [ 378.49350649  322.49350649]\n",
      "  [ 407.39727754  451.01222586]\n",
      "  [ 234.27232116  627.71403562]\n",
      "  [1966.45045875  744.73146762]\n",
      "  [ 785.38512205  871.66690403]\n",
      "  [ 141.5         792.5       ]\n",
      "  [1094.37245013  902.39397817]\n",
      "  [ 130.57567881  956.6339103 ]\n",
      "  [1992.73415299 1193.00398268]\n",
      "  [ 525.52005468 1075.33799777]\n",
      "  [1553.25298932 1172.01448399]\n",
      "  [1064.24870001 1308.38552617]\n",
      "  [1407.79749064 1875.67034953]\n",
      "  [ 305.81935484 1645.51659461]\n",
      "  [ 622.79866649 1757.73856278]\n",
      "  [ 773.         1997.5       ]]]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 1 is out of bounds for axis 0 with size 1",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mIndexError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[63], line 25\u001B[0m\n\u001B[1;32m     23\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;241m1\u001B[39m, \u001B[38;5;28mlen\u001B[39m(w_list)):\n\u001B[1;32m     24\u001B[0m     temp_Worm \u001B[38;5;241m=\u001B[39m  w_list[i]\n\u001B[0;32m---> 25\u001B[0m     temp_Worm\u001B[38;5;241m.\u001B[39mtake_the_bundle_in(bundle)\n\u001B[1;32m     26\u001B[0m     temp_Worm\u001B[38;5;241m.\u001B[39mwrite_frame(\u001B[38;5;241m4\u001B[39m)\n\u001B[1;32m     27\u001B[0m     w_list[i] \u001B[38;5;241m=\u001B[39m temp_Worm\n",
      "Cell \u001B[0;32mIn[56], line 82\u001B[0m, in \u001B[0;36mWorm.take_the_bundle_in\u001B[0;34m(self, bundle)\u001B[0m\n\u001B[1;32m     80\u001B[0m this_centroid \u001B[38;5;241m=\u001B[39m centroids[this_label_selected]\n\u001B[1;32m     81\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mSelected Centroid\u001B[39m\u001B[38;5;124m'\u001B[39m, this_centroid)\n\u001B[0;32m---> 82\u001B[0m this_distance \u001B[38;5;241m=\u001B[39m distance_between_points(this_centroid[\u001B[38;5;241m0\u001B[39m], this_centroid[\u001B[38;5;241m1\u001B[39m], \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcurrent_centroid[\u001B[38;5;241m0\u001B[39m], \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcurrent_centroid[\u001B[38;5;241m1\u001B[39m])\n\u001B[1;32m     83\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mSelected Distance\u001B[39m\u001B[38;5;124m'\u001B[39m, this_distance)\n\u001B[1;32m     84\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m this_distance \u001B[38;5;241m<\u001B[39m centroid_distance:\n",
      "\u001B[0;31mIndexError\u001B[0m: index 1 is out of bounds for axis 0 with size 1"
     ]
    }
   ],
   "source": [
    "# Read the second frame\n",
    "ret2, frame2 = cap.read()\n",
    "next = cv2.cvtColor(frame2, cv2.COLOR_BGR2GRAY)\n",
    "# Read the original frame\n",
    "ret3, ori_frame = original_cap.read()\n",
    "\n",
    "# Calculate Optical Flow\n",
    "flow = cv2.calcOpticalFlowFarneback(prvs, next, None, 0.5, 3, 15, 3, 5, 1.2, 0)\n",
    "# Compute magnitude and angle of the flow\n",
    "mag, ang = cv2.cartToPolar(flow[..., 0], flow[..., 1])\n",
    "# Create motion mask\n",
    "thresh = 1  # Set threshold for motion detection\n",
    "motion_mask = cv2.threshold(mag, thresh, 255, cv2.THRESH_BINARY)[1]\n",
    "eroded_frame = cv2.erode(motion_mask, erode_kernel, iterations=1)\n",
    "dilated_frame = cv2.dilate(eroded_frame, dilate_kernel, iterations=1)\n",
    "bundle = cv2.connectedComponentsWithStats(np.uint8(dilated_frame), connectivity=8)\n",
    "num_labels, labels, stats, centroids = cv2.connectedComponentsWithStats(np.uint8(dilated_frame), connectivity=8)\n",
    "\n",
    "worm_1.take_the_bundle_in(bundle)\n",
    "worm_1.write_frame(4)\n",
    "\n",
    "# process the worm list\n",
    "for i in range(1, len(w_list)):\n",
    "    temp_Worm =  w_list[i]\n",
    "    temp_Worm.take_the_bundle_in(bundle)\n",
    "    temp_Worm.write_frame(4)\n",
    "    w_list[i] = temp_Worm\n",
    "\n",
    "for i in range(1, num_labels):\n",
    "    x, y, w, h, area = stats[i]\n",
    "    if area > min_area:\n",
    "        cv2.rectangle(dilated_frame, (x, y), (x + w, y + h), (255, 255, 255), 2)\n",
    "for i in range(1, num_labels):\n",
    "    x, y, w, h, area = stats[i]\n",
    "    if area > min_area:\n",
    "        cv2.rectangle(ori_frame, (x, y), (x + w, y + h), (250,128,114), 2)\n",
    "temp_frame = cv2.normalize(dilated_frame, None, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX, dtype=cv2.CV_8U)\n",
    "rgb_frame = cv2.cvtColor(temp_frame, cv2.COLOR_GRAY2RGB)\n",
    "prvs = next\n",
    "cv2.imwrite(os.path.join(monitor_folder, f\"frame_{4}.jpg\"), rgb_frame)\n",
    "cv2.imwrite(os.path.join(output_folder, f\"frame_{4}.jpg\"), ori_frame)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "worm_1.display_info()\n",
    "print(worm_1.output_folder)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Proceed to the Fifth Frame"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Read the second frame\n",
    "ret2, frame2 = cap.read()\n",
    "next = cv2.cvtColor(frame2, cv2.COLOR_BGR2GRAY)\n",
    "# Read the original frame\n",
    "ret3, ori_frame = original_cap.read()\n",
    "\n",
    "# Calculate Optical Flow\n",
    "flow = cv2.calcOpticalFlowFarneback(prvs, next, None, 0.5, 3, 15, 3, 5, 1.2, 0)\n",
    "# Compute magnitude and angle of the flow\n",
    "mag, ang = cv2.cartToPolar(flow[..., 0], flow[..., 1])\n",
    "# Create motion mask\n",
    "thresh = 1  # Set threshold for motion detection\n",
    "motion_mask = cv2.threshold(mag, thresh, 255, cv2.THRESH_BINARY)[1]\n",
    "eroded_frame = cv2.erode(motion_mask, erode_kernel, iterations=1)\n",
    "dilated_frame = cv2.dilate(eroded_frame, dilate_kernel, iterations=1)\n",
    "bundle = cv2.connectedComponentsWithStats(np.uint8(dilated_frame), connectivity=8)\n",
    "num_labels, labels, stats, centroids = cv2.connectedComponentsWithStats(np.uint8(dilated_frame), connectivity=8)\n",
    "\n",
    "worm_1.take_the_bundle_in(bundle)\n",
    "worm_1.write_frame(5)\n",
    "\n",
    "# process the worm list\n",
    "for i in range(1, len(w_list)):\n",
    "    temp_Worm = w_list[i]\n",
    "    temp_Worm.take_the_bundle_in(bundle)\n",
    "    temp_Worm.write_frame(5)\n",
    "    w_list[i] = temp_Worm\n",
    "\n",
    "for i in range(1, num_labels):\n",
    "    x, y, w, h, area = stats[i]\n",
    "    if area > min_area:\n",
    "        cv2.rectangle(dilated_frame, (x, y), (x + w, y + h), (255, 255, 255), 2)\n",
    "for i in range(1, num_labels):\n",
    "    x, y, w, h, area = stats[i]\n",
    "    if area > min_area:\n",
    "        cv2.rectangle(ori_frame, (x, y), (x + w, y + h), (250,128,114), 2)\n",
    "temp_frame = cv2.normalize(dilated_frame, None, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX, dtype=cv2.CV_8U)\n",
    "rgb_frame = cv2.cvtColor(temp_frame, cv2.COLOR_GRAY2RGB)\n",
    "prvs = next\n",
    "cv2.imwrite(os.path.join(monitor_folder, f\"frame_{5}.jpg\"), rgb_frame)\n",
    "cv2.imwrite(os.path.join(output_folder, f\"frame_{5}.jpg\"), ori_frame)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "worm_1.display_info()\n",
    "print(worm_1.output_folder)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Proceed to the Sixth Frame"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Read the second frame\n",
    "ret2, frame2 = cap.read()\n",
    "next = cv2.cvtColor(frame2, cv2.COLOR_BGR2GRAY)\n",
    "# Read the original frame\n",
    "ret3, ori_frame = original_cap.read()\n",
    "\n",
    "# Calculate Optical Flow\n",
    "flow = cv2.calcOpticalFlowFarneback(prvs, next, None, 0.5, 3, 15, 3, 5, 1.2, 0)\n",
    "# Compute magnitude and angle of the flow\n",
    "mag, ang = cv2.cartToPolar(flow[..., 0], flow[..., 1])\n",
    "# Create motion mask\n",
    "thresh = 1  # Set threshold for motion detection\n",
    "motion_mask = cv2.threshold(mag, thresh, 255, cv2.THRESH_BINARY)[1]\n",
    "eroded_frame = cv2.erode(motion_mask, erode_kernel, iterations=1)\n",
    "dilated_frame = cv2.dilate(eroded_frame, dilate_kernel, iterations=1)\n",
    "bundle = cv2.connectedComponentsWithStats(np.uint8(dilated_frame), connectivity=8)\n",
    "num_labels, labels, stats, centroids = cv2.connectedComponentsWithStats(np.uint8(dilated_frame), connectivity=8)\n",
    "\n",
    "worm_1.take_the_bundle_in(bundle)\n",
    "worm_1.write_frame(6)\n",
    "\n",
    "# process the worm list\n",
    "for i in range(1, len(w_list)):\n",
    "    temp_Worm = w_list[i]\n",
    "    temp_Worm.take_the_bundle_in(bundle)\n",
    "    temp_Worm.write_frame(6)\n",
    "    w_list[i] = temp_Worm\n",
    "\n",
    "for i in range(1, num_labels):\n",
    "    x, y, w, h, area = stats[i]\n",
    "    if area > min_area:\n",
    "        cv2.rectangle(dilated_frame, (x, y), (x + w, y + h), (255, 255, 255), 2)\n",
    "for i in range(1, num_labels):\n",
    "    x, y, w, h, area = stats[i]\n",
    "    if area > min_area:\n",
    "        cv2.rectangle(ori_frame, (x, y), (x + w, y + h), (250,128,114), 2)\n",
    "temp_frame = cv2.normalize(dilated_frame, None, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX, dtype=cv2.CV_8U)\n",
    "rgb_frame = cv2.cvtColor(temp_frame, cv2.COLOR_GRAY2RGB)\n",
    "prvs = next\n",
    "cv2.imwrite(os.path.join(monitor_folder, f\"frame_{6}.jpg\"), rgb_frame)\n",
    "cv2.imwrite(os.path.join(output_folder, f\"frame_{6}.jpg\"), ori_frame)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "worm_1.display_info()\n",
    "print(worm_1.output_folder)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Reset and Process all the frames"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Start with the First Frame"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "original_cap = cv2.VideoCapture('shorter.mp4')\n",
    "\n",
    "cap = cv2.VideoCapture('processed_v2.mp4')\n",
    "\n",
    "frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "frame_rate = cap.get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "erode_kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (2, 2))\n",
    "dilate_kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (75, 75))\n",
    "\n",
    "# Read the first frame\n",
    "ret1, frame1 = cap.read()\n",
    "prvs = cv2.cvtColor(frame1, cv2.COLOR_BGR2GRAY)\n",
    "# Read the second frame\n",
    "ret2, frame2 = cap.read()\n",
    "next = cv2.cvtColor(frame2, cv2.COLOR_BGR2GRAY)\n",
    "# Read the original frame\n",
    "ret3, ori_frame = original_cap.read()\n",
    "\n",
    "# Calculate Optical Flow\n",
    "flow = cv2.calcOpticalFlowFarneback(prvs, next, None, 0.5, 3, 15, 3, 5, 1.2, 0)\n",
    "# Compute magnitude and angle of the flow\n",
    "mag, ang = cv2.cartToPolar(flow[..., 0], flow[..., 1])\n",
    "# Create motion mask\n",
    "thresh = 1  # Set threshold for motion detection\n",
    "motion_mask = cv2.threshold(mag, thresh, 255, cv2.THRESH_BINARY)[1]\n",
    "eroded_frame = cv2.erode(motion_mask, erode_kernel, iterations=1)\n",
    "dilated_frame = cv2.dilate(eroded_frame, dilate_kernel, iterations=1)\n",
    "num_labels, labels, stats, centroids = cv2.connectedComponentsWithStats(np.uint8(dilated_frame), connectivity=8)\n",
    "for i in range(1, num_labels):\n",
    "    x, y, w, h, area = stats[i]\n",
    "    if area > min_area:\n",
    "        cv2.rectangle(dilated_frame, (x, y), (x + w, y + h), (255, 255, 255), 2)\n",
    "for i in range(1, num_labels):\n",
    "    x, y, w, h, area = stats[i]\n",
    "    if area > min_area:\n",
    "        cv2.rectangle(ori_frame, (x, y), (x + w, y + h), (250,128,114), 2)\n",
    "temp_frame = cv2.normalize(dilated_frame, None, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX, dtype=cv2.CV_8U)\n",
    "rgb_frame = cv2.cvtColor(temp_frame, cv2.COLOR_GRAY2RGB)\n",
    "prvs = next\n",
    "cv2.imwrite(os.path.join(monitor_folder, f\"frame_{1}.jpg\"), rgb_frame)\n",
    "cv2.imwrite(os.path.join(output_folder, f\"frame_{1}.jpg\"), ori_frame)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Process the Second Frame"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "outputs": [
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read the second frame\n",
    "ret2, frame2 = cap.read()\n",
    "next = cv2.cvtColor(frame2, cv2.COLOR_BGR2GRAY)\n",
    "# Read the original frame\n",
    "ret3, ori_frame = original_cap.read()\n",
    "\n",
    "# Calculate Optical Flow\n",
    "flow = cv2.calcOpticalFlowFarneback(prvs, next, None, 0.5, 3, 15, 3, 5, 1.2, 0)\n",
    "# Compute magnitude and angle of the flow\n",
    "mag, ang = cv2.cartToPolar(flow[..., 0], flow[..., 1])\n",
    "# Create motion mask\n",
    "thresh = 1  # Set threshold for motion detection\n",
    "motion_mask = cv2.threshold(mag, thresh, 255, cv2.THRESH_BINARY)[1]\n",
    "eroded_frame = cv2.erode(motion_mask, erode_kernel, iterations=1)\n",
    "dilated_frame = cv2.dilate(eroded_frame, dilate_kernel, iterations=1)\n",
    "num_labels, labels, stats, centroids = cv2.connectedComponentsWithStats(np.uint8(dilated_frame), connectivity=8)\n",
    "for i in range(1, num_labels):\n",
    "    x, y, w, h, area = stats[i]\n",
    "    if area > min_area:\n",
    "        cv2.rectangle(dilated_frame, (x, y), (x + w, y + h), (255, 255, 255), 2)\n",
    "for i in range(1, num_labels):\n",
    "    x, y, w, h, area = stats[i]\n",
    "    if area > min_area:\n",
    "        cv2.rectangle(ori_frame, (x, y), (x + w, y + h), (250,128,114), 2)\n",
    "temp_frame = cv2.normalize(dilated_frame, None, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX, dtype=cv2.CV_8U)\n",
    "rgb_frame = cv2.cvtColor(temp_frame, cv2.COLOR_GRAY2RGB)\n",
    "prvs = next\n",
    "cv2.imwrite(os.path.join(monitor_folder, f\"frame_{2}.jpg\"), rgb_frame)\n",
    "cv2.imwrite(os.path.join(output_folder, f\"frame_{2}.jpg\"), ori_frame)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Set up worm list"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "outputs": [],
   "source": [
    "# set up the number that I want to track for worms\n",
    "track_number = 10\n",
    "\n",
    "# create the list of n tracking number\n",
    "w_list = create_list_of_n_objects(track_number)\n",
    "\n",
    "for i in range(1, len(w_list)):\n",
    "    temp_Worm = Worm(i, original_cap, i, stats[i], centroids[i])\n",
    "    w_list[i] = temp_Worm"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Make sure the list is well set"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "worn index: 1, video_cap: < cv2.VideoCapture 0x12cb74570>, label_history: <queue.Queue object at 0x12c5b58d0>, stats_history: <queue.Queue object at 0x12c5b7850>, centroid_history: <queue.Queue object at 0x12c5b6690>, \n",
      "\n",
      "current label: 1, current_stats: [ 930    0   82   82 6690], current_centroid: [970.39895366  40.39596413], stats_history: <queue.Queue object at 0x12c5b7850>, centroid_history: <queue.Queue object at 0x12c5b6690>, \n",
      "\n",
      "Current Stats\n",
      "[ 930    0   82   82 6690]\n",
      "Current Label\n",
      "1\n",
      "Current Centroid\n",
      "[970.39895366  40.39596413]\n",
      "worn index: 2, video_cap: < cv2.VideoCapture 0x12cb74570>, label_history: <queue.Queue object at 0x12659ee90>, stats_history: <queue.Queue object at 0x12c3c0710>, centroid_history: <queue.Queue object at 0x12c412890>, \n",
      "\n",
      "current label: 2, current_stats: [1071   12   78   75 5850], current_centroid: [1109.5   49. ], stats_history: <queue.Queue object at 0x12c3c0710>, centroid_history: <queue.Queue object at 0x12c412890>, \n",
      "\n",
      "Current Stats\n",
      "[1071   12   78   75 5850]\n",
      "Current Label\n",
      "2\n",
      "Current Centroid\n",
      "[1109.5   49. ]\n",
      "worn index: 3, video_cap: < cv2.VideoCapture 0x12cb74570>, label_history: <queue.Queue object at 0x12c502810>, stats_history: <queue.Queue object at 0x126713c50>, centroid_history: <queue.Queue object at 0x12c3c16d0>, \n",
      "\n",
      "current label: 3, current_stats: [  364   121   459   376 77099], current_centroid: [575.54205632 276.38399979], stats_history: <queue.Queue object at 0x126713c50>, centroid_history: <queue.Queue object at 0x12c3c16d0>, \n",
      "\n",
      "Current Stats\n",
      "[  364   121   459   376 77099]\n",
      "Current Label\n",
      "3\n",
      "Current Centroid\n",
      "[575.54205632 276.38399979]\n",
      "worn index: 4, video_cap: < cv2.VideoCapture 0x12cb74570>, label_history: <queue.Queue object at 0x12c5929d0>, stats_history: <queue.Queue object at 0x12c0dc890>, centroid_history: <queue.Queue object at 0x12c0d4250>, \n",
      "\n",
      "current label: 4, current_stats: [1225  144   76   75 5700], current_centroid: [1262.5  181. ], stats_history: <queue.Queue object at 0x12c0dc890>, centroid_history: <queue.Queue object at 0x12c0d4250>, \n",
      "\n",
      "Current Stats\n",
      "[1225  144   76   75 5700]\n",
      "Current Label\n",
      "4\n",
      "Current Centroid\n",
      "[1262.5  181. ]\n",
      "worn index: 5, video_cap: < cv2.VideoCapture 0x12cb74570>, label_history: <queue.Queue object at 0x12c41c250>, stats_history: <queue.Queue object at 0x12c593950>, centroid_history: <queue.Queue object at 0x12c5c6890>, \n",
      "\n",
      "current label: 5, current_stats: [ 324  274   77   75 5775], current_centroid: [362. 311.], stats_history: <queue.Queue object at 0x12c593950>, centroid_history: <queue.Queue object at 0x12c5c6890>, \n",
      "\n",
      "Current Stats\n",
      "[ 324  274   77   75 5775]\n",
      "Current Label\n",
      "5\n",
      "Current Centroid\n",
      "[362. 311.]\n",
      "worn index: 6, video_cap: < cv2.VideoCapture 0x12cb74570>, label_history: <queue.Queue object at 0x12c48e350>, stats_history: <queue.Queue object at 0x12c48fe90>, centroid_history: <queue.Queue object at 0x12c48d450>, \n",
      "\n",
      "current label: 6, current_stats: [  170   551   132   147 17909], current_centroid: [237.48567759 623.42481434], stats_history: <queue.Queue object at 0x12c48fe90>, centroid_history: <queue.Queue object at 0x12c48d450>, \n",
      "\n",
      "Current Stats\n",
      "[  170   551   132   147 17909]\n",
      "Current Label\n",
      "6\n",
      "Current Centroid\n",
      "[237.48567759 623.42481434]\n",
      "worn index: 7, video_cap: < cv2.VideoCapture 0x12cb74570>, label_history: <queue.Queue object at 0x12c5bd510>, stats_history: <queue.Queue object at 0x12c5bdbd0>, centroid_history: <queue.Queue object at 0x12c5e5910>, \n",
      "\n",
      "current label: 7, current_stats: [ 1886   625   166   231 32470], current_centroid: [1968.24696643  731.15060055], stats_history: <queue.Queue object at 0x12c5bdbd0>, centroid_history: <queue.Queue object at 0x12c5e5910>, \n",
      "\n",
      "Current Stats\n",
      "[ 1886   625   166   231 32470]\n",
      "Current Label\n",
      "7\n",
      "Current Centroid\n",
      "[1968.24696643  731.15060055]\n",
      "worn index: 8, video_cap: < cv2.VideoCapture 0x12cb74570>, label_history: <queue.Queue object at 0x12c5bdd50>, stats_history: <queue.Queue object at 0x12c5bf610>, centroid_history: <queue.Queue object at 0x12c5bc910>, \n",
      "\n",
      "current label: 8, current_stats: [  61  685   77   80 6156], current_centroid: [ 99.         724.48732943], stats_history: <queue.Queue object at 0x12c5bf610>, centroid_history: <queue.Queue object at 0x12c5bc910>, \n",
      "\n",
      "Current Stats\n",
      "[  61  685   77   80 6156]\n",
      "Current Label\n",
      "8\n",
      "Current Centroid\n",
      "[ 99.         724.48732943]\n",
      "worn index: 9, video_cap: < cv2.VideoCapture 0x12cb74570>, label_history: <queue.Queue object at 0x12c46aa90>, stats_history: <queue.Queue object at 0x12c633a90>, centroid_history: <queue.Queue object at 0x12c5a5010>, \n",
      "\n",
      "current label: 9, current_stats: [  675   707   232   328 50039], current_centroid: [786.31535402 873.65083235], stats_history: <queue.Queue object at 0x12c633a90>, centroid_history: <queue.Queue object at 0x12c5a5010>, \n",
      "\n",
      "Current Stats\n",
      "[  675   707   232   328 50039]\n",
      "Current Label\n",
      "9\n",
      "Current Centroid\n",
      "[786.31535402 873.65083235]\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, len(w_list)):\n",
    "    temp_Worm = Worm(i, original_cap, i, stats[i], centroids[i])\n",
    "    temp_Worm.display_info()\n",
    "    w_list[i] = temp_Worm"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Run the loop to parse the video"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frame Number: 3 \n",
      "Number of Labels 17\n",
      "Labels [[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n",
      "stats [[      0       0    2160    2160 3845960]\n",
      " [    425     123     397     267   57098]\n",
      " [    327     278      85      82    6937]\n",
      " ...\n",
      " [    231    1580     148     128   17436]\n",
      " [    541    1591     337     359   78658]\n",
      " [    385    1627     223     232   41013]]\n",
      "centroid [[1074.00021087 1029.17049683]\n",
      " [ 617.63856878  248.42047007]\n",
      " [ 369.00562203  318.6055932 ]\n",
      " [ 241.00559399  447.52907405]\n",
      " [ 234.33637489  631.99857857]\n",
      " [1988.49712845 1061.91055517]\n",
      " [  94.92513654  719.95883842]\n",
      " [ 784.86353267  870.38376529]\n",
      " [1096.34045132  907.20630281]\n",
      " [ 131.14996755  945.50997891]\n",
      " [ 527.00341228 1074.64923725]\n",
      " [1552.81099578 1171.14885884]\n",
      " [1060.19597674 1300.80867515]\n",
      " [1414.45199885 1875.97230703]\n",
      " [ 306.00154852 1645.06509521]\n",
      " [ 711.7786112  1772.67215032]\n",
      " [ 487.40562748 1729.43593495]]\n",
      "\n",
      " \n",
      " \n",
      " bundle in\n",
      "bundle (17, array([[0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       ...,\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0]], dtype=int32), array([[      0,       0,    2160,    2160, 3845960],\n",
      "       [    425,     123,     397,     267,   57098],\n",
      "       [    327,     278,      85,      82,    6937],\n",
      "       ...,\n",
      "       [    231,    1580,     148,     128,   17436],\n",
      "       [    541,    1591,     337,     359,   78658],\n",
      "       [    385,    1627,     223,     232,   41013]], dtype=int32), array([[1074.00021087, 1029.17049683],\n",
      "       [ 617.63856878,  248.42047007],\n",
      "       [ 369.00562203,  318.6055932 ],\n",
      "       [ 241.00559399,  447.52907405],\n",
      "       [ 234.33637489,  631.99857857],\n",
      "       [1988.49712845, 1061.91055517],\n",
      "       [  94.92513654,  719.95883842],\n",
      "       [ 784.86353267,  870.38376529],\n",
      "       [1096.34045132,  907.20630281],\n",
      "       [ 131.14996755,  945.50997891],\n",
      "       [ 527.00341228, 1074.64923725],\n",
      "       [1552.81099578, 1171.14885884],\n",
      "       [1060.19597674, 1300.80867515],\n",
      "       [1414.45199885, 1875.97230703],\n",
      "       [ 306.00154852, 1645.06509521],\n",
      "       [ 711.7786112 , 1772.67215032],\n",
      "       [ 487.40562748, 1729.43593495]]))\n",
      "worn index: 1, video_cap: < cv2.VideoCapture 0x12cb74570>, label_history: <queue.Queue object at 0x12c5b58d0>, stats_history: <queue.Queue object at 0x12c5b7850>, centroid_history: <queue.Queue object at 0x12c5b6690>, \n",
      "\n",
      "current label: 1, current_stats: [ 930    0   82   82 6690], current_centroid: [970.39895366  40.39596413], stats_history: <queue.Queue object at 0x12c5b7850>, centroid_history: <queue.Queue object at 0x12c5b6690>, \n",
      "\n",
      "Current Stats\n",
      "[ 930    0   82   82 6690]\n",
      "Current Label\n",
      "1\n",
      "Current Centroid\n",
      "[970.39895366  40.39596413]\n",
      "stats, [  425   123   397   267 57098]\n",
      "stats, [ 327  278   85   82 6937]\n",
      "stats, [ 200  407   83   82 6793]\n",
      "stats, [  164   545   142   176 21809]\n",
      "stats, [  1883    627    234    871 130069]\n",
      "stats, [  53  676   85   89 7507]\n",
      "stats, [  668   698   243   341 53478]\n",
      "stats, [  994   816   200   190 28051]\n",
      "stats, [   70   832   122   224 24652]\n",
      "stats, [  444   949   160   261 29892]\n",
      "stats, [ 1484  1051   144   248 30539]\n",
      "stats, [  888  1145   357   305 63630]\n",
      "stats, [   883   1561   1031    541 222078]\n",
      "stats, [  231  1580   148   128 17436]\n",
      "stats, [  541  1591   337   359 78658]\n",
      "stats, [  385  1627   223   232 41013]\n",
      "Print the Queue\n",
      "worn index: 1, video_cap: < cv2.VideoCapture 0x12cb74570>, label_history: <queue.Queue object at 0x12c5b58d0>, stats_history: <queue.Queue object at 0x12c5b7850>, centroid_history: <queue.Queue object at 0x12c5b6690>, \n",
      "\n",
      "current label: 1, current_stats: [ 930    0   82   82 6690], current_centroid: [970.39895366  40.39596413], stats_history: <queue.Queue object at 0x12c5b7850>, centroid_history: <queue.Queue object at 0x12c5b6690>, \n",
      "\n",
      "Current Stats\n",
      "[ 930    0   82   82 6690]\n",
      "Current Label\n",
      "1\n",
      "Current Centroid\n",
      "[970.39895366  40.39596413]\n",
      "Let us see what is the label now -1\n",
      "worn index: 1, video_cap: < cv2.VideoCapture 0x12cb74570>, label_history: <queue.Queue object at 0x12c5b58d0>, stats_history: <queue.Queue object at 0x12c5b7850>, centroid_history: <queue.Queue object at 0x12c5b6690>, \n",
      "\n",
      "current label: 1, current_stats: [ 930    0   82   82 6690], current_centroid: [970.39895366  40.39596413], stats_history: <queue.Queue object at 0x12c5b7850>, centroid_history: <queue.Queue object at 0x12c5b6690>, \n",
      "\n",
      "Current Stats\n",
      "[ 930    0   82   82 6690]\n",
      "Current Label\n",
      "1\n",
      "Current Centroid\n",
      "[970.39895366  40.39596413]\n",
      "No Criterion satisfied, jump to the nearest frame\n",
      "L label 1\n",
      "Let us update\n",
      "worn index: 1, video_cap: < cv2.VideoCapture 0x12cb74570>, label_history: <queue.Queue object at 0x12c5b58d0>, stats_history: <queue.Queue object at 0x12c5b7850>, centroid_history: <queue.Queue object at 0x12c5b6690>, \n",
      "\n",
      "current label: 1, current_stats: [  425   123   397   267 57098], current_centroid: [617.63856878 248.42047007], stats_history: <queue.Queue object at 0x12c5b7850>, centroid_history: <queue.Queue object at 0x12c5b6690>, \n",
      "\n",
      "Current Stats\n",
      "[  425   123   397   267 57098]\n",
      "Current Label\n",
      "1\n",
      "Current Centroid\n",
      "[617.63856878 248.42047007]\n",
      "\n",
      " \n",
      " \n",
      " bundle in\n",
      "bundle (17, array([[0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       ...,\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0]], dtype=int32), array([[      0,       0,    2160,    2160, 3845960],\n",
      "       [    425,     123,     397,     267,   57098],\n",
      "       [    327,     278,      85,      82,    6937],\n",
      "       ...,\n",
      "       [    231,    1580,     148,     128,   17436],\n",
      "       [    541,    1591,     337,     359,   78658],\n",
      "       [    385,    1627,     223,     232,   41013]], dtype=int32), array([[1074.00021087, 1029.17049683],\n",
      "       [ 617.63856878,  248.42047007],\n",
      "       [ 369.00562203,  318.6055932 ],\n",
      "       [ 241.00559399,  447.52907405],\n",
      "       [ 234.33637489,  631.99857857],\n",
      "       [1988.49712845, 1061.91055517],\n",
      "       [  94.92513654,  719.95883842],\n",
      "       [ 784.86353267,  870.38376529],\n",
      "       [1096.34045132,  907.20630281],\n",
      "       [ 131.14996755,  945.50997891],\n",
      "       [ 527.00341228, 1074.64923725],\n",
      "       [1552.81099578, 1171.14885884],\n",
      "       [1060.19597674, 1300.80867515],\n",
      "       [1414.45199885, 1875.97230703],\n",
      "       [ 306.00154852, 1645.06509521],\n",
      "       [ 711.7786112 , 1772.67215032],\n",
      "       [ 487.40562748, 1729.43593495]]))\n",
      "worn index: 2, video_cap: < cv2.VideoCapture 0x12cb74570>, label_history: <queue.Queue object at 0x12659ee90>, stats_history: <queue.Queue object at 0x12c3c0710>, centroid_history: <queue.Queue object at 0x12c412890>, \n",
      "\n",
      "current label: 2, current_stats: [1071   12   78   75 5850], current_centroid: [1109.5   49. ], stats_history: <queue.Queue object at 0x12c3c0710>, centroid_history: <queue.Queue object at 0x12c412890>, \n",
      "\n",
      "Current Stats\n",
      "[1071   12   78   75 5850]\n",
      "Current Label\n",
      "2\n",
      "Current Centroid\n",
      "[1109.5   49. ]\n",
      "stats, [  425   123   397   267 57098]\n",
      "stats, [ 327  278   85   82 6937]\n",
      "stats, [ 200  407   83   82 6793]\n",
      "stats, [  164   545   142   176 21809]\n",
      "stats, [  1883    627    234    871 130069]\n",
      "stats, [  53  676   85   89 7507]\n",
      "stats, [  668   698   243   341 53478]\n",
      "stats, [  994   816   200   190 28051]\n",
      "stats, [   70   832   122   224 24652]\n",
      "stats, [  444   949   160   261 29892]\n",
      "stats, [ 1484  1051   144   248 30539]\n",
      "stats, [  888  1145   357   305 63630]\n",
      "stats, [   883   1561   1031    541 222078]\n",
      "stats, [  231  1580   148   128 17436]\n",
      "stats, [  541  1591   337   359 78658]\n",
      "stats, [  385  1627   223   232 41013]\n",
      "Print the Queue\n",
      "worn index: 2, video_cap: < cv2.VideoCapture 0x12cb74570>, label_history: <queue.Queue object at 0x12659ee90>, stats_history: <queue.Queue object at 0x12c3c0710>, centroid_history: <queue.Queue object at 0x12c412890>, \n",
      "\n",
      "current label: 2, current_stats: [1071   12   78   75 5850], current_centroid: [1109.5   49. ], stats_history: <queue.Queue object at 0x12c3c0710>, centroid_history: <queue.Queue object at 0x12c412890>, \n",
      "\n",
      "Current Stats\n",
      "[1071   12   78   75 5850]\n",
      "Current Label\n",
      "2\n",
      "Current Centroid\n",
      "[1109.5   49. ]\n",
      "Let us see what is the label now -1\n",
      "worn index: 2, video_cap: < cv2.VideoCapture 0x12cb74570>, label_history: <queue.Queue object at 0x12659ee90>, stats_history: <queue.Queue object at 0x12c3c0710>, centroid_history: <queue.Queue object at 0x12c412890>, \n",
      "\n",
      "current label: 2, current_stats: [1071   12   78   75 5850], current_centroid: [1109.5   49. ], stats_history: <queue.Queue object at 0x12c3c0710>, centroid_history: <queue.Queue object at 0x12c412890>, \n",
      "\n",
      "Current Stats\n",
      "[1071   12   78   75 5850]\n",
      "Current Label\n",
      "2\n",
      "Current Centroid\n",
      "[1109.5   49. ]\n",
      "No Criterion satisfied, jump to the nearest frame\n",
      "L label 1\n",
      "Let us update\n",
      "worn index: 2, video_cap: < cv2.VideoCapture 0x12cb74570>, label_history: <queue.Queue object at 0x12659ee90>, stats_history: <queue.Queue object at 0x12c3c0710>, centroid_history: <queue.Queue object at 0x12c412890>, \n",
      "\n",
      "current label: 1, current_stats: [  425   123   397   267 57098], current_centroid: [617.63856878 248.42047007], stats_history: <queue.Queue object at 0x12c3c0710>, centroid_history: <queue.Queue object at 0x12c412890>, \n",
      "\n",
      "Current Stats\n",
      "[  425   123   397   267 57098]\n",
      "Current Label\n",
      "1\n",
      "Current Centroid\n",
      "[617.63856878 248.42047007]\n",
      "\n",
      " \n",
      " \n",
      " bundle in\n",
      "bundle (17, array([[0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       ...,\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0]], dtype=int32), array([[      0,       0,    2160,    2160, 3845960],\n",
      "       [    425,     123,     397,     267,   57098],\n",
      "       [    327,     278,      85,      82,    6937],\n",
      "       ...,\n",
      "       [    231,    1580,     148,     128,   17436],\n",
      "       [    541,    1591,     337,     359,   78658],\n",
      "       [    385,    1627,     223,     232,   41013]], dtype=int32), array([[1074.00021087, 1029.17049683],\n",
      "       [ 617.63856878,  248.42047007],\n",
      "       [ 369.00562203,  318.6055932 ],\n",
      "       [ 241.00559399,  447.52907405],\n",
      "       [ 234.33637489,  631.99857857],\n",
      "       [1988.49712845, 1061.91055517],\n",
      "       [  94.92513654,  719.95883842],\n",
      "       [ 784.86353267,  870.38376529],\n",
      "       [1096.34045132,  907.20630281],\n",
      "       [ 131.14996755,  945.50997891],\n",
      "       [ 527.00341228, 1074.64923725],\n",
      "       [1552.81099578, 1171.14885884],\n",
      "       [1060.19597674, 1300.80867515],\n",
      "       [1414.45199885, 1875.97230703],\n",
      "       [ 306.00154852, 1645.06509521],\n",
      "       [ 711.7786112 , 1772.67215032],\n",
      "       [ 487.40562748, 1729.43593495]]))\n",
      "worn index: 3, video_cap: < cv2.VideoCapture 0x12cb74570>, label_history: <queue.Queue object at 0x12c502810>, stats_history: <queue.Queue object at 0x126713c50>, centroid_history: <queue.Queue object at 0x12c3c16d0>, \n",
      "\n",
      "current label: 3, current_stats: [  364   121   459   376 77099], current_centroid: [575.54205632 276.38399979], stats_history: <queue.Queue object at 0x126713c50>, centroid_history: <queue.Queue object at 0x12c3c16d0>, \n",
      "\n",
      "Current Stats\n",
      "[  364   121   459   376 77099]\n",
      "Current Label\n",
      "3\n",
      "Current Centroid\n",
      "[575.54205632 276.38399979]\n",
      "stats, [  425   123   397   267 57098]\n",
      "Overlapped Label 1\n",
      "stats, [ 327  278   85   82 6937]\n",
      "Overlapped Label 2\n",
      "stats, [ 200  407   83   82 6793]\n",
      "stats, [  164   545   142   176 21809]\n",
      "stats, [  1883    627    234    871 130069]\n",
      "stats, [  53  676   85   89 7507]\n",
      "stats, [  668   698   243   341 53478]\n",
      "stats, [  994   816   200   190 28051]\n",
      "stats, [   70   832   122   224 24652]\n",
      "stats, [  444   949   160   261 29892]\n",
      "stats, [ 1484  1051   144   248 30539]\n",
      "stats, [  888  1145   357   305 63630]\n",
      "stats, [   883   1561   1031    541 222078]\n",
      "stats, [  231  1580   148   128 17436]\n",
      "stats, [  541  1591   337   359 78658]\n",
      "stats, [  385  1627   223   232 41013]\n",
      "Print the Queue\n",
      "1\n",
      "2\n",
      "worn index: 3, video_cap: < cv2.VideoCapture 0x12cb74570>, label_history: <queue.Queue object at 0x12c502810>, stats_history: <queue.Queue object at 0x126713c50>, centroid_history: <queue.Queue object at 0x12c3c16d0>, \n",
      "\n",
      "current label: 3, current_stats: [  364   121   459   376 77099], current_centroid: [575.54205632 276.38399979], stats_history: <queue.Queue object at 0x126713c50>, centroid_history: <queue.Queue object at 0x12c3c16d0>, \n",
      "\n",
      "Current Stats\n",
      "[  364   121   459   376 77099]\n",
      "Current Label\n",
      "3\n",
      "Current Centroid\n",
      "[575.54205632 276.38399979]\n",
      "J 0\n",
      "Selected Label 1\n",
      "Selected Centroid [617.63856878 248.42047007]\n",
      "Selected Distance 50.53786061786077\n",
      "New Distance 50.53786061786077\n",
      "New Label\n",
      "J 1\n",
      "Selected Label None\n",
      "Selected Centroid [[[1074.00021087 1029.17049683]\n",
      "  [ 617.63856878  248.42047007]\n",
      "  [ 369.00562203  318.6055932 ]\n",
      "  [ 241.00559399  447.52907405]\n",
      "  [ 234.33637489  631.99857857]\n",
      "  [1988.49712845 1061.91055517]\n",
      "  [  94.92513654  719.95883842]\n",
      "  [ 784.86353267  870.38376529]\n",
      "  [1096.34045132  907.20630281]\n",
      "  [ 131.14996755  945.50997891]\n",
      "  [ 527.00341228 1074.64923725]\n",
      "  [1552.81099578 1171.14885884]\n",
      "  [1060.19597674 1300.80867515]\n",
      "  [1414.45199885 1875.97230703]\n",
      "  [ 306.00154852 1645.06509521]\n",
      "  [ 711.7786112  1772.67215032]\n",
      "  [ 487.40562748 1729.43593495]]]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 1 is out of bounds for axis 0 with size 1",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mIndexError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[67], line 29\u001B[0m\n\u001B[1;32m     27\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;241m1\u001B[39m, \u001B[38;5;28mlen\u001B[39m(w_list)):\n\u001B[1;32m     28\u001B[0m     temp_Worm \u001B[38;5;241m=\u001B[39m w_list[i]\n\u001B[0;32m---> 29\u001B[0m     temp_Worm\u001B[38;5;241m.\u001B[39mtake_the_bundle_in(bundle)\n\u001B[1;32m     30\u001B[0m     temp_Worm\u001B[38;5;241m.\u001B[39mwrite_frame(iteration_Number)\n\u001B[1;32m     31\u001B[0m     w_list[i] \u001B[38;5;241m=\u001B[39m temp_Worm\n",
      "Cell \u001B[0;32mIn[56], line 82\u001B[0m, in \u001B[0;36mWorm.take_the_bundle_in\u001B[0;34m(self, bundle)\u001B[0m\n\u001B[1;32m     80\u001B[0m this_centroid \u001B[38;5;241m=\u001B[39m centroids[this_label_selected]\n\u001B[1;32m     81\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mSelected Centroid\u001B[39m\u001B[38;5;124m'\u001B[39m, this_centroid)\n\u001B[0;32m---> 82\u001B[0m this_distance \u001B[38;5;241m=\u001B[39m distance_between_points(this_centroid[\u001B[38;5;241m0\u001B[39m], this_centroid[\u001B[38;5;241m1\u001B[39m], \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcurrent_centroid[\u001B[38;5;241m0\u001B[39m], \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcurrent_centroid[\u001B[38;5;241m1\u001B[39m])\n\u001B[1;32m     83\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mSelected Distance\u001B[39m\u001B[38;5;124m'\u001B[39m, this_distance)\n\u001B[1;32m     84\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m this_distance \u001B[38;5;241m<\u001B[39m centroid_distance:\n",
      "\u001B[0;31mIndexError\u001B[0m: index 1 is out of bounds for axis 0 with size 1"
     ]
    }
   ],
   "source": [
    "for iteration_Number in range(3, int(cap.get(cv2.CAP_PROP_FRAME_COUNT)-1)):\n",
    "\n",
    "    print(f'\\rFrame Number: {iteration_Number} \\n', end='')\n",
    "    # Read the second frame\n",
    "    ret2, frame2 = cap.read()\n",
    "    next = cv2.cvtColor(frame2, cv2.COLOR_BGR2GRAY)\n",
    "    # Read the original frame\n",
    "    ret3, ori_frame = original_cap.read()\n",
    "\n",
    "    # Calculate Optical Flow\n",
    "    flow = cv2.calcOpticalFlowFarneback(prvs, next, None, 0.5, 3, 15, 3, 5, 1.2, 0)\n",
    "    # Compute magnitude and angle of the flow\n",
    "    mag, ang = cv2.cartToPolar(flow[..., 0], flow[..., 1])\n",
    "    # Create motion mask\n",
    "    thresh = 1  # Set threshold for motion detection\n",
    "    motion_mask = cv2.threshold(mag, thresh, 255, cv2.THRESH_BINARY)[1]\n",
    "    eroded_frame = cv2.erode(motion_mask, erode_kernel, iterations=1)\n",
    "    dilated_frame = cv2.dilate(eroded_frame, dilate_kernel, iterations=1)\n",
    "    bundle = cv2.connectedComponentsWithStats(np.uint8(dilated_frame), connectivity=8)\n",
    "    num_labels, labels, stats, centroids = cv2.connectedComponentsWithStats(np.uint8(dilated_frame), connectivity=8)\n",
    "    print('Number of Labels', num_labels)\n",
    "    print('Labels', labels)\n",
    "    print('stats', stats)\n",
    "    print('centroid', centroids)\n",
    "\n",
    "    # process the worm list\n",
    "    for i in range(1, len(w_list)):\n",
    "        temp_Worm = w_list[i]\n",
    "        temp_Worm.take_the_bundle_in(bundle)\n",
    "        temp_Worm.write_frame(iteration_Number)\n",
    "        w_list[i] = temp_Worm\n",
    "\n",
    "    for i in range(1, num_labels):\n",
    "        x, y, w, h, area = stats[i]\n",
    "        if area > min_area:\n",
    "            cv2.rectangle(dilated_frame, (x, y), (x + w, y + h), (255, 255, 255), 2)\n",
    "    for i in range(1, num_labels):\n",
    "        x, y, w, h, area = stats[i]\n",
    "        if area > min_area:\n",
    "            cv2.rectangle(ori_frame, (x, y), (x + w, y + h), (250,128,114), 2)\n",
    "    temp_frame = cv2.normalize(dilated_frame, None, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX, dtype=cv2.CV_8U)\n",
    "    rgb_frame = cv2.cvtColor(temp_frame, cv2.COLOR_GRAY2RGB)\n",
    "    prvs = next\n",
    "    cv2.imwrite(os.path.join(monitor_folder, f\"frame_{iteration_Number}.jpg\"), rgb_frame)\n",
    "    cv2.imwrite(os.path.join(output_folder, f\"frame_{iteration_Number}.jpg\"), ori_frame)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
